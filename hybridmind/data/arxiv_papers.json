[
  {
    "id": "arxiv-0",
    "title": "Proximal Policy Optimization with Continuous Bounded Action Space via\n  the Beta Distribution",
    "abstract": "Reinforcement learning methods for continuous control tasks have evolved in\nrecent years generating a family of policy gradient methods that rely primarily\non a Gaussian distribution for modeling a stochastic policy. However, the\nGaussian distribution has an infinite support, whereas real world applications\nusually have a bounded action space. This dissonance causes an estimation bias\nthat can be eliminated if the Beta distribution is used for the policy instead,\nas it presents a finite support. In this work, we investigate how this Beta\npolicy performs when it is trained by the Proximal Policy Optimization (PPO)\nalgorithm on two continuous control tasks from OpenAI gym. For both tasks, the\nBeta policy is superior to the Gaussian policy in terms of agent's final\nexpected reward, also showing more stability and faster convergence of the\ntraining process. For the CarRacing environment with high-dimensional image\ninput, the agent's success rate was improved by 63% over the Gaussian policy.",
    "text": "Proximal Policy Optimization with Continuous Bounded Action Space via\n  the Beta Distribution\n\nReinforcement learning methods for continuous control tasks have evolved in\nrecent years generating a family of policy gradient methods that rely primarily\non a Gaussian distribution for modeling a stochastic policy. However, the\nGaussian distribution has an infinite support, whereas real world applications\nusually have a bounded action space. This dissonance causes an estimation bias\nthat can be eliminated if the Beta distribution is used for the policy instead,\nas it presents a finite support. In this work, we investigate how this Beta\npolicy performs when it is trained by the Proximal Policy Optimization (PPO)\nalgorithm on two continuous control tasks from OpenAI gym. For both tasks, the\nBeta policy is superior to the Gaussian policy in terms of agent's final\nexpected reward, also showing more stability and faster convergence of the\ntraining process. For the CarRacing environment with high-dimensional image\ninput, the agent's success rate was improved by 63% over the Gaussian policy."
  },
  {
    "id": "arxiv-1",
    "title": "Machine Learning Pipeline for Pulsar Star Dataset",
    "abstract": "This work brings together some of the most common machine learning (ML)\nalgorithms, and the objective is to make a comparison at the level of obtained\nresults from a set of unbalanced data. This dataset is composed of almost 17\nthousand observations made to astronomical objects to identify pulsars (HTRU2).\nThe methodological proposal based on evaluating the accuracy of these different\nmodels on the same database treated with two different strategies for\nunbalanced data. The results show that in spite of the noise and unbalance of\nclasses present in this type of data, it is possible to apply them on standard\nML algorithms and obtain promising accuracy ratios.",
    "text": "Machine Learning Pipeline for Pulsar Star Dataset\n\nThis work brings together some of the most common machine learning (ML)\nalgorithms, and the objective is to make a comparison at the level of obtained\nresults from a set of unbalanced data. This dataset is composed of almost 17\nthousand observations made to astronomical objects to identify pulsars (HTRU2).\nThe methodological proposal based on evaluating the accuracy of these different\nmodels on the same database treated with two different strategies for\nunbalanced data. The results show that in spite of the noise and unbalance of\nclasses present in this type of data, it is possible to apply them on standard\nML algorithms and obtain promising accuracy ratios."
  },
  {
    "id": "arxiv-2",
    "title": "Inverse Reinforcement Learning Under Noisy Observations",
    "abstract": "We consider the problem of performing inverse reinforcement learning when the\ntrajectory of the expert is not perfectly observed by the learner. Instead, a\nnoisy continuous-time observation of the trajectory is provided to the learner.\nThis problem exhibits wide-ranging applications and the specific application we\nconsider here is the scenario in which the learner seeks to penetrate a\nperimeter patrolled by a robot. The learner's field of view is limited due to\nwhich it cannot observe the patroller's complete trajectory. Instead, we allow\nthe learner to listen to the expert's movement sound, which it can also use to\nestimate the expert's state and action using an observation model. We treat the\nexpert's state and action as hidden data and present an algorithm based on\nexpectation maximization and maximum entropy principle to solve the non-linear,\nnon-convex problem. Related work considers discrete-time observations and an\nobservation model that does not include actions. In contrast, our technique\ntakes expectations over both state and action of the expert, enabling learning\neven in the presence of extreme noise and broader applications.",
    "text": "Inverse Reinforcement Learning Under Noisy Observations\n\nWe consider the problem of performing inverse reinforcement learning when the\ntrajectory of the expert is not perfectly observed by the learner. Instead, a\nnoisy continuous-time observation of the trajectory is provided to the learner.\nThis problem exhibits wide-ranging applications and the specific application we\nconsider here is the scenario in which the learner seeks to penetrate a\nperimeter patrolled by a robot. The learner's field of view is limited due to\nwhich it cannot observe the patroller's complete trajectory. Instead, we allow\nthe learner to listen to the expert's movement sound, which it can also use to\nestimate the expert's state and action using an observation model. We treat the\nexpert's state and action as hidden data and present an algorithm based on\nexpectation maximization and maximum entropy principle to solve the non-linear,\nnon-convex problem. Related work considers discrete-time observations and an\nobservation model that does not include actions. In contrast, our technique\ntakes expectations over both state and action of the expert, enabling learning\neven in the presence of extreme noise and broader applications."
  },
  {
    "id": "arxiv-3",
    "title": "Diagnosing COVID-19 Pneumonia from X-Ray and CT Images using Deep\n  Learning and Transfer Learning Algorithms",
    "abstract": "COVID-19 (also known as 2019 Novel Coronavirus) first emerged in Wuhan, China\nand spread across the globe with unprecedented effect and has now become the\ngreatest crisis of the modern era. The COVID-19 has proved much more pervasive\ndemands for diagnosis that has driven researchers to develop more intelligent,\nhighly responsive and efficient detection methods. In this work, we focus on\nproposing AI tools that can be used by radiologists or healthcare professionals\nto diagnose COVID-19 cases in a quick and accurate manner. However, the lack of\na publicly available dataset of X-ray and CT images makes the design of such AI\ntools a challenging task. To this end, this study aims to build a comprehensive\ndataset of X-rays and CT scan images from multiple sources as well as provides\na simple but an effective COVID-19 detection technique using deep learning and\ntransfer learning algorithms. In this vein, a simple convolution neural network\n(CNN) and modified pre-trained AlexNet model are applied on the prepared X-rays\nand CT scan images dataset. The result of the experiments shows that the\nutilized models can provide accuracy up to 98 % via pre-trained network and\n94.1 % accuracy by using the modified CNN.",
    "text": "Diagnosing COVID-19 Pneumonia from X-Ray and CT Images using Deep\n  Learning and Transfer Learning Algorithms\n\nCOVID-19 (also known as 2019 Novel Coronavirus) first emerged in Wuhan, China\nand spread across the globe with unprecedented effect and has now become the\ngreatest crisis of the modern era. The COVID-19 has proved much more pervasive\ndemands for diagnosis that has driven researchers to develop more intelligent,\nhighly responsive and efficient detection methods. In this work, we focus on\nproposing AI tools that can be used by radiologists or healthcare professionals\nto diagnose COVID-19 cases in a quick and accurate manner. However, the lack of\na publicly available dataset of X-ray and CT images makes the design of such AI\ntools a challenging task. To this end, this study aims to build a comprehensive\ndataset of X-rays and CT scan images from multiple sources as well as provides\na simple but an effective COVID-19 detection technique using deep learning and\ntransfer learning algorithms. In this vein, a simple convolution neural network\n(CNN) and modified pre-trained AlexNet model are applied on the prepared X-rays\nand CT scan images dataset. The result of the experiments shows that the\nutilized models can provide accuracy up to 98 % via pre-trained network and\n94.1 % accuracy by using the modified CNN."
  },
  {
    "id": "arxiv-4",
    "title": "Data Augmentation for Compositional Data: Advancing Predictive Models of the Microbiome",
    "abstract": "Data augmentation plays a key role in modern machine learning pipelines.\nWhile numerous augmentation strategies have been studied in the context of\ncomputer vision and natural language processing, less is known for other data\nmodalities. Our work extends the success of data augmentation to compositional\ndata, i.e., simplex-valued data, which is of particular interest in the context\nof the human microbiome. Drawing on key principles from compositional data\nanalysis, such as the Aitchison geometry of the simplex and subcompositions, we\ndefine novel augmentation strategies for this data modality. Incorporating our\ndata augmentations into standard supervised learning pipelines results in\nconsistent performance gains across a wide range of standard benchmark\ndatasets. In particular, we set a new state-of-the-art for key disease\nprediction tasks including colorectal cancer, type 2 diabetes, and Crohn's\ndisease. In addition, our data augmentations enable us to define a novel\ncontrastive learning model, which improves on previous representation learning\napproaches for microbiome compositional data. Our code is available at\nhttps://github.com/cunningham-lab/AugCoDa.",
    "text": "Data Augmentation for Compositional Data: Advancing Predictive Models of the Microbiome\n\nData augmentation plays a key role in modern machine learning pipelines.\nWhile numerous augmentation strategies have been studied in the context of\ncomputer vision and natural language processing, less is known for other data\nmodalities. Our work extends the success of data augmentation to compositional\ndata, i.e., simplex-valued data, which is of particular interest in the context\nof the human microbiome. Drawing on key principles from compositional data\nanalysis, such as the Aitchison geometry of the simplex and subcompositions, we\ndefine novel augmentation strategies for this data modality. Incorporating our\ndata augmentations into standard supervised learning pipelines results in\nconsistent performance gains across a wide range of standard benchmark\ndatasets. In particular, we set a new state-of-the-art for key disease\nprediction tasks including colorectal cancer, type 2 diabetes, and Crohn's\ndisease. In addition, our data augmentations enable us to define a novel\ncontrastive learning model, which improves on previous representation learning\napproaches for microbiome compositional data. Our code is available at\nhttps://github.com/cunningham-lab/AugCoDa."
  },
  {
    "id": "arxiv-5",
    "title": "C-AllOut: Catching & Calling Outliers by Type",
    "abstract": "Given an unlabeled dataset, wherein we have access only to pairwise\nsimilarities (or distances), how can we effectively (1) detect outliers, and\n(2) annotate/tag the outliers by type? Outlier detection has a large\nliterature, yet we find a key gap in the field: to our knowledge, no existing\nwork addresses the outlier annotation problem. Outliers are broadly classified\ninto 3 types, representing distinct patterns that could be valuable to\nanalysts: (a) global outliers are severe yet isolate cases that do not repeat,\ne.g., a data collection error; (b) local outliers diverge from their peers\nwithin a context, e.g., a particularly short basketball player; and (c)\ncollective outliers are isolated micro-clusters that may indicate coalition or\nrepetitions, e.g., frauds that exploit the same loophole. This paper presents\nC-AllOut: a novel and effective outlier detector that annotates outliers by\ntype. It is parameter-free and scalable, besides working only with pairwise\nsimilarities (or distances) when it is needed. We show that C-AllOut achieves\non par or significantly better performance than state-of-the-art detectors when\nspotting outliers regardless of their type. It is also highly effective in\nannotating outliers of particular types, a task that none of the baselines can\nperform.",
    "text": "C-AllOut: Catching & Calling Outliers by Type\n\nGiven an unlabeled dataset, wherein we have access only to pairwise\nsimilarities (or distances), how can we effectively (1) detect outliers, and\n(2) annotate/tag the outliers by type? Outlier detection has a large\nliterature, yet we find a key gap in the field: to our knowledge, no existing\nwork addresses the outlier annotation problem. Outliers are broadly classified\ninto 3 types, representing distinct patterns that could be valuable to\nanalysts: (a) global outliers are severe yet isolate cases that do not repeat,\ne.g., a data collection error; (b) local outliers diverge from their peers\nwithin a context, e.g., a particularly short basketball player; and (c)\ncollective outliers are isolated micro-clusters that may indicate coalition or\nrepetitions, e.g., frauds that exploit the same loophole. This paper presents\nC-AllOut: a novel and effective outlier detector that annotates outliers by\ntype. It is parameter-free and scalable, besides working only with pairwise\nsimilarities (or distances) when it is needed. We show that C-AllOut achieves\non par or significantly better performance than state-of-the-art detectors when\nspotting outliers regardless of their type. It is also highly effective in\nannotating outliers of particular types, a task that none of the baselines can\nperform."
  },
  {
    "id": "arxiv-6",
    "title": "PAnDR: Fast Adaptation to New Environments from Offline Experiences via\n  Decoupling Policy and Environment Representations",
    "abstract": "Deep Reinforcement Learning (DRL) has been a promising solution to many\ncomplex decision-making problems. Nevertheless, the notorious weakness in\ngeneralization among environments prevent widespread application of DRL agents\nin real-world scenarios. Although advances have been made recently, most prior\nworks assume sufficient online interaction on training environments, which can\nbe costly in practical cases. To this end, we focus on an\noffline-training-online-adaptation setting, in which the agent first learns\nfrom offline experiences collected in environments with different dynamics and\nthen performs online policy adaptation in environments with new dynamics. In\nthis paper, we propose Policy Adaptation with Decoupled Representations (PAnDR)\nfor fast policy adaptation. In offline training phase, the environment\nrepresentation and policy representation are learned through contrastive\nlearning and policy recovery, respectively. The representations are further\nrefined by mutual information optimization to make them more decoupled and\ncomplete. With learned representations, a Policy-Dynamics Value Function (PDVF)\n[Raileanu et al., 2020] network is trained to approximate the values for\ndifferent combinations of policies and environments from offline experiences.\nIn online adaptation phase, with the environment context inferred from few\nexperiences collected in new environments, the policy is optimized by gradient\nascent with respect to the PDVF. Our experiments show that PAnDR outperforms\nexisting algorithms in several representative policy adaptation problems.",
    "text": "PAnDR: Fast Adaptation to New Environments from Offline Experiences via\n  Decoupling Policy and Environment Representations\n\nDeep Reinforcement Learning (DRL) has been a promising solution to many\ncomplex decision-making problems. Nevertheless, the notorious weakness in\ngeneralization among environments prevent widespread application of DRL agents\nin real-world scenarios. Although advances have been made recently, most prior\nworks assume sufficient online interaction on training environments, which can\nbe costly in practical cases. To this end, we focus on an\noffline-training-online-adaptation setting, in which the agent first learns\nfrom offline experiences collected in environments with different dynamics and\nthen performs online policy adaptation in environments with new dynamics. In\nthis paper, we propose Policy Adaptation with Decoupled Representations (PAnDR)\nfor fast policy adaptation. In offline training phase, the environment\nrepresentation and policy representation are learned through contrastive\nlearning and policy recovery, respectively. The representations are further\nrefined by mutual information optimization to make them more decoupled and\ncomplete. With learned representations, a Policy-Dynamics Value Function (PDVF)\n[Raileanu et al., 2020] network is trained to approximate the values for\ndifferent combinations of policies and environments from offline experiences.\nIn online adaptation phase, with the environment context inferred from few\nexperiences collected in new environments, the policy is optimized by gradient\nascent with respect to the PDVF. Our experiments show that PAnDR outperforms\nexisting algorithms in several representative policy adaptation problems."
  },
  {
    "id": "arxiv-7",
    "title": "A New Intelligence Based Approach for Computer-Aided Diagnosis of Dengue\n  Fever",
    "abstract": "Identification of the influential clinical symptoms and laboratory features\nthat help in the diagnosis of dengue fever in early phase of the illness would\naid in designing effective public health management and virological\nsurveillance strategies. Keeping this as our main objective we develop in this\npaper, a new computational intelligence based methodology that predicts the\ndiagnosis in real time, minimizing the number of false positives and false\nnegatives. Our methodology consists of three major components (i) a novel\nmissing value imputation procedure that can be applied on any data set\nconsisting of categorical (nominal) and/or numeric (real or integer) (ii) a\nwrapper based features selection method with genetic search for extracting a\nsubset of most influential symptoms that can diagnose the illness and (iii) an\nalternating decision tree method that employs boosting for generating highly\naccurate decision rules. The predictive models developed using our methodology\nare found to be more accurate than the state-of-the-art methodologies used in\nthe diagnosis of the dengue fever.",
    "text": "A New Intelligence Based Approach for Computer-Aided Diagnosis of Dengue\n  Fever\n\nIdentification of the influential clinical symptoms and laboratory features\nthat help in the diagnosis of dengue fever in early phase of the illness would\naid in designing effective public health management and virological\nsurveillance strategies. Keeping this as our main objective we develop in this\npaper, a new computational intelligence based methodology that predicts the\ndiagnosis in real time, minimizing the number of false positives and false\nnegatives. Our methodology consists of three major components (i) a novel\nmissing value imputation procedure that can be applied on any data set\nconsisting of categorical (nominal) and/or numeric (real or integer) (ii) a\nwrapper based features selection method with genetic search for extracting a\nsubset of most influential symptoms that can diagnose the illness and (iii) an\nalternating decision tree method that employs boosting for generating highly\naccurate decision rules. The predictive models developed using our methodology\nare found to be more accurate than the state-of-the-art methodologies used in\nthe diagnosis of the dengue fever."
  },
  {
    "id": "arxiv-8",
    "title": "Sparse Graph Learning with Eigen-gap for Spectral Filter Training in\n  Graph Convolutional Networks",
    "abstract": "It is now known that the expressive power of graph convolutional neural nets\n(GCN) does not grow infinitely with the number of layers. Instead, the GCN\noutput approaches a subspace spanned by the first eigenvector of the normalized\ngraph Laplacian matrix with the convergence rate characterized by the\n\"eigen-gap\": the difference between the Laplacian's first two distinct\neigenvalues. To promote a deeper GCN architecture with sufficient\nexpressiveness, in this paper, given an empirical covariance matrix $\\bar{C}$\ncomputed from observable data, we learn a sparse graph Laplacian matrix $L$\nclosest to $\\bar{C}^{-1}$ while maintaining a desirable eigen-gap that slows\ndown convergence. Specifically, we first define a sparse graph learning problem\nwith constraints on the first eigenvector (the most common signal) and the\neigen-gap. We solve the corresponding dual problem greedily, where a locally\noptimal eigen-pair is computed one at a time via a fast approximation of a\nsemi-definite programming (SDP) formulation. The computed $L$ with the desired\neigen-gap is normalized spectrally and used for supervised training of GCN for\na targeted task. Experiments show that our proposal produced deeper GCNs and\nsmaller errors compared to a competing scheme without explicit eigen-gap\noptimization.",
    "text": "Sparse Graph Learning with Eigen-gap for Spectral Filter Training in\n  Graph Convolutional Networks\n\nIt is now known that the expressive power of graph convolutional neural nets\n(GCN) does not grow infinitely with the number of layers. Instead, the GCN\noutput approaches a subspace spanned by the first eigenvector of the normalized\ngraph Laplacian matrix with the convergence rate characterized by the\n\"eigen-gap\": the difference between the Laplacian's first two distinct\neigenvalues. To promote a deeper GCN architecture with sufficient\nexpressiveness, in this paper, given an empirical covariance matrix $\\bar{C}$\ncomputed from observable data, we learn a sparse graph Laplacian matrix $L$\nclosest to $\\bar{C}^{-1}$ while maintaining a desirable eigen-gap that slows\ndown convergence. Specifically, we first define a sparse graph learning problem\nwith constraints on the first eigenvector (the most common signal) and the\neigen-gap. We solve the corresponding dual problem greedily, where a locally\noptimal eigen-pair is computed one at a time via a fast approximation of a\nsemi-definite programming (SDP) formulation. The computed $L$ with the desired\neigen-gap is normalized spectrally and used for supervised training of GCN for\na targeted task. Experiments show that our proposal produced deeper GCNs and\nsmaller errors compared to a competing scheme without explicit eigen-gap\noptimization."
  },
  {
    "id": "arxiv-9",
    "title": "Non-Gaussian Gaussian Processes for Few-Shot Regression",
    "abstract": "Gaussian Processes (GPs) have been widely used in machine learning to model\ndistributions over functions, with applications including multi-modal\nregression, time-series prediction, and few-shot learning. GPs are particularly\nuseful in the last application since they rely on Normal distributions and\nenable closed-form computation of the posterior probability function.\nUnfortunately, because the resulting posterior is not flexible enough to\ncapture complex distributions, GPs assume high similarity between subsequent\ntasks - a requirement rarely met in real-world conditions. In this work, we\naddress this limitation by leveraging the flexibility of Normalizing Flows to\nmodulate the posterior predictive distribution of the GP. This makes the GP\nposterior locally non-Gaussian, therefore we name our method Non-Gaussian\nGaussian Processes (NGGPs). More precisely, we propose an invertible ODE-based\nmapping that operates on each component of the random variable vectors and\nshares the parameters across all of them. We empirically tested the flexibility\nof NGGPs on various few-shot learning regression datasets, showing that the\nmapping can incorporate context embedding information to model different noise\nlevels for periodic functions. As a result, our method shares the structure of\nthe problem between subsequent tasks, but the contextualization allows for\nadaptation to dissimilarities. NGGPs outperform the competing state-of-the-art\napproaches on a diversified set of benchmarks and applications.",
    "text": "Non-Gaussian Gaussian Processes for Few-Shot Regression\n\nGaussian Processes (GPs) have been widely used in machine learning to model\ndistributions over functions, with applications including multi-modal\nregression, time-series prediction, and few-shot learning. GPs are particularly\nuseful in the last application since they rely on Normal distributions and\nenable closed-form computation of the posterior probability function.\nUnfortunately, because the resulting posterior is not flexible enough to\ncapture complex distributions, GPs assume high similarity between subsequent\ntasks - a requirement rarely met in real-world conditions. In this work, we\naddress this limitation by leveraging the flexibility of Normalizing Flows to\nmodulate the posterior predictive distribution of the GP. This makes the GP\nposterior locally non-Gaussian, therefore we name our method Non-Gaussian\nGaussian Processes (NGGPs). More precisely, we propose an invertible ODE-based\nmapping that operates on each component of the random variable vectors and\nshares the parameters across all of them. We empirically tested the flexibility\nof NGGPs on various few-shot learning regression datasets, showing that the\nmapping can incorporate context embedding information to model different noise\nlevels for periodic functions. As a result, our method shares the structure of\nthe problem between subsequent tasks, but the contextualization allows for\nadaptation to dissimilarities. NGGPs outperform the competing state-of-the-art\napproaches on a diversified set of benchmarks and applications."
  },
  {
    "id": "arxiv-10",
    "title": "Doubly Stochastic Adversarial Autoencoder",
    "abstract": "Any autoencoder network can be turned into a generative model by imposing an\narbitrary prior distribution on its hidden code vector. Variational Autoencoder\n(VAE) [2] uses a KL divergence penalty to impose the prior, whereas Adversarial\nAutoencoder (AAE) [1] uses {\\it generative adversarial networks} GAN [3]. GAN\ntrades the complexities of {\\it sampling} algorithms with the complexities of\n{\\it searching} Nash equilibrium in minimax games. Such minimax architectures\nget trained with the help of data examples and gradients flowing through a\ngenerator and an adversary. A straightforward modification of AAE is to replace\nthe adversary with the maximum mean discrepancy (MMD) test [4-5]. This\nreplacement leads to a new type of probabilistic autoencoder, which is also\ndiscussed in our paper. We propose a novel probabilistic autoencoder in which\nthe adversary of AAE is replaced with a space of {\\it stochastic} functions.\nThis replacement introduces a new source of randomness, which can be considered\nas a continuous control for encouraging {\\it explorations}. This prevents the\nadversary from fitting too closely to the generator and therefore leads to a\nmore diverse set of generated samples.",
    "text": "Doubly Stochastic Adversarial Autoencoder\n\nAny autoencoder network can be turned into a generative model by imposing an\narbitrary prior distribution on its hidden code vector. Variational Autoencoder\n(VAE) [2] uses a KL divergence penalty to impose the prior, whereas Adversarial\nAutoencoder (AAE) [1] uses {\\it generative adversarial networks} GAN [3]. GAN\ntrades the complexities of {\\it sampling} algorithms with the complexities of\n{\\it searching} Nash equilibrium in minimax games. Such minimax architectures\nget trained with the help of data examples and gradients flowing through a\ngenerator and an adversary. A straightforward modification of AAE is to replace\nthe adversary with the maximum mean discrepancy (MMD) test [4-5]. This\nreplacement leads to a new type of probabilistic autoencoder, which is also\ndiscussed in our paper. We propose a novel probabilistic autoencoder in which\nthe adversary of AAE is replaced with a space of {\\it stochastic} functions.\nThis replacement introduces a new source of randomness, which can be considered\nas a continuous control for encouraging {\\it explorations}. This prevents the\nadversary from fitting too closely to the generator and therefore leads to a\nmore diverse set of generated samples."
  },
  {
    "id": "arxiv-11",
    "title": "Neural Approximate Sufficient Statistics for Implicit Models",
    "abstract": "We consider the fundamental problem of how to automatically construct summary\nstatistics for implicit generative models where the evaluation of the\nlikelihood function is intractable, but sampling data from the model is\npossible. The idea is to frame the task of constructing sufficient statistics\nas learning mutual information maximizing representations of the data with the\nhelp of deep neural networks. The infomax learning procedure does not need to\nestimate any density or density ratio. We apply our approach to both\ntraditional approximate Bayesian computation and recent neural likelihood\nmethods, boosting their performance on a range of tasks.",
    "text": "Neural Approximate Sufficient Statistics for Implicit Models\n\nWe consider the fundamental problem of how to automatically construct summary\nstatistics for implicit generative models where the evaluation of the\nlikelihood function is intractable, but sampling data from the model is\npossible. The idea is to frame the task of constructing sufficient statistics\nas learning mutual information maximizing representations of the data with the\nhelp of deep neural networks. The infomax learning procedure does not need to\nestimate any density or density ratio. We apply our approach to both\ntraditional approximate Bayesian computation and recent neural likelihood\nmethods, boosting their performance on a range of tasks."
  },
  {
    "id": "arxiv-12",
    "title": "Multi-Sample $\\zeta$-mixup: Richer, More Realistic Synthetic Samples\n  from a $p$-Series Interpolant",
    "abstract": "Modern deep learning training procedures rely on model regularization\ntechniques such as data augmentation methods, which generate training samples\nthat increase the diversity of data and richness of label information. A\npopular recent method, mixup, uses convex combinations of pairs of original\nsamples to generate new samples. However, as we show in our experiments, mixup\ncan produce undesirable synthetic samples, where the data is sampled off the\nmanifold and can contain incorrect labels. We propose $\\zeta$-mixup, a\ngeneralization of mixup with provably and demonstrably desirable properties\nthat allows convex combinations of $N \\geq 2$ samples, leading to more\nrealistic and diverse outputs that incorporate information from $N$ original\nsamples by using a $p$-series interpolant. We show that, compared to mixup,\n$\\zeta$-mixup better preserves the intrinsic dimensionality of the original\ndatasets, which is a desirable property for training generalizable models.\nFurthermore, we show that our implementation of $\\zeta$-mixup is faster than\nmixup, and extensive evaluation on controlled synthetic and 24 real-world\nnatural and medical image classification datasets shows that $\\zeta$-mixup\noutperforms mixup and traditional data augmentation techniques.",
    "text": "Multi-Sample $\\zeta$-mixup: Richer, More Realistic Synthetic Samples\n  from a $p$-Series Interpolant\n\nModern deep learning training procedures rely on model regularization\ntechniques such as data augmentation methods, which generate training samples\nthat increase the diversity of data and richness of label information. A\npopular recent method, mixup, uses convex combinations of pairs of original\nsamples to generate new samples. However, as we show in our experiments, mixup\ncan produce undesirable synthetic samples, where the data is sampled off the\nmanifold and can contain incorrect labels. We propose $\\zeta$-mixup, a\ngeneralization of mixup with provably and demonstrably desirable properties\nthat allows convex combinations of $N \\geq 2$ samples, leading to more\nrealistic and diverse outputs that incorporate information from $N$ original\nsamples by using a $p$-series interpolant. We show that, compared to mixup,\n$\\zeta$-mixup better preserves the intrinsic dimensionality of the original\ndatasets, which is a desirable property for training generalizable models.\nFurthermore, we show that our implementation of $\\zeta$-mixup is faster than\nmixup, and extensive evaluation on controlled synthetic and 24 real-world\nnatural and medical image classification datasets shows that $\\zeta$-mixup\noutperforms mixup and traditional data augmentation techniques."
  },
  {
    "id": "arxiv-13",
    "title": "Adversarial Unlearning of Backdoors via Implicit Hypergradient",
    "abstract": "We propose a minimax formulation for removing backdoors from a given poisoned\nmodel based on a small set of clean data. This formulation encompasses much of\nprior work on backdoor removal. We propose the Implicit Bacdoor Adversarial\nUnlearning (I-BAU) algorithm to solve the minimax. Unlike previous work, which\nbreaks down the minimax into separate inner and outer problems, our algorithm\nutilizes the implicit hypergradient to account for the interdependence between\ninner and outer optimization. We theoretically analyze its convergence and the\ngeneralizability of the robustness gained by solving minimax on clean data to\nunseen test data. In our evaluation, we compare I-BAU with six state-of-art\nbackdoor defenses on seven backdoor attacks over two datasets and various\nattack settings, including the common setting where the attacker targets one\nclass as well as important but underexplored settings where multiple classes\nare targeted. I-BAU's performance is comparable to and most often significantly\nbetter than the best baseline. Particularly, its performance is more robust to\nthe variation on triggers, attack settings, poison ratio, and clean data size.\nMoreover, I-BAU requires less computation to take effect; particularly, it is\nmore than $13\\times$ faster than the most efficient baseline in the\nsingle-target attack setting. Furthermore, it can remain effective in the\nextreme case where the defender can only access 100 clean samples -- a setting\nwhere all the baselines fail to produce acceptable results.",
    "text": "Adversarial Unlearning of Backdoors via Implicit Hypergradient\n\nWe propose a minimax formulation for removing backdoors from a given poisoned\nmodel based on a small set of clean data. This formulation encompasses much of\nprior work on backdoor removal. We propose the Implicit Bacdoor Adversarial\nUnlearning (I-BAU) algorithm to solve the minimax. Unlike previous work, which\nbreaks down the minimax into separate inner and outer problems, our algorithm\nutilizes the implicit hypergradient to account for the interdependence between\ninner and outer optimization. We theoretically analyze its convergence and the\ngeneralizability of the robustness gained by solving minimax on clean data to\nunseen test data. In our evaluation, we compare I-BAU with six state-of-art\nbackdoor defenses on seven backdoor attacks over two datasets and various\nattack settings, including the common setting where the attacker targets one\nclass as well as important but underexplored settings where multiple classes\nare targeted. I-BAU's performance is comparable to and most often significantly\nbetter than the best baseline. Particularly, its performance is more robust to\nthe variation on triggers, attack settings, poison ratio, and clean data size.\nMoreover, I-BAU requires less computation to take effect; particularly, it is\nmore than $13\\times$ faster than the most efficient baseline in the\nsingle-target attack setting. Furthermore, it can remain effective in the\nextreme case where the defender can only access 100 clean samples -- a setting\nwhere all the baselines fail to produce acceptable results."
  },
  {
    "id": "arxiv-14",
    "title": "Trustworthy AI and Robotics and the Implications for the AEC Industry: A\n  Systematic Literature Review and Future Potentials",
    "abstract": "Human-technology interaction deals with trust as an inevitable requirement\nfor user acceptance. As the applications of artificial intelligence (AI) and\nrobotics emerge and with their ever-growing socio-economic influence in various\nfields of research and practice, there is an imminent need to study trust in\nsuch systems. With the opaque work mechanism of AI-based systems and the\nprospect of intelligent robots as workers' companions, context-specific\ninterdisciplinary studies on trust are key in increasing their adoption.\nThrough a thorough systematic literature review on (1) trust in AI and robotics\n(AIR) and (2) AIR applications in the architecture, engineering, and\nconstruction (AEC) industry, this study identifies common trust dimensions in\nthe literature and uses them to organize the paper. Furthermore, the\nconnections of the identified dimensions to the existing and potential AEC\napplications are determined and discussed. Finally, major future directions on\ntrustworthy AI and robotics in AEC research and practice are outlined.",
    "text": "Trustworthy AI and Robotics and the Implications for the AEC Industry: A\n  Systematic Literature Review and Future Potentials\n\nHuman-technology interaction deals with trust as an inevitable requirement\nfor user acceptance. As the applications of artificial intelligence (AI) and\nrobotics emerge and with their ever-growing socio-economic influence in various\nfields of research and practice, there is an imminent need to study trust in\nsuch systems. With the opaque work mechanism of AI-based systems and the\nprospect of intelligent robots as workers' companions, context-specific\ninterdisciplinary studies on trust are key in increasing their adoption.\nThrough a thorough systematic literature review on (1) trust in AI and robotics\n(AIR) and (2) AIR applications in the architecture, engineering, and\nconstruction (AEC) industry, this study identifies common trust dimensions in\nthe literature and uses them to organize the paper. Furthermore, the\nconnections of the identified dimensions to the existing and potential AEC\napplications are determined and discussed. Finally, major future directions on\ntrustworthy AI and robotics in AEC research and practice are outlined."
  },
  {
    "id": "arxiv-15",
    "title": "Dual Principal Component Pursuit: Probability Analysis and Efficient\n  Algorithms",
    "abstract": "Recent methods for learning a linear subspace from data corrupted by outliers\nare based on convex $\\ell_1$ and nuclear norm optimization and require the\ndimension of the subspace and the number of outliers to be sufficiently small.\nIn sharp contrast, the recently proposed Dual Principal Component Pursuit\n(DPCP) method can provably handle subspaces of high dimension by solving a\nnon-convex $\\ell_1$ optimization problem on the sphere. However, its geometric\nanalysis is based on quantities that are difficult to interpret and are not\namenable to statistical analysis. In this paper we provide a refined geometric\nanalysis and a new statistical analysis that show that DPCP can tolerate as\nmany outliers as the square of the number of inliers, thus improving upon other\nprovably correct robust PCA methods. We also propose a scalable Projected\nSub-Gradient Method method (DPCP-PSGM) for solving the DPCP problem and show it\nadmits linear convergence even though the underlying optimization problem is\nnon-convex and non-smooth. Experiments on road plane detection from 3D point\ncloud data demonstrate that DPCP-PSGM can be more efficient than the\ntraditional RANSAC algorithm, which is one of the most popular methods for such\ncomputer vision applications.",
    "text": "Dual Principal Component Pursuit: Probability Analysis and Efficient\n  Algorithms\n\nRecent methods for learning a linear subspace from data corrupted by outliers\nare based on convex $\\ell_1$ and nuclear norm optimization and require the\ndimension of the subspace and the number of outliers to be sufficiently small.\nIn sharp contrast, the recently proposed Dual Principal Component Pursuit\n(DPCP) method can provably handle subspaces of high dimension by solving a\nnon-convex $\\ell_1$ optimization problem on the sphere. However, its geometric\nanalysis is based on quantities that are difficult to interpret and are not\namenable to statistical analysis. In this paper we provide a refined geometric\nanalysis and a new statistical analysis that show that DPCP can tolerate as\nmany outliers as the square of the number of inliers, thus improving upon other\nprovably correct robust PCA methods. We also propose a scalable Projected\nSub-Gradient Method method (DPCP-PSGM) for solving the DPCP problem and show it\nadmits linear convergence even though the underlying optimization problem is\nnon-convex and non-smooth. Experiments on road plane detection from 3D point\ncloud data demonstrate that DPCP-PSGM can be more efficient than the\ntraditional RANSAC algorithm, which is one of the most popular methods for such\ncomputer vision applications."
  },
  {
    "id": "arxiv-16",
    "title": "Generative Adversarial Networks (GANs): What it can generate and What it\n  cannot?",
    "abstract": "In recent years, Generative Adversarial Networks (GANs) have received\nsignificant attention from the research community. With a straightforward\nimplementation and outstanding results, GANs have been used for numerous\napplications. Despite the success, GANs lack a proper theoretical explanation.\nThese models suffer from issues like mode collapse, non-convergence, and\ninstability during training. To address these issues, researchers have proposed\ntheoretically rigorous frameworks inspired by varied fields of Game theory,\nStatistical theory, Dynamical systems, etc.\n  In this paper, we propose to give an appropriate structure to study these\ncontributions systematically. We essentially categorize the papers based on the\nissues they raise and the kind of novelty they introduce to address them.\nBesides, we provide insight into how each of the discussed articles solves the\nconcerned problems. We compare and contrast different results and put forth a\nsummary of theoretical contributions about GANs with focus on image/visual\napplications. We expect this summary paper to give a bird's eye view to a\nperson wishing to understand the theoretical progress in GANs so far.",
    "text": "Generative Adversarial Networks (GANs): What it can generate and What it\n  cannot?\n\nIn recent years, Generative Adversarial Networks (GANs) have received\nsignificant attention from the research community. With a straightforward\nimplementation and outstanding results, GANs have been used for numerous\napplications. Despite the success, GANs lack a proper theoretical explanation.\nThese models suffer from issues like mode collapse, non-convergence, and\ninstability during training. To address these issues, researchers have proposed\ntheoretically rigorous frameworks inspired by varied fields of Game theory,\nStatistical theory, Dynamical systems, etc.\n  In this paper, we propose to give an appropriate structure to study these\ncontributions systematically. We essentially categorize the papers based on the\nissues they raise and the kind of novelty they introduce to address them.\nBesides, we provide insight into how each of the discussed articles solves the\nconcerned problems. We compare and contrast different results and put forth a\nsummary of theoretical contributions about GANs with focus on image/visual\napplications. We expect this summary paper to give a bird's eye view to a\nperson wishing to understand the theoretical progress in GANs so far."
  },
  {
    "id": "arxiv-17",
    "title": "Deep Joint Source-Channel Coding for Wireless Image Transmission",
    "abstract": "We propose a joint source and channel coding (JSCC) technique for wireless\nimage transmission that does not rely on explicit codes for either compression\nor error correction; instead, it directly maps the image pixel values to the\ncomplex-valued channel input symbols. We parameterize the encoder and decoder\nfunctions by two convolutional neural networks (CNNs), which are trained\njointly, and can be considered as an autoencoder with a non-trainable layer in\nthe middle that represents the noisy communication channel. Our results show\nthat the proposed deep JSCC scheme outperforms digital transmission\nconcatenating JPEG or JPEG2000 compression with a capacity achieving channel\ncode at low signal-to-noise ratio (SNR) and channel bandwidth values in the\npresence of additive white Gaussian noise (AWGN). More strikingly, deep JSCC\ndoes not suffer from the ``cliff effect'', and it provides a graceful\nperformance degradation as the channel SNR varies with respect to the SNR value\nassumed during training. In the case of a slow Rayleigh fading channel, deep\nJSCC learns noise resilient coded representations and significantly outperforms\nseparation-based digital communication at all SNR and channel bandwidth values.",
    "text": "Deep Joint Source-Channel Coding for Wireless Image Transmission\n\nWe propose a joint source and channel coding (JSCC) technique for wireless\nimage transmission that does not rely on explicit codes for either compression\nor error correction; instead, it directly maps the image pixel values to the\ncomplex-valued channel input symbols. We parameterize the encoder and decoder\nfunctions by two convolutional neural networks (CNNs), which are trained\njointly, and can be considered as an autoencoder with a non-trainable layer in\nthe middle that represents the noisy communication channel. Our results show\nthat the proposed deep JSCC scheme outperforms digital transmission\nconcatenating JPEG or JPEG2000 compression with a capacity achieving channel\ncode at low signal-to-noise ratio (SNR) and channel bandwidth values in the\npresence of additive white Gaussian noise (AWGN). More strikingly, deep JSCC\ndoes not suffer from the ``cliff effect'', and it provides a graceful\nperformance degradation as the channel SNR varies with respect to the SNR value\nassumed during training. In the case of a slow Rayleigh fading channel, deep\nJSCC learns noise resilient coded representations and significantly outperforms\nseparation-based digital communication at all SNR and channel bandwidth values."
  },
  {
    "id": "arxiv-18",
    "title": "Open Vocabulary Extreme Classification Using Generative Models",
    "abstract": "The extreme multi-label classification (XMC) task aims at tagging content\nwith a subset of labels from an extremely large label set. The label vocabulary\nis typically defined in advance by domain experts and assumed to capture all\nnecessary tags. However in real world scenarios this label set, although large,\nis often incomplete and experts frequently need to refine it. To develop\nsystems that simplify this process, we introduce the task of open vocabulary\nXMC (OXMC): given a piece of content, predict a set of labels, some of which\nmay be outside of the known tag set. Hence, in addition to not having training\ndata for some labels - as is the case in zero-shot classification - models need\nto invent some labels on-the-fly. We propose GROOV, a fine-tuned seq2seq model\nfor OXMC that generates the set of labels as a flat sequence and is trained\nusing a novel loss independent of predicted label order. We show the efficacy\nof the approach, experimenting with popular XMC datasets for which GROOV is\nable to predict meaningful labels outside the given vocabulary while performing\non par with state-of-the-art solutions for known labels.",
    "text": "Open Vocabulary Extreme Classification Using Generative Models\n\nThe extreme multi-label classification (XMC) task aims at tagging content\nwith a subset of labels from an extremely large label set. The label vocabulary\nis typically defined in advance by domain experts and assumed to capture all\nnecessary tags. However in real world scenarios this label set, although large,\nis often incomplete and experts frequently need to refine it. To develop\nsystems that simplify this process, we introduce the task of open vocabulary\nXMC (OXMC): given a piece of content, predict a set of labels, some of which\nmay be outside of the known tag set. Hence, in addition to not having training\ndata for some labels - as is the case in zero-shot classification - models need\nto invent some labels on-the-fly. We propose GROOV, a fine-tuned seq2seq model\nfor OXMC that generates the set of labels as a flat sequence and is trained\nusing a novel loss independent of predicted label order. We show the efficacy\nof the approach, experimenting with popular XMC datasets for which GROOV is\nable to predict meaningful labels outside the given vocabulary while performing\non par with state-of-the-art solutions for known labels."
  },
  {
    "id": "arxiv-19",
    "title": "Individually Fair Gradient Boosting",
    "abstract": "We consider the task of enforcing individual fairness in gradient boosting.\nGradient boosting is a popular method for machine learning from tabular data,\nwhich arise often in applications where algorithmic fairness is a concern. At a\nhigh level, our approach is a functional gradient descent on a\n(distributionally) robust loss function that encodes our intuition of\nalgorithmic fairness for the ML task at hand. Unlike prior approaches to\nindividual fairness that only work with smooth ML models, our approach also\nworks with non-smooth models such as decision trees. We show that our algorithm\nconverges globally and generalizes. We also demonstrate the efficacy of our\nalgorithm on three ML problems susceptible to algorithmic bias.",
    "text": "Individually Fair Gradient Boosting\n\nWe consider the task of enforcing individual fairness in gradient boosting.\nGradient boosting is a popular method for machine learning from tabular data,\nwhich arise often in applications where algorithmic fairness is a concern. At a\nhigh level, our approach is a functional gradient descent on a\n(distributionally) robust loss function that encodes our intuition of\nalgorithmic fairness for the ML task at hand. Unlike prior approaches to\nindividual fairness that only work with smooth ML models, our approach also\nworks with non-smooth models such as decision trees. We show that our algorithm\nconverges globally and generalizes. We also demonstrate the efficacy of our\nalgorithm on three ML problems susceptible to algorithmic bias."
  },
  {
    "id": "arxiv-20",
    "title": "Predicting Porosity, Permeability, and Tortuosity of Porous Media from\n  Images by Deep Learning",
    "abstract": "Convolutional neural networks (CNN) are utilized to encode the relation\nbetween initial configurations of obstacles and three fundamental quantities in\nporous media: porosity ($\\varphi$), permeability $k$, and tortuosity ($T$). The\ntwo-dimensional systems with obstacles are considered. The fluid flow through a\nporous medium is simulated with the lattice Boltzmann method. It is\ndemonstrated that the CNNs are able to predict the porosity, permeability, and\ntortuosity with good accuracy. With the usage of the CNN models, the relation\nbetween $T$ and $\\varphi$ has been reproduced and compared with the empirical\nestimate. The analysis has been performed for the systems with $\\varphi \\in\n(0.37,0.99)$ which covers five orders of magnitude span for permeability $k \\in\n(0.78, 2.1\\times 10^5)$ and tortuosity $T \\in (1.03,2.74)$.",
    "text": "Predicting Porosity, Permeability, and Tortuosity of Porous Media from\n  Images by Deep Learning\n\nConvolutional neural networks (CNN) are utilized to encode the relation\nbetween initial configurations of obstacles and three fundamental quantities in\nporous media: porosity ($\\varphi$), permeability $k$, and tortuosity ($T$). The\ntwo-dimensional systems with obstacles are considered. The fluid flow through a\nporous medium is simulated with the lattice Boltzmann method. It is\ndemonstrated that the CNNs are able to predict the porosity, permeability, and\ntortuosity with good accuracy. With the usage of the CNN models, the relation\nbetween $T$ and $\\varphi$ has been reproduced and compared with the empirical\nestimate. The analysis has been performed for the systems with $\\varphi \\in\n(0.37,0.99)$ which covers five orders of magnitude span for permeability $k \\in\n(0.78, 2.1\\times 10^5)$ and tortuosity $T \\in (1.03,2.74)$."
  },
  {
    "id": "arxiv-21",
    "title": "Universal and Composite Hypothesis Testing via Mismatched Divergence",
    "abstract": "For the universal hypothesis testing problem, where the goal is to decide\nbetween the known null hypothesis distribution and some other unknown\ndistribution, Hoeffding proposed a universal test in the nineteen sixties.\nHoeffding's universal test statistic can be written in terms of\nKullback-Leibler (K-L) divergence between the empirical distribution of the\nobservations and the null hypothesis distribution. In this paper a modification\nof Hoeffding's test is considered based on a relaxation of the K-L divergence\ntest statistic, referred to as the mismatched divergence. The resulting\nmismatched test is shown to be a generalized likelihood-ratio test (GLRT) for\nthe case where the alternate distribution lies in a parametric family of the\ndistributions characterized by a finite dimensional parameter, i.e., it is a\nsolution to the corresponding composite hypothesis testing problem. For certain\nchoices of the alternate distribution, it is shown that both the Hoeffding test\nand the mismatched test have the same asymptotic performance in terms of error\nexponents. A consequence of this result is that the GLRT is optimal in\ndifferentiating a particular distribution from others in an exponential family.\nIt is also shown that the mismatched test has a significant advantage over the\nHoeffding test in terms of finite sample size performance. This advantage is\ndue to the difference in the asymptotic variances of the two test statistics\nunder the null hypothesis. In particular, the variance of the K-L divergence\ngrows linearly with the alphabet size, making the test impractical for\napplications involving large alphabet distributions. The variance of the\nmismatched divergence on the other hand grows linearly with the dimension of\nthe parameter space, and can hence be controlled through a prudent choice of\nthe function class defining the mismatched divergence.",
    "text": "Universal and Composite Hypothesis Testing via Mismatched Divergence\n\nFor the universal hypothesis testing problem, where the goal is to decide\nbetween the known null hypothesis distribution and some other unknown\ndistribution, Hoeffding proposed a universal test in the nineteen sixties.\nHoeffding's universal test statistic can be written in terms of\nKullback-Leibler (K-L) divergence between the empirical distribution of the\nobservations and the null hypothesis distribution. In this paper a modification\nof Hoeffding's test is considered based on a relaxation of the K-L divergence\ntest statistic, referred to as the mismatched divergence. The resulting\nmismatched test is shown to be a generalized likelihood-ratio test (GLRT) for\nthe case where the alternate distribution lies in a parametric family of the\ndistributions characterized by a finite dimensional parameter, i.e., it is a\nsolution to the corresponding composite hypothesis testing problem. For certain\nchoices of the alternate distribution, it is shown that both the Hoeffding test\nand the mismatched test have the same asymptotic performance in terms of error\nexponents. A consequence of this result is that the GLRT is optimal in\ndifferentiating a particular distribution from others in an exponential family.\nIt is also shown that the mismatched test has a significant advantage over the\nHoeffding test in terms of finite sample size performance. This advantage is\ndue to the difference in the asymptotic variances of the two test statistics\nunder the null hypothesis. In particular, the variance of the K-L divergence\ngrows linearly with the alphabet size, making the test impractical for\napplications involving large alphabet distributions. The variance of the\nmismatched divergence on the other hand grows linearly with the dimension of\nthe parameter space, and can hence be controlled through a prudent choice of\nthe function class defining the mismatched divergence."
  },
  {
    "id": "arxiv-22",
    "title": "Muppet: Massive Multi-task Representations with Pre-Finetuning",
    "abstract": "We propose pre-finetuning, an additional large-scale learning stage between\nlanguage model pre-training and fine-tuning. Pre-finetuning is massively\nmulti-task learning (around 50 datasets, over 4.8 million total labeled\nexamples), and is designed to encourage learning of representations that\ngeneralize better to many different tasks. We show that pre-finetuning\nconsistently improves performance for pretrained discriminators (e.g.~RoBERTa)\nand generation models (e.g.~BART) on a wide range of tasks (sentence\nprediction, commonsense reasoning, MRC, etc.), while also significantly\nimproving sample efficiency during fine-tuning. We also show that large-scale\nmulti-tasking is crucial; pre-finetuning can hurt performance when few tasks\nare used up until a critical point (usually above 15) after which performance\nimproves linearly in the number of tasks.",
    "text": "Muppet: Massive Multi-task Representations with Pre-Finetuning\n\nWe propose pre-finetuning, an additional large-scale learning stage between\nlanguage model pre-training and fine-tuning. Pre-finetuning is massively\nmulti-task learning (around 50 datasets, over 4.8 million total labeled\nexamples), and is designed to encourage learning of representations that\ngeneralize better to many different tasks. We show that pre-finetuning\nconsistently improves performance for pretrained discriminators (e.g.~RoBERTa)\nand generation models (e.g.~BART) on a wide range of tasks (sentence\nprediction, commonsense reasoning, MRC, etc.), while also significantly\nimproving sample efficiency during fine-tuning. We also show that large-scale\nmulti-tasking is crucial; pre-finetuning can hurt performance when few tasks\nare used up until a critical point (usually above 15) after which performance\nimproves linearly in the number of tasks."
  },
  {
    "id": "arxiv-23",
    "title": "Two Instances of Interpretable Neural Network for Universal\n  Approximations",
    "abstract": "This paper proposes two bottom-up interpretable neural network (NN)\nconstructions for universal approximation, namely Triangularly-constructed NN\n(TNN) and Semi-Quantized Activation NN (SQANN). Further notable properties are\n(1) resistance to catastrophic forgetting (2) existence of proof for\narbitrarily high accuracies (3) the ability to identify samples that are\nout-of-distribution through interpretable activation \"fingerprints\".",
    "text": "Two Instances of Interpretable Neural Network for Universal\n  Approximations\n\nThis paper proposes two bottom-up interpretable neural network (NN)\nconstructions for universal approximation, namely Triangularly-constructed NN\n(TNN) and Semi-Quantized Activation NN (SQANN). Further notable properties are\n(1) resistance to catastrophic forgetting (2) existence of proof for\narbitrarily high accuracies (3) the ability to identify samples that are\nout-of-distribution through interpretable activation \"fingerprints\"."
  },
  {
    "id": "arxiv-24",
    "title": "An Ensemble Approach for Automatic Structuring of Radiology Reports",
    "abstract": "Automatic structuring of electronic medical records is of high demand for\nclinical workflow solutions to facilitate extraction, storage, and querying of\npatient care information. However, developing a scalable solution is extremely\nchallenging, specifically for radiology reports, as most healthcare institutes\nuse either no template or department/institute specific templates. Moreover,\nradiologists' reporting style varies from one to another as sentences are\ntelegraphic and do not follow general English grammar rules. We present an\nensemble method that consolidates the predictions of three models, capturing\nvarious attributes of textual information for automatic labeling of sentences\nwith section labels. These three models are: 1) Focus Sentence model, capturing\ncontext of the target sentence; 2) Surrounding Context model, capturing the\nneighboring context of the target sentence; and finally, 3) Formatting/Layout\nmodel, aimed at learning report formatting cues. We utilize Bi-directional\nLSTMs, followed by sentence encoders, to acquire the context. Furthermore, we\ndefine several features that incorporate the structure of reports. We compare\nour proposed approach against multiple baselines and state-of-the-art\napproaches on a proprietary dataset as well as 100 manually annotated radiology\nnotes from the MIMIC-III dataset, which we are making publicly available. Our\nproposed approach significantly outperforms other approaches by achieving 97.1%\naccuracy.",
    "text": "An Ensemble Approach for Automatic Structuring of Radiology Reports\n\nAutomatic structuring of electronic medical records is of high demand for\nclinical workflow solutions to facilitate extraction, storage, and querying of\npatient care information. However, developing a scalable solution is extremely\nchallenging, specifically for radiology reports, as most healthcare institutes\nuse either no template or department/institute specific templates. Moreover,\nradiologists' reporting style varies from one to another as sentences are\ntelegraphic and do not follow general English grammar rules. We present an\nensemble method that consolidates the predictions of three models, capturing\nvarious attributes of textual information for automatic labeling of sentences\nwith section labels. These three models are: 1) Focus Sentence model, capturing\ncontext of the target sentence; 2) Surrounding Context model, capturing the\nneighboring context of the target sentence; and finally, 3) Formatting/Layout\nmodel, aimed at learning report formatting cues. We utilize Bi-directional\nLSTMs, followed by sentence encoders, to acquire the context. Furthermore, we\ndefine several features that incorporate the structure of reports. We compare\nour proposed approach against multiple baselines and state-of-the-art\napproaches on a proprietary dataset as well as 100 manually annotated radiology\nnotes from the MIMIC-III dataset, which we are making publicly available. Our\nproposed approach significantly outperforms other approaches by achieving 97.1%\naccuracy."
  },
  {
    "id": "arxiv-25",
    "title": "StressedNets: Efficient Feature Representations via Stress-induced\n  Evolutionary Synthesis of Deep Neural Networks",
    "abstract": "The computational complexity of leveraging deep neural networks for\nextracting deep feature representations is a significant barrier to its\nwidespread adoption, particularly for use in embedded devices. One particularly\npromising strategy to addressing the complexity issue is the notion of\nevolutionary synthesis of deep neural networks, which was demonstrated to\nsuccessfully produce highly efficient deep neural networks while retaining\nmodeling performance. Here, we further extend upon the evolutionary synthesis\nstrategy for achieving efficient feature extraction via the introduction of a\nstress-induced evolutionary synthesis framework, where stress signals are\nimposed upon the synapses of a deep neural network during training to induce\nstress and steer the synthesis process towards the production of more efficient\ndeep neural networks over successive generations and improved model fidelity at\na greater efficiency. The proposed stress-induced evolutionary synthesis\napproach is evaluated on a variety of different deep neural network\narchitectures (LeNet5, AlexNet, and YOLOv2) on different tasks (object\nclassification and object detection) to synthesize efficient StressedNets over\nmultiple generations. Experimental results demonstrate the efficacy of the\nproposed framework to synthesize StressedNets with significant improvement in\nnetwork architecture efficiency (e.g., 40x for AlexNet and 33x for YOLOv2) and\nspeed improvements (e.g., 5.5x inference speed-up for YOLOv2 on an Nvidia Tegra\nX1 mobile processor).",
    "text": "StressedNets: Efficient Feature Representations via Stress-induced\n  Evolutionary Synthesis of Deep Neural Networks\n\nThe computational complexity of leveraging deep neural networks for\nextracting deep feature representations is a significant barrier to its\nwidespread adoption, particularly for use in embedded devices. One particularly\npromising strategy to addressing the complexity issue is the notion of\nevolutionary synthesis of deep neural networks, which was demonstrated to\nsuccessfully produce highly efficient deep neural networks while retaining\nmodeling performance. Here, we further extend upon the evolutionary synthesis\nstrategy for achieving efficient feature extraction via the introduction of a\nstress-induced evolutionary synthesis framework, where stress signals are\nimposed upon the synapses of a deep neural network during training to induce\nstress and steer the synthesis process towards the production of more efficient\ndeep neural networks over successive generations and improved model fidelity at\na greater efficiency. The proposed stress-induced evolutionary synthesis\napproach is evaluated on a variety of different deep neural network\narchitectures (LeNet5, AlexNet, and YOLOv2) on different tasks (object\nclassification and object detection) to synthesize efficient StressedNets over\nmultiple generations. Experimental results demonstrate the efficacy of the\nproposed framework to synthesize StressedNets with significant improvement in\nnetwork architecture efficiency (e.g., 40x for AlexNet and 33x for YOLOv2) and\nspeed improvements (e.g., 5.5x inference speed-up for YOLOv2 on an Nvidia Tegra\nX1 mobile processor)."
  },
  {
    "id": "arxiv-26",
    "title": "kLog: A Language for Logical and Relational Learning with Kernels",
    "abstract": "We introduce kLog, a novel approach to statistical relational learning.\nUnlike standard approaches, kLog does not represent a probability distribution\ndirectly. It is rather a language to perform kernel-based learning on\nexpressive logical and relational representations. kLog allows users to specify\nlearning problems declaratively. It builds on simple but powerful concepts:\nlearning from interpretations, entity/relationship data modeling, logic\nprogramming, and deductive databases. Access by the kernel to the rich\nrepresentation is mediated by a technique we call graphicalization: the\nrelational representation is first transformed into a graph --- in particular,\na grounded entity/relationship diagram. Subsequently, a choice of graph kernel\ndefines the feature space. kLog supports mixed numerical and symbolic data, as\nwell as background knowledge in the form of Prolog or Datalog programs as in\ninductive logic programming systems. The kLog framework can be applied to\ntackle the same range of tasks that has made statistical relational learning so\npopular, including classification, regression, multitask learning, and\ncollective classification. We also report about empirical comparisons, showing\nthat kLog can be either more accurate, or much faster at the same level of\naccuracy, than Tilde and Alchemy. kLog is GPLv3 licensed and is available at\nhttp://klog.dinfo.unifi.it along with tutorials.",
    "text": "kLog: A Language for Logical and Relational Learning with Kernels\n\nWe introduce kLog, a novel approach to statistical relational learning.\nUnlike standard approaches, kLog does not represent a probability distribution\ndirectly. It is rather a language to perform kernel-based learning on\nexpressive logical and relational representations. kLog allows users to specify\nlearning problems declaratively. It builds on simple but powerful concepts:\nlearning from interpretations, entity/relationship data modeling, logic\nprogramming, and deductive databases. Access by the kernel to the rich\nrepresentation is mediated by a technique we call graphicalization: the\nrelational representation is first transformed into a graph --- in particular,\na grounded entity/relationship diagram. Subsequently, a choice of graph kernel\ndefines the feature space. kLog supports mixed numerical and symbolic data, as\nwell as background knowledge in the form of Prolog or Datalog programs as in\ninductive logic programming systems. The kLog framework can be applied to\ntackle the same range of tasks that has made statistical relational learning so\npopular, including classification, regression, multitask learning, and\ncollective classification. We also report about empirical comparisons, showing\nthat kLog can be either more accurate, or much faster at the same level of\naccuracy, than Tilde and Alchemy. kLog is GPLv3 licensed and is available at\nhttp://klog.dinfo.unifi.it along with tutorials."
  },
  {
    "id": "arxiv-27",
    "title": "Higher-Order Spectral Clustering under Superimposed Stochastic Block\n  Model",
    "abstract": "Higher-order motif structures and multi-vertex interactions are becoming\nincreasingly important in studies that aim to improve our understanding of\nfunctionalities and evolution patterns of networks. To elucidate the role of\nhigher-order structures in community detection problems over complex networks,\nwe introduce the notion of a Superimposed Stochastic Block Model (SupSBM). The\nmodel is based on a random graph framework in which certain higher-order\nstructures or subgraphs are generated through an independent hyperedge\ngeneration process, and are then replaced with graphs that are superimposed\nwith directed or undirected edges generated by an inhomogeneous random graph\nmodel. Consequently, the model introduces controlled dependencies between edges\nwhich allow for capturing more realistic network phenomena, namely strong local\nclustering in a sparse network, short average path length, and community\nstructure. We proceed to rigorously analyze the performance of a number of\nrecently proposed higher-order spectral clustering methods on the SupSBM. In\nparticular, we prove non-asymptotic upper bounds on the misclustering error of\nspectral community detection for a SupSBM setting in which triangles or\n3-uniform hyperedges are superimposed with undirected edges. As part of our\nanalysis, we also derive new bounds on the misclustering error of higher-order\nspectral clustering methods for the standard SBM and the 3-uniform hypergraph\nSBM. Furthermore, for a non-uniform hypergraph SBM model in which one directly\nobserves both edges and 3-uniform hyperedges, we obtain a criterion that\ndescribes when to perform spectral clustering based on edges and when on\nhyperedges, based on a function of hyperedge density and observation quality.",
    "text": "Higher-Order Spectral Clustering under Superimposed Stochastic Block\n  Model\n\nHigher-order motif structures and multi-vertex interactions are becoming\nincreasingly important in studies that aim to improve our understanding of\nfunctionalities and evolution patterns of networks. To elucidate the role of\nhigher-order structures in community detection problems over complex networks,\nwe introduce the notion of a Superimposed Stochastic Block Model (SupSBM). The\nmodel is based on a random graph framework in which certain higher-order\nstructures or subgraphs are generated through an independent hyperedge\ngeneration process, and are then replaced with graphs that are superimposed\nwith directed or undirected edges generated by an inhomogeneous random graph\nmodel. Consequently, the model introduces controlled dependencies between edges\nwhich allow for capturing more realistic network phenomena, namely strong local\nclustering in a sparse network, short average path length, and community\nstructure. We proceed to rigorously analyze the performance of a number of\nrecently proposed higher-order spectral clustering methods on the SupSBM. In\nparticular, we prove non-asymptotic upper bounds on the misclustering error of\nspectral community detection for a SupSBM setting in which triangles or\n3-uniform hyperedges are superimposed with undirected edges. As part of our\nanalysis, we also derive new bounds on the misclustering error of higher-order\nspectral clustering methods for the standard SBM and the 3-uniform hypergraph\nSBM. Furthermore, for a non-uniform hypergraph SBM model in which one directly\nobserves both edges and 3-uniform hyperedges, we obtain a criterion that\ndescribes when to perform spectral clustering based on edges and when on\nhyperedges, based on a function of hyperedge density and observation quality."
  },
  {
    "id": "arxiv-28",
    "title": "CertainNet: Sampling-free Uncertainty Estimation for Object Detection",
    "abstract": "Estimating the uncertainty of a neural network plays a fundamental role in\nsafety-critical settings. In perception for autonomous driving, measuring the\nuncertainty means providing additional calibrated information to downstream\ntasks, such as path planning, that can use it towards safe navigation. In this\nwork, we propose a novel sampling-free uncertainty estimation method for object\ndetection. We call it CertainNet, and it is the first to provide separate\nuncertainties for each output signal: objectness, class, location and size. To\nachieve this, we propose an uncertainty-aware heatmap, and exploit the\nneighboring bounding boxes provided by the detector at inference time. We\nevaluate the detection performance and the quality of the different uncertainty\nestimates separately, also with challenging out-of-domain samples: BDD100K and\nnuImages with models trained on KITTI. Additionally, we propose a new metric to\nevaluate location and size uncertainties. When transferring to unseen datasets,\nCertainNet generalizes substantially better than previous methods and an\nensemble, while being real-time and providing high quality and comprehensive\nuncertainty estimates.",
    "text": "CertainNet: Sampling-free Uncertainty Estimation for Object Detection\n\nEstimating the uncertainty of a neural network plays a fundamental role in\nsafety-critical settings. In perception for autonomous driving, measuring the\nuncertainty means providing additional calibrated information to downstream\ntasks, such as path planning, that can use it towards safe navigation. In this\nwork, we propose a novel sampling-free uncertainty estimation method for object\ndetection. We call it CertainNet, and it is the first to provide separate\nuncertainties for each output signal: objectness, class, location and size. To\nachieve this, we propose an uncertainty-aware heatmap, and exploit the\nneighboring bounding boxes provided by the detector at inference time. We\nevaluate the detection performance and the quality of the different uncertainty\nestimates separately, also with challenging out-of-domain samples: BDD100K and\nnuImages with models trained on KITTI. Additionally, we propose a new metric to\nevaluate location and size uncertainties. When transferring to unseen datasets,\nCertainNet generalizes substantially better than previous methods and an\nensemble, while being real-time and providing high quality and comprehensive\nuncertainty estimates."
  },
  {
    "id": "arxiv-29",
    "title": "LISA: Learning Interpretable Skill Abstractions from Language",
    "abstract": "Learning policies that effectually utilize language instructions in complex,\nmulti-task environments is an important problem in imitation learning. While it\nis possible to condition on the entire language instruction directly, such an\napproach could suffer from generalization issues. To encode complex\ninstructions into skills that can generalize to unseen instructions, we propose\nLearning Interpretable Skill Abstractions (LISA), a hierarchical imitation\nlearning framework that can learn diverse, interpretable skills from\nlanguage-conditioned demonstrations. LISA uses vector quantization to learn\ndiscrete skill codes that are highly correlated with language instructions and\nthe behavior of the learned policy. In navigation and robotic manipulation\nenvironments, LISA is able to outperform a strong non-hierarchical baseline in\nthe low data regime and compose learned skills to solve tasks containing unseen\nlong-range instructions. Our method demonstrates a more natural way to\ncondition on language in sequential decision-making problems and achieve\ninterpretable and controllable behavior with the learned skills.",
    "text": "LISA: Learning Interpretable Skill Abstractions from Language\n\nLearning policies that effectually utilize language instructions in complex,\nmulti-task environments is an important problem in imitation learning. While it\nis possible to condition on the entire language instruction directly, such an\napproach could suffer from generalization issues. To encode complex\ninstructions into skills that can generalize to unseen instructions, we propose\nLearning Interpretable Skill Abstractions (LISA), a hierarchical imitation\nlearning framework that can learn diverse, interpretable skills from\nlanguage-conditioned demonstrations. LISA uses vector quantization to learn\ndiscrete skill codes that are highly correlated with language instructions and\nthe behavior of the learned policy. In navigation and robotic manipulation\nenvironments, LISA is able to outperform a strong non-hierarchical baseline in\nthe low data regime and compose learned skills to solve tasks containing unseen\nlong-range instructions. Our method demonstrates a more natural way to\ncondition on language in sequential decision-making problems and achieve\ninterpretable and controllable behavior with the learned skills."
  },
  {
    "id": "arxiv-30",
    "title": "Primal-dual Learning for the Model-free Risk-constrained Linear\n  Quadratic Regulator",
    "abstract": "Risk-aware control, though with promise to tackle unexpected events, requires\na known exact dynamical model. In this work, we propose a model-free framework\nto learn a risk-aware controller with a focus on the linear system. We\nformulate it as a discrete-time infinite-horizon LQR problem with a state\npredictive variance constraint. To solve it, we parameterize the policy with a\nfeedback gain pair and leverage primal-dual methods to optimize it by solely\nusing data. We first study the optimization landscape of the Lagrangian\nfunction and establish the strong duality in spite of its non-convex nature.\nAlongside, we find that the Lagrangian function enjoys an important local\ngradient dominance property, which is then exploited to develop a convergent\nrandom search algorithm to learn the dual function. Furthermore, we propose a\nprimal-dual algorithm with global convergence to learn the optimal\npolicy-multiplier pair. Finally, we validate our results via simulations.",
    "text": "Primal-dual Learning for the Model-free Risk-constrained Linear\n  Quadratic Regulator\n\nRisk-aware control, though with promise to tackle unexpected events, requires\na known exact dynamical model. In this work, we propose a model-free framework\nto learn a risk-aware controller with a focus on the linear system. We\nformulate it as a discrete-time infinite-horizon LQR problem with a state\npredictive variance constraint. To solve it, we parameterize the policy with a\nfeedback gain pair and leverage primal-dual methods to optimize it by solely\nusing data. We first study the optimization landscape of the Lagrangian\nfunction and establish the strong duality in spite of its non-convex nature.\nAlongside, we find that the Lagrangian function enjoys an important local\ngradient dominance property, which is then exploited to develop a convergent\nrandom search algorithm to learn the dual function. Furthermore, we propose a\nprimal-dual algorithm with global convergence to learn the optimal\npolicy-multiplier pair. Finally, we validate our results via simulations."
  },
  {
    "id": "arxiv-31",
    "title": "Coded Computing for Federated Learning at the Edge",
    "abstract": "Federated Learning (FL) is an exciting new paradigm that enables training a\nglobal model from data generated locally at the client nodes, without moving\nclient data to a centralized server. Performance of FL in a multi-access edge\ncomputing (MEC) network suffers from slow convergence due to heterogeneity and\nstochastic fluctuations in compute power and communication link qualities\nacross clients. A recent work, Coded Federated Learning (CFL), proposes to\nmitigate stragglers and speed up training for linear regression tasks by\nassigning redundant computations at the MEC server. Coding redundancy in CFL is\ncomputed by exploiting statistical properties of compute and communication\ndelays. We develop CodedFedL that addresses the difficult task of extending CFL\nto distributed non-linear regression and classification problems with\nmultioutput labels. The key innovation of our work is to exploit distributed\nkernel embedding using random Fourier features that transforms the training\ntask into distributed linear regression. We provide an analytical solution for\nload allocation, and demonstrate significant performance gains for CodedFedL\nthrough experiments over benchmark datasets using practical network parameters.",
    "text": "Coded Computing for Federated Learning at the Edge\n\nFederated Learning (FL) is an exciting new paradigm that enables training a\nglobal model from data generated locally at the client nodes, without moving\nclient data to a centralized server. Performance of FL in a multi-access edge\ncomputing (MEC) network suffers from slow convergence due to heterogeneity and\nstochastic fluctuations in compute power and communication link qualities\nacross clients. A recent work, Coded Federated Learning (CFL), proposes to\nmitigate stragglers and speed up training for linear regression tasks by\nassigning redundant computations at the MEC server. Coding redundancy in CFL is\ncomputed by exploiting statistical properties of compute and communication\ndelays. We develop CodedFedL that addresses the difficult task of extending CFL\nto distributed non-linear regression and classification problems with\nmultioutput labels. The key innovation of our work is to exploit distributed\nkernel embedding using random Fourier features that transforms the training\ntask into distributed linear regression. We provide an analytical solution for\nload allocation, and demonstrate significant performance gains for CodedFedL\nthrough experiments over benchmark datasets using practical network parameters."
  },
  {
    "id": "arxiv-32",
    "title": "Bipartite Graph Reasoning GANs for Person Image Generation",
    "abstract": "We present a novel Bipartite Graph Reasoning GAN (BiGraphGAN) for the\nchallenging person image generation task. The proposed graph generator mainly\nconsists of two novel blocks that aim to model the pose-to-pose and\npose-to-image relations, respectively. Specifically, the proposed Bipartite\nGraph Reasoning (BGR) block aims to reason the crossing long-range relations\nbetween the source pose and the target pose in a bipartite graph, which\nmitigates some challenges caused by pose deformation. Moreover, we propose a\nnew Interaction-and-Aggregation (IA) block to effectively update and enhance\nthe feature representation capability of both person's shape and appearance in\nan interactive way. Experiments on two challenging and public datasets, i.e.,\nMarket-1501 and DeepFashion, show the effectiveness of the proposed BiGraphGAN\nin terms of objective quantitative scores and subjective visual realness. The\nsource code and trained models are available at\nhttps://github.com/Ha0Tang/BiGraphGAN.",
    "text": "Bipartite Graph Reasoning GANs for Person Image Generation\n\nWe present a novel Bipartite Graph Reasoning GAN (BiGraphGAN) for the\nchallenging person image generation task. The proposed graph generator mainly\nconsists of two novel blocks that aim to model the pose-to-pose and\npose-to-image relations, respectively. Specifically, the proposed Bipartite\nGraph Reasoning (BGR) block aims to reason the crossing long-range relations\nbetween the source pose and the target pose in a bipartite graph, which\nmitigates some challenges caused by pose deformation. Moreover, we propose a\nnew Interaction-and-Aggregation (IA) block to effectively update and enhance\nthe feature representation capability of both person's shape and appearance in\nan interactive way. Experiments on two challenging and public datasets, i.e.,\nMarket-1501 and DeepFashion, show the effectiveness of the proposed BiGraphGAN\nin terms of objective quantitative scores and subjective visual realness. The\nsource code and trained models are available at\nhttps://github.com/Ha0Tang/BiGraphGAN."
  },
  {
    "id": "arxiv-33",
    "title": "Chain of Thought Imitation with Procedure Cloning",
    "abstract": "Imitation learning aims to extract high-performance policies from logged\ndemonstrations of expert behavior. It is common to frame imitation learning as\na supervised learning problem in which one fits a function approximator to the\ninput-output mapping exhibited by the logged demonstrations (input observations\nto output actions). While the framing of imitation learning as a supervised\ninput-output learning problem allows for applicability in a wide variety of\nsettings, it is also an overly simplistic view of the problem in situations\nwhere the expert demonstrations provide much richer insight into expert\nbehavior. For example, applications such as path navigation, robot\nmanipulation, and strategy games acquire expert demonstrations via planning,\nsearch, or some other multi-step algorithm, revealing not just the output\naction to be imitated but also the procedure for how to determine this action.\nWhile these intermediate computations may use tools not available to the agent\nduring inference (e.g., environment simulators), they are nevertheless\ninformative as a way to explain an expert's mapping of state to actions. To\nproperly leverage expert procedure information without relying on the\nprivileged tools the expert may have used to perform the procedure, we propose\nprocedure cloning, which applies supervised sequence prediction to imitate the\nseries of expert computations. This way, procedure cloning learns not only what\nto do (i.e., the output action), but how and why to do it (i.e., the\nprocedure). Through empirical analysis on navigation, simulated robotic\nmanipulation, and game-playing environments, we show that imitating the\nintermediate computations of an expert's behavior enables procedure cloning to\nlearn policies exhibiting significant generalization to unseen environment\nconfigurations, including those configurations for which running the expert's\nprocedure directly is infeasible.",
    "text": "Chain of Thought Imitation with Procedure Cloning\n\nImitation learning aims to extract high-performance policies from logged\ndemonstrations of expert behavior. It is common to frame imitation learning as\na supervised learning problem in which one fits a function approximator to the\ninput-output mapping exhibited by the logged demonstrations (input observations\nto output actions). While the framing of imitation learning as a supervised\ninput-output learning problem allows for applicability in a wide variety of\nsettings, it is also an overly simplistic view of the problem in situations\nwhere the expert demonstrations provide much richer insight into expert\nbehavior. For example, applications such as path navigation, robot\nmanipulation, and strategy games acquire expert demonstrations via planning,\nsearch, or some other multi-step algorithm, revealing not just the output\naction to be imitated but also the procedure for how to determine this action.\nWhile these intermediate computations may use tools not available to the agent\nduring inference (e.g., environment simulators), they are nevertheless\ninformative as a way to explain an expert's mapping of state to actions. To\nproperly leverage expert procedure information without relying on the\nprivileged tools the expert may have used to perform the procedure, we propose\nprocedure cloning, which applies supervised sequence prediction to imitate the\nseries of expert computations. This way, procedure cloning learns not only what\nto do (i.e., the output action), but how and why to do it (i.e., the\nprocedure). Through empirical analysis on navigation, simulated robotic\nmanipulation, and game-playing environments, we show that imitating the\nintermediate computations of an expert's behavior enables procedure cloning to\nlearn policies exhibiting significant generalization to unseen environment\nconfigurations, including those configurations for which running the expert's\nprocedure directly is infeasible."
  },
  {
    "id": "arxiv-34",
    "title": "Simplifying approach to Node Classification in Graph Neural Networks",
    "abstract": "Graph Neural Networks have become one of the indispensable tools to learn\nfrom graph-structured data, and their usefulness has been shown in wide variety\nof tasks. In recent years, there have been tremendous improvements in\narchitecture design, resulting in better performance on various prediction\ntasks. In general, these neural architectures combine node feature aggregation\nand feature transformation using learnable weight matrix in the same layer.\nThis makes it challenging to analyze the importance of node features aggregated\nfrom various hops and the expressiveness of the neural network layers. As\ndifferent graph datasets show varying levels of homophily and heterophily in\nfeatures and class label distribution, it becomes essential to understand which\nfeatures are important for the prediction tasks without any prior information.\nIn this work, we decouple the node feature aggregation step and depth of graph\nneural network, and empirically analyze how different aggregated features play\na role in prediction performance. We show that not all features generated via\naggregation steps are useful, and often using these less informative features\ncan be detrimental to the performance of the GNN model. Through our\nexperiments, we show that learning certain subsets of these features can lead\nto better performance on wide variety of datasets. We propose to use softmax as\na regularizer and \"soft-selector\" of features aggregated from neighbors at\ndifferent hop distances; and L2-Normalization over GNN layers. Combining these\ntechniques, we present a simple and shallow model, Feature Selection Graph\nNeural Network (FSGNN), and show empirically that the proposed model achieves\ncomparable or even higher accuracy than state-of-the-art GNN models in nine\nbenchmark datasets for the node classification task, with remarkable\nimprovements up to 51.1%.",
    "text": "Simplifying approach to Node Classification in Graph Neural Networks\n\nGraph Neural Networks have become one of the indispensable tools to learn\nfrom graph-structured data, and their usefulness has been shown in wide variety\nof tasks. In recent years, there have been tremendous improvements in\narchitecture design, resulting in better performance on various prediction\ntasks. In general, these neural architectures combine node feature aggregation\nand feature transformation using learnable weight matrix in the same layer.\nThis makes it challenging to analyze the importance of node features aggregated\nfrom various hops and the expressiveness of the neural network layers. As\ndifferent graph datasets show varying levels of homophily and heterophily in\nfeatures and class label distribution, it becomes essential to understand which\nfeatures are important for the prediction tasks without any prior information.\nIn this work, we decouple the node feature aggregation step and depth of graph\nneural network, and empirically analyze how different aggregated features play\na role in prediction performance. We show that not all features generated via\naggregation steps are useful, and often using these less informative features\ncan be detrimental to the performance of the GNN model. Through our\nexperiments, we show that learning certain subsets of these features can lead\nto better performance on wide variety of datasets. We propose to use softmax as\na regularizer and \"soft-selector\" of features aggregated from neighbors at\ndifferent hop distances; and L2-Normalization over GNN layers. Combining these\ntechniques, we present a simple and shallow model, Feature Selection Graph\nNeural Network (FSGNN), and show empirically that the proposed model achieves\ncomparable or even higher accuracy than state-of-the-art GNN models in nine\nbenchmark datasets for the node classification task, with remarkable\nimprovements up to 51.1%."
  },
  {
    "id": "arxiv-35",
    "title": "A Unified Joint Maximum Mean Discrepancy for Domain Adaptation",
    "abstract": "Domain adaptation has received a lot of attention in recent years, and many\nalgorithms have been proposed with impressive progress. However, it is still\nnot fully explored concerning the joint probability distribution (P(X, Y))\ndistance for this problem, since its empirical estimation derived from the\nmaximum mean discrepancy (joint maximum mean discrepancy, JMMD) will involve\ncomplex tensor-product operator that is hard to manipulate. To solve this\nissue, this paper theoretically derives a unified form of JMMD that is easy to\noptimize, and proves that the marginal, class conditional and weighted class\nconditional probability distribution distances are our special cases with\ndifferent label kernels, among which the weighted class conditional one not\nonly can realize feature alignment across domains in the category level, but\nalso deal with imbalance dataset using the class prior probabilities. From the\nrevealed unified JMMD, we illustrate that JMMD degrades the feature-label\ndependence (discriminability) that benefits to classification, and it is\nsensitive to the label distribution shift when the label kernel is the weighted\nclass conditional one. Therefore, we leverage Hilbert Schmidt independence\ncriterion and propose a novel MMD matrix to promote the dependence, and devise\na novel label kernel that is robust to label distribution shift. Finally, we\nconduct extensive experiments on several cross-domain datasets to demonstrate\nthe validity and effectiveness of the revealed theoretical results.",
    "text": "A Unified Joint Maximum Mean Discrepancy for Domain Adaptation\n\nDomain adaptation has received a lot of attention in recent years, and many\nalgorithms have been proposed with impressive progress. However, it is still\nnot fully explored concerning the joint probability distribution (P(X, Y))\ndistance for this problem, since its empirical estimation derived from the\nmaximum mean discrepancy (joint maximum mean discrepancy, JMMD) will involve\ncomplex tensor-product operator that is hard to manipulate. To solve this\nissue, this paper theoretically derives a unified form of JMMD that is easy to\noptimize, and proves that the marginal, class conditional and weighted class\nconditional probability distribution distances are our special cases with\ndifferent label kernels, among which the weighted class conditional one not\nonly can realize feature alignment across domains in the category level, but\nalso deal with imbalance dataset using the class prior probabilities. From the\nrevealed unified JMMD, we illustrate that JMMD degrades the feature-label\ndependence (discriminability) that benefits to classification, and it is\nsensitive to the label distribution shift when the label kernel is the weighted\nclass conditional one. Therefore, we leverage Hilbert Schmidt independence\ncriterion and propose a novel MMD matrix to promote the dependence, and devise\na novel label kernel that is robust to label distribution shift. Finally, we\nconduct extensive experiments on several cross-domain datasets to demonstrate\nthe validity and effectiveness of the revealed theoretical results."
  },
  {
    "id": "arxiv-36",
    "title": "Control-Aware Representations for Model-based Reinforcement Learning",
    "abstract": "A major challenge in modern reinforcement learning (RL) is efficient control\nof dynamical systems from high-dimensional sensory observations. Learning\ncontrollable embedding (LCE) is a promising approach that addresses this\nchallenge by embedding the observations into a lower-dimensional latent space,\nestimating the latent dynamics, and utilizing it to perform control in the\nlatent space. Two important questions in this area are how to learn a\nrepresentation that is amenable to the control problem at hand, and how to\nachieve an end-to-end framework for representation learning and control. In\nthis paper, we take a few steps towards addressing these questions. We first\nformulate a LCE model to learn representations that are suitable to be used by\na policy iteration style algorithm in the latent space. We call this model\ncontrol-aware representation learning (CARL). We derive a loss function for\nCARL that has close connection to the prediction, consistency, and curvature\n(PCC) principle for representation learning. We derive three implementations of\nCARL. In the offline implementation, we replace the locally-linear control\nalgorithm (e.g.,~iLQR) used by the existing LCE methods with a RL algorithm,\nnamely model-based soft actor-critic, and show that it results in significant\nimprovement. In online CARL, we interleave representation learning and control,\nand demonstrate further gain in performance. Finally, we propose value-guided\nCARL, a variation in which we optimize a weighted version of the CARL loss\nfunction, where the weights depend on the TD-error of the current policy. We\nevaluate the proposed algorithms by extensive experiments on benchmark tasks\nand compare them with several LCE baselines.",
    "text": "Control-Aware Representations for Model-based Reinforcement Learning\n\nA major challenge in modern reinforcement learning (RL) is efficient control\nof dynamical systems from high-dimensional sensory observations. Learning\ncontrollable embedding (LCE) is a promising approach that addresses this\nchallenge by embedding the observations into a lower-dimensional latent space,\nestimating the latent dynamics, and utilizing it to perform control in the\nlatent space. Two important questions in this area are how to learn a\nrepresentation that is amenable to the control problem at hand, and how to\nachieve an end-to-end framework for representation learning and control. In\nthis paper, we take a few steps towards addressing these questions. We first\nformulate a LCE model to learn representations that are suitable to be used by\na policy iteration style algorithm in the latent space. We call this model\ncontrol-aware representation learning (CARL). We derive a loss function for\nCARL that has close connection to the prediction, consistency, and curvature\n(PCC) principle for representation learning. We derive three implementations of\nCARL. In the offline implementation, we replace the locally-linear control\nalgorithm (e.g.,~iLQR) used by the existing LCE methods with a RL algorithm,\nnamely model-based soft actor-critic, and show that it results in significant\nimprovement. In online CARL, we interleave representation learning and control,\nand demonstrate further gain in performance. Finally, we propose value-guided\nCARL, a variation in which we optimize a weighted version of the CARL loss\nfunction, where the weights depend on the TD-error of the current policy. We\nevaluate the proposed algorithms by extensive experiments on benchmark tasks\nand compare them with several LCE baselines."
  },
  {
    "id": "arxiv-37",
    "title": "Transfer Learning for Neural Semantic Parsing",
    "abstract": "The goal of semantic parsing is to map natural language to a machine\ninterpretable meaning representation language (MRL). One of the constraints\nthat limits full exploration of deep learning technologies for semantic parsing\nis the lack of sufficient annotation training data. In this paper, we propose\nusing sequence-to-sequence in a multi-task setup for semantic parsing with a\nfocus on transfer learning. We explore three multi-task architectures for\nsequence-to-sequence modeling and compare their performance with an\nindependently trained model. Our experiments show that the multi-task setup\naids transfer learning from an auxiliary task with large labeled data to a\ntarget task with smaller labeled data. We see absolute accuracy gains ranging\nfrom 1.0% to 4.4% in our in- house data set, and we also see good gains ranging\nfrom 2.5% to 7.0% on the ATIS semantic parsing tasks with syntactic and\nsemantic auxiliary tasks.",
    "text": "Transfer Learning for Neural Semantic Parsing\n\nThe goal of semantic parsing is to map natural language to a machine\ninterpretable meaning representation language (MRL). One of the constraints\nthat limits full exploration of deep learning technologies for semantic parsing\nis the lack of sufficient annotation training data. In this paper, we propose\nusing sequence-to-sequence in a multi-task setup for semantic parsing with a\nfocus on transfer learning. We explore three multi-task architectures for\nsequence-to-sequence modeling and compare their performance with an\nindependently trained model. Our experiments show that the multi-task setup\naids transfer learning from an auxiliary task with large labeled data to a\ntarget task with smaller labeled data. We see absolute accuracy gains ranging\nfrom 1.0% to 4.4% in our in- house data set, and we also see good gains ranging\nfrom 2.5% to 7.0% on the ATIS semantic parsing tasks with syntactic and\nsemantic auxiliary tasks."
  },
  {
    "id": "arxiv-38",
    "title": "Minimax Distribution Estimation in Wasserstein Distance",
    "abstract": "The Wasserstein metric is an important measure of distance between\nprobability distributions, with applications in machine learning, statistics,\nprobability theory, and data analysis. This paper provides upper and lower\nbounds on statistical minimax rates for the problem of estimating a probability\ndistribution under Wasserstein loss, using only metric properties, such as\ncovering and packing numbers, of the sample space, and weak moment assumptions\non the probability distributions.",
    "text": "Minimax Distribution Estimation in Wasserstein Distance\n\nThe Wasserstein metric is an important measure of distance between\nprobability distributions, with applications in machine learning, statistics,\nprobability theory, and data analysis. This paper provides upper and lower\nbounds on statistical minimax rates for the problem of estimating a probability\ndistribution under Wasserstein loss, using only metric properties, such as\ncovering and packing numbers, of the sample space, and weak moment assumptions\non the probability distributions."
  },
  {
    "id": "arxiv-39",
    "title": "Recurrent Neural Networks for Time Series Forecasting",
    "abstract": "Time series forecasting is difficult. It is difficult even for recurrent\nneural networks with their inherent ability to learn sequentiality. This\narticle presents a recurrent neural network based time series forecasting\nframework covering feature engineering, feature importances, point and interval\npredictions, and forecast evaluation. The description of the method is followed\nby an empirical study using both LSTM and GRU networks.",
    "text": "Recurrent Neural Networks for Time Series Forecasting\n\nTime series forecasting is difficult. It is difficult even for recurrent\nneural networks with their inherent ability to learn sequentiality. This\narticle presents a recurrent neural network based time series forecasting\nframework covering feature engineering, feature importances, point and interval\npredictions, and forecast evaluation. The description of the method is followed\nby an empirical study using both LSTM and GRU networks."
  },
  {
    "id": "arxiv-40",
    "title": "dynnode2vec: Scalable Dynamic Network Embedding",
    "abstract": "Network representation learning in low dimensional vector space has attracted\nconsiderable attention in both academic and industrial domains. Most real-world\nnetworks are dynamic with addition/deletion of nodes and edges. The existing\ngraph embedding methods are designed for static networks and they cannot\ncapture evolving patterns in a large dynamic network. In this paper, we propose\na dynamic embedding method, dynnode2vec, based on the well-known graph\nembedding method node2vec. Node2vec is a random walk based embedding method for\nstatic networks. Applying static network embedding in dynamic settings has two\ncrucial problems: 1) Generating random walks for every time step is time\nconsuming 2) Embedding vector spaces in each timestamp are different. In order\nto tackle these challenges, dynnode2vec uses evolving random walks and\ninitializes the current graph embedding with previous embedding vectors. We\ndemonstrate the advantages of the proposed dynamic network embedding by\nconducting empirical evaluations on several large dynamic network datasets.",
    "text": "dynnode2vec: Scalable Dynamic Network Embedding\n\nNetwork representation learning in low dimensional vector space has attracted\nconsiderable attention in both academic and industrial domains. Most real-world\nnetworks are dynamic with addition/deletion of nodes and edges. The existing\ngraph embedding methods are designed for static networks and they cannot\ncapture evolving patterns in a large dynamic network. In this paper, we propose\na dynamic embedding method, dynnode2vec, based on the well-known graph\nembedding method node2vec. Node2vec is a random walk based embedding method for\nstatic networks. Applying static network embedding in dynamic settings has two\ncrucial problems: 1) Generating random walks for every time step is time\nconsuming 2) Embedding vector spaces in each timestamp are different. In order\nto tackle these challenges, dynnode2vec uses evolving random walks and\ninitializes the current graph embedding with previous embedding vectors. We\ndemonstrate the advantages of the proposed dynamic network embedding by\nconducting empirical evaluations on several large dynamic network datasets."
  },
  {
    "id": "arxiv-41",
    "title": "Explainable Text-Driven Neural Network for Stock Prediction",
    "abstract": "It has been shown that financial news leads to the fluctuation of stock\nprices. However, previous work on news-driven financial market prediction\nfocused only on predicting stock price movement without providing an\nexplanation. In this paper, we propose a dual-layer attention-based neural\nnetwork to address this issue. In the initial stage, we introduce a\nknowledge-based method to adaptively extract relevant financial news. Then, we\nuse input attention to pay more attention to the more influential news and\nconcatenate the day embeddings with the output of the news representation.\nFinally, we use an output attention mechanism to allocate different weights to\ndifferent days in terms of their contribution to stock price movement. Thorough\nempirical studies based upon historical prices of several individual stocks\ndemonstrate the superiority of our proposed method in stock price prediction\ncompared to state-of-the-art methods.",
    "text": "Explainable Text-Driven Neural Network for Stock Prediction\n\nIt has been shown that financial news leads to the fluctuation of stock\nprices. However, previous work on news-driven financial market prediction\nfocused only on predicting stock price movement without providing an\nexplanation. In this paper, we propose a dual-layer attention-based neural\nnetwork to address this issue. In the initial stage, we introduce a\nknowledge-based method to adaptively extract relevant financial news. Then, we\nuse input attention to pay more attention to the more influential news and\nconcatenate the day embeddings with the output of the news representation.\nFinally, we use an output attention mechanism to allocate different weights to\ndifferent days in terms of their contribution to stock price movement. Thorough\nempirical studies based upon historical prices of several individual stocks\ndemonstrate the superiority of our proposed method in stock price prediction\ncompared to state-of-the-art methods."
  },
  {
    "id": "arxiv-42",
    "title": "A Machine Learning-Based Migration Strategy for Virtual Network Function\n  Instances",
    "abstract": "With the growing demand for data connectivity, network service providers are\nfaced with the task of reducing their capital and operational expenses while\nsimultaneously improving network performance and addressing the increased\ndemand. Although Network Function Virtualization (NFV) has been identified as a\npromising solution, several challenges must be addressed to ensure its\nfeasibility. In this paper, we address the Virtual Network Function (VNF)\nmigration problem by developing the VNF Neural Network for Instance Migration\n(VNNIM), a migration strategy for VNF instances. The performance of VNNIM is\nfurther improved through the optimization of the learning rate hyperparameter\nthrough particle swarm optimization. Results show that the VNNIM is very\neffective in predicting the post-migration server exhibiting a binary accuracy\nof 99.07% and a delay difference distribution that is centered around a mean of\nzero when compared to the optimization model. The greatest advantage of VNNIM,\nhowever, is its run-time efficiency highlighted through a run-time analysis.",
    "text": "A Machine Learning-Based Migration Strategy for Virtual Network Function\n  Instances\n\nWith the growing demand for data connectivity, network service providers are\nfaced with the task of reducing their capital and operational expenses while\nsimultaneously improving network performance and addressing the increased\ndemand. Although Network Function Virtualization (NFV) has been identified as a\npromising solution, several challenges must be addressed to ensure its\nfeasibility. In this paper, we address the Virtual Network Function (VNF)\nmigration problem by developing the VNF Neural Network for Instance Migration\n(VNNIM), a migration strategy for VNF instances. The performance of VNNIM is\nfurther improved through the optimization of the learning rate hyperparameter\nthrough particle swarm optimization. Results show that the VNNIM is very\neffective in predicting the post-migration server exhibiting a binary accuracy\nof 99.07% and a delay difference distribution that is centered around a mean of\nzero when compared to the optimization model. The greatest advantage of VNNIM,\nhowever, is its run-time efficiency highlighted through a run-time analysis."
  },
  {
    "id": "arxiv-43",
    "title": "Learning under $p$-Tampering Attacks",
    "abstract": "Recently, Mahloujifar and Mahmoody (TCC'17) studied attacks against learning\nalgorithms using a special case of Valiant's malicious noise, called\n$p$-tampering, in which the adversary gets to change any training example with\nindependent probability $p$ but is limited to only choose malicious examples\nwith correct labels. They obtained $p$-tampering attacks that increase the\nerror probability in the so called targeted poisoning model in which the\nadversary's goal is to increase the loss of the trained hypothesis over a\nparticular test example. At the heart of their attack was an efficient\nalgorithm to bias the expected value of any bounded real-output function\nthrough $p$-tampering.\n  In this work, we present new biasing attacks for increasing the expected\nvalue of bounded real-valued functions. Our improved biasing attacks, directly\nimply improved $p$-tampering attacks against learners in the targeted poisoning\nmodel. As a bonus, our attacks come with considerably simpler analysis. We also\nstudy the possibility of PAC learning under $p$-tampering attacks in the\nnon-targeted (aka indiscriminate) setting where the adversary's goal is to\nincrease the risk of the generated hypothesis (for a random test example). We\nshow that PAC learning is possible under $p$-tampering poisoning attacks\nessentially whenever it is possible in the realizable setting without the\nattacks. We further show that PAC learning under \"correct-label\" adversarial\nnoise is not possible in general, if the adversary could choose the (still\nlimited to only $p$ fraction of) tampered examples that she substitutes with\nadversarially chosen ones. Our formal model for such \"bounded-budget\" tampering\nattackers is inspired by the notions of (strong) adaptive corruption in secure\nmulti-party computation.",
    "text": "Learning under $p$-Tampering Attacks\n\nRecently, Mahloujifar and Mahmoody (TCC'17) studied attacks against learning\nalgorithms using a special case of Valiant's malicious noise, called\n$p$-tampering, in which the adversary gets to change any training example with\nindependent probability $p$ but is limited to only choose malicious examples\nwith correct labels. They obtained $p$-tampering attacks that increase the\nerror probability in the so called targeted poisoning model in which the\nadversary's goal is to increase the loss of the trained hypothesis over a\nparticular test example. At the heart of their attack was an efficient\nalgorithm to bias the expected value of any bounded real-output function\nthrough $p$-tampering.\n  In this work, we present new biasing attacks for increasing the expected\nvalue of bounded real-valued functions. Our improved biasing attacks, directly\nimply improved $p$-tampering attacks against learners in the targeted poisoning\nmodel. As a bonus, our attacks come with considerably simpler analysis. We also\nstudy the possibility of PAC learning under $p$-tampering attacks in the\nnon-targeted (aka indiscriminate) setting where the adversary's goal is to\nincrease the risk of the generated hypothesis (for a random test example). We\nshow that PAC learning is possible under $p$-tampering poisoning attacks\nessentially whenever it is possible in the realizable setting without the\nattacks. We further show that PAC learning under \"correct-label\" adversarial\nnoise is not possible in general, if the adversary could choose the (still\nlimited to only $p$ fraction of) tampered examples that she substitutes with\nadversarially chosen ones. Our formal model for such \"bounded-budget\" tampering\nattackers is inspired by the notions of (strong) adaptive corruption in secure\nmulti-party computation."
  },
  {
    "id": "arxiv-44",
    "title": "Learning meters of Arabic and English poems with Recurrent Neural\n  Networks: a step forward for language understanding and synthesis",
    "abstract": "Recognizing a piece of writing as a poem or prose is usually easy for the\nmajority of people; however, only specialists can determine which meter a poem\nbelongs to. In this paper, we build Recurrent Neural Network (RNN) models that\ncan classify poems according to their meters from plain text. The input text is\nencoded at the character level and directly fed to the models without feature\nhandcrafting. This is a step forward for machine understanding and synthesis of\nlanguages in general, and Arabic language in particular. Among the 16 poem\nmeters of Arabic and the 4 meters of English the networks were able to\ncorrectly classify poem with an overall accuracy of 96.38\\% and 82.31\\%\nrespectively. The poem datasets used to conduct this research were massive,\nover 1.5 million of verses, and were crawled from different nontechnical\nsources, almost Arabic and English literature sites, and in different\nheterogeneous and unstructured formats. These datasets are now made publicly\navailable in clean, structured, and documented format for other future\nresearch. To the best of the authors' knowledge, this research is the first to\naddress classifying poem meters in a machine learning approach, in general, and\nin RNN featureless based approach, in particular. In addition, the dataset is\nthe first publicly available dataset ready for the purpose of future\ncomputational research.",
    "text": "Learning meters of Arabic and English poems with Recurrent Neural\n  Networks: a step forward for language understanding and synthesis\n\nRecognizing a piece of writing as a poem or prose is usually easy for the\nmajority of people; however, only specialists can determine which meter a poem\nbelongs to. In this paper, we build Recurrent Neural Network (RNN) models that\ncan classify poems according to their meters from plain text. The input text is\nencoded at the character level and directly fed to the models without feature\nhandcrafting. This is a step forward for machine understanding and synthesis of\nlanguages in general, and Arabic language in particular. Among the 16 poem\nmeters of Arabic and the 4 meters of English the networks were able to\ncorrectly classify poem with an overall accuracy of 96.38\\% and 82.31\\%\nrespectively. The poem datasets used to conduct this research were massive,\nover 1.5 million of verses, and were crawled from different nontechnical\nsources, almost Arabic and English literature sites, and in different\nheterogeneous and unstructured formats. These datasets are now made publicly\navailable in clean, structured, and documented format for other future\nresearch. To the best of the authors' knowledge, this research is the first to\naddress classifying poem meters in a machine learning approach, in general, and\nin RNN featureless based approach, in particular. In addition, the dataset is\nthe first publicly available dataset ready for the purpose of future\ncomputational research."
  },
  {
    "id": "arxiv-45",
    "title": "Zero-Shot Recognition via Optimal Transport",
    "abstract": "We propose an optimal transport (OT) framework for generalized zero-shot\nlearning (GZSL), seeking to distinguish samples for both seen and unseen\nclasses, with the assist of auxiliary attributes. The discrepancy between\nfeatures and attributes is minimized by solving an optimal transport problem.\n{Specifically, we build a conditional generative model to generate features\nfrom seen-class attributes, and establish an optimal transport between the\ndistribution of the generated features and that of the real features.} The\ngenerative model and the optimal transport are optimized iteratively with an\nattribute-based regularizer, that further enhances the discriminative power of\nthe generated features. A classifier is learned based on the features generated\nfor both the seen and unseen classes. In addition to generalized zero-shot\nlearning, our framework is also applicable to standard and transductive ZSL\nproblems. Experiments show that our optimal transport-based method outperforms\nstate-of-the-art methods on several benchmark datasets.",
    "text": "Zero-Shot Recognition via Optimal Transport\n\nWe propose an optimal transport (OT) framework for generalized zero-shot\nlearning (GZSL), seeking to distinguish samples for both seen and unseen\nclasses, with the assist of auxiliary attributes. The discrepancy between\nfeatures and attributes is minimized by solving an optimal transport problem.\n{Specifically, we build a conditional generative model to generate features\nfrom seen-class attributes, and establish an optimal transport between the\ndistribution of the generated features and that of the real features.} The\ngenerative model and the optimal transport are optimized iteratively with an\nattribute-based regularizer, that further enhances the discriminative power of\nthe generated features. A classifier is learned based on the features generated\nfor both the seen and unseen classes. In addition to generalized zero-shot\nlearning, our framework is also applicable to standard and transductive ZSL\nproblems. Experiments show that our optimal transport-based method outperforms\nstate-of-the-art methods on several benchmark datasets."
  },
  {
    "id": "arxiv-46",
    "title": "Addressing Privacy Threats from Machine Learning",
    "abstract": "Every year at NeurIPS, machine learning researchers gather and discuss\nexciting applications of machine learning in areas such as public health,\ndisaster response, climate change, education, and more. However, many of these\nsame researchers are expressing growing concern about applications of machine\nlearning for surveillance (Nanayakkara et al., 2021). This paper presents a\nbrief overview of strategies for resisting these surveillance technologies and\ncalls for greater collaboration between machine learning and human-computer\ninteraction researchers to address the threats that these technologies pose.",
    "text": "Addressing Privacy Threats from Machine Learning\n\nEvery year at NeurIPS, machine learning researchers gather and discuss\nexciting applications of machine learning in areas such as public health,\ndisaster response, climate change, education, and more. However, many of these\nsame researchers are expressing growing concern about applications of machine\nlearning for surveillance (Nanayakkara et al., 2021). This paper presents a\nbrief overview of strategies for resisting these surveillance technologies and\ncalls for greater collaboration between machine learning and human-computer\ninteraction researchers to address the threats that these technologies pose."
  },
  {
    "id": "arxiv-47",
    "title": "OpenLORIS-Object: A Robotic Vision Dataset and Benchmark for Lifelong\n  Deep Learning",
    "abstract": "The recent breakthroughs in computer vision have benefited from the\navailability of large representative datasets (e.g. ImageNet and COCO) for\ntraining. Yet, robotic vision poses unique challenges for applying visual\nalgorithms developed from these standard computer vision datasets due to their\nimplicit assumption over non-varying distributions for a fixed set of tasks.\nFully retraining models each time a new task becomes available is infeasible\ndue to computational, storage and sometimes privacy issues, while na\\\"{i}ve\nincremental strategies have been shown to suffer from catastrophic forgetting.\nIt is crucial for the robots to operate continuously under open-set and\ndetrimental conditions with adaptive visual perceptual systems, where lifelong\nlearning is a fundamental capability. However, very few datasets and benchmarks\nare available to evaluate and compare emerging techniques. To fill this gap, we\nprovide a new lifelong robotic vision dataset (\"OpenLORIS-Object\") collected\nvia RGB-D cameras. The dataset embeds the challenges faced by a robot in the\nreal-life application and provides new benchmarks for validating lifelong\nobject recognition algorithms. Moreover, we have provided a testbed of $9$\nstate-of-the-art lifelong learning algorithms. Each of them involves $48$ tasks\nwith $4$ evaluation metrics over the OpenLORIS-Object dataset. The results\ndemonstrate that the object recognition task in the ever-changing difficulty\nenvironments is far from being solved and the bottlenecks are at the\nforward/backward transfer designs. Our dataset and benchmark are publicly\navailable at at\n\\href{https://lifelong-robotic-vision.github.io/dataset/object}{\\underline{https://lifelong-robotic-vision.github.io/dataset/object}}.",
    "text": "OpenLORIS-Object: A Robotic Vision Dataset and Benchmark for Lifelong\n  Deep Learning\n\nThe recent breakthroughs in computer vision have benefited from the\navailability of large representative datasets (e.g. ImageNet and COCO) for\ntraining. Yet, robotic vision poses unique challenges for applying visual\nalgorithms developed from these standard computer vision datasets due to their\nimplicit assumption over non-varying distributions for a fixed set of tasks.\nFully retraining models each time a new task becomes available is infeasible\ndue to computational, storage and sometimes privacy issues, while na\\\"{i}ve\nincremental strategies have been shown to suffer from catastrophic forgetting.\nIt is crucial for the robots to operate continuously under open-set and\ndetrimental conditions with adaptive visual perceptual systems, where lifelong\nlearning is a fundamental capability. However, very few datasets and benchmarks\nare available to evaluate and compare emerging techniques. To fill this gap, we\nprovide a new lifelong robotic vision dataset (\"OpenLORIS-Object\") collected\nvia RGB-D cameras. The dataset embeds the challenges faced by a robot in the\nreal-life application and provides new benchmarks for validating lifelong\nobject recognition algorithms. Moreover, we have provided a testbed of $9$\nstate-of-the-art lifelong learning algorithms. Each of them involves $48$ tasks\nwith $4$ evaluation metrics over the OpenLORIS-Object dataset. The results\ndemonstrate that the object recognition task in the ever-changing difficulty\nenvironments is far from being solved and the bottlenecks are at the\nforward/backward transfer designs. Our dataset and benchmark are publicly\navailable at at\n\\href{https://lifelong-robotic-vision.github.io/dataset/object}{\\underline{https://lifelong-robotic-vision.github.io/dataset/object}}."
  },
  {
    "id": "arxiv-48",
    "title": "Efficient Defenses Against Adversarial Attacks",
    "abstract": "Following the recent adoption of deep neural networks (DNN) accross a wide\nrange of applications, adversarial attacks against these models have proven to\nbe an indisputable threat. Adversarial samples are crafted with a deliberate\nintention of undermining a system. In the case of DNNs, the lack of better\nunderstanding of their working has prevented the development of efficient\ndefenses. In this paper, we propose a new defense method based on practical\nobservations which is easy to integrate into models and performs better than\nstate-of-the-art defenses. Our proposed solution is meant to reinforce the\nstructure of a DNN, making its prediction more stable and less likely to be\nfooled by adversarial samples. We conduct an extensive experimental study\nproving the efficiency of our method against multiple attacks, comparing it to\nnumerous defenses, both in white-box and black-box setups. Additionally, the\nimplementation of our method brings almost no overhead to the training\nprocedure, while maintaining the prediction performance of the original model\non clean samples.",
    "text": "Efficient Defenses Against Adversarial Attacks\n\nFollowing the recent adoption of deep neural networks (DNN) accross a wide\nrange of applications, adversarial attacks against these models have proven to\nbe an indisputable threat. Adversarial samples are crafted with a deliberate\nintention of undermining a system. In the case of DNNs, the lack of better\nunderstanding of their working has prevented the development of efficient\ndefenses. In this paper, we propose a new defense method based on practical\nobservations which is easy to integrate into models and performs better than\nstate-of-the-art defenses. Our proposed solution is meant to reinforce the\nstructure of a DNN, making its prediction more stable and less likely to be\nfooled by adversarial samples. We conduct an extensive experimental study\nproving the efficiency of our method against multiple attacks, comparing it to\nnumerous defenses, both in white-box and black-box setups. Additionally, the\nimplementation of our method brings almost no overhead to the training\nprocedure, while maintaining the prediction performance of the original model\non clean samples."
  },
  {
    "id": "arxiv-49",
    "title": "Exploratory Combinatorial Optimization with Reinforcement Learning",
    "abstract": "Many real-world problems can be reduced to combinatorial optimization on a\ngraph, where the subset or ordering of vertices that maximize some objective\nfunction must be found. With such tasks often NP-hard and analytically\nintractable, reinforcement learning (RL) has shown promise as a framework with\nwhich efficient heuristic methods to tackle these problems can be learned.\nPrevious works construct the solution subset incrementally, adding one element\nat a time, however, the irreversible nature of this approach prevents the agent\nfrom revising its earlier decisions, which may be necessary given the\ncomplexity of the optimization task. We instead propose that the agent should\nseek to continuously improve the solution by learning to explore at test time.\nOur approach of exploratory combinatorial optimization (ECO-DQN) is, in\nprinciple, applicable to any combinatorial problem that can be defined on a\ngraph. Experimentally, we show our method to produce state-of-the-art RL\nperformance on the Maximum Cut problem. Moreover, because ECO-DQN can start\nfrom any arbitrary configuration, it can be combined with other search methods\nto further improve performance, which we demonstrate using a simple random\nsearch.",
    "text": "Exploratory Combinatorial Optimization with Reinforcement Learning\n\nMany real-world problems can be reduced to combinatorial optimization on a\ngraph, where the subset or ordering of vertices that maximize some objective\nfunction must be found. With such tasks often NP-hard and analytically\nintractable, reinforcement learning (RL) has shown promise as a framework with\nwhich efficient heuristic methods to tackle these problems can be learned.\nPrevious works construct the solution subset incrementally, adding one element\nat a time, however, the irreversible nature of this approach prevents the agent\nfrom revising its earlier decisions, which may be necessary given the\ncomplexity of the optimization task. We instead propose that the agent should\nseek to continuously improve the solution by learning to explore at test time.\nOur approach of exploratory combinatorial optimization (ECO-DQN) is, in\nprinciple, applicable to any combinatorial problem that can be defined on a\ngraph. Experimentally, we show our method to produce state-of-the-art RL\nperformance on the Maximum Cut problem. Moreover, because ECO-DQN can start\nfrom any arbitrary configuration, it can be combined with other search methods\nto further improve performance, which we demonstrate using a simple random\nsearch."
  },
  {
    "id": "arxiv-50",
    "title": "Using Deep Reinforcement Learning Methods for Autonomous Vessels in 2D\n  Environments",
    "abstract": "Unmanned Surface Vehicles technology (USVs) is an exciting topic that\nessentially deploys an algorithm to safely and efficiently performs a mission.\nAlthough reinforcement learning is a well-known approach to modeling such a\ntask, instability and divergence may occur when combining off-policy and\nfunction approximation. In this work, we used deep reinforcement learning\ncombining Q-learning with a neural representation to avoid instability. Our\nmethodology uses deep q-learning and combines it with a rolling wave planning\napproach on agile methodology. Our method contains two critical parts in order\nto perform missions in an unknown environment. The first is a path planner that\nis responsible for generating a potential effective path to a destination\nwithout considering the details of the root. The latter is a decision-making\nmodule that is responsible for short-term decisions on avoiding obstacles\nduring the near future steps of USV exploitation within the context of the\nvalue function. Simulations were performed using two algorithms: a basic\nvanilla vessel navigator (VVN) as a baseline and an improved one for the vessel\nnavigator with a planner and local view (VNPLV). Experimental results show that\nthe proposed method enhanced the performance of VVN by 55.31 on average for\nlong-distance missions. Our model successfully demonstrated obstacle avoidance\nby means of deep reinforcement learning using planning adaptive paths in\nunknown environments.",
    "text": "Using Deep Reinforcement Learning Methods for Autonomous Vessels in 2D\n  Environments\n\nUnmanned Surface Vehicles technology (USVs) is an exciting topic that\nessentially deploys an algorithm to safely and efficiently performs a mission.\nAlthough reinforcement learning is a well-known approach to modeling such a\ntask, instability and divergence may occur when combining off-policy and\nfunction approximation. In this work, we used deep reinforcement learning\ncombining Q-learning with a neural representation to avoid instability. Our\nmethodology uses deep q-learning and combines it with a rolling wave planning\napproach on agile methodology. Our method contains two critical parts in order\nto perform missions in an unknown environment. The first is a path planner that\nis responsible for generating a potential effective path to a destination\nwithout considering the details of the root. The latter is a decision-making\nmodule that is responsible for short-term decisions on avoiding obstacles\nduring the near future steps of USV exploitation within the context of the\nvalue function. Simulations were performed using two algorithms: a basic\nvanilla vessel navigator (VVN) as a baseline and an improved one for the vessel\nnavigator with a planner and local view (VNPLV). Experimental results show that\nthe proposed method enhanced the performance of VVN by 55.31 on average for\nlong-distance missions. Our model successfully demonstrated obstacle avoidance\nby means of deep reinforcement learning using planning adaptive paths in\nunknown environments."
  },
  {
    "id": "arxiv-51",
    "title": "Timely Communication in Federated Learning",
    "abstract": "We consider a federated learning framework in which a parameter server (PS)\ntrains a global model by using $n$ clients without actually storing the client\ndata centrally at a cloud server. Focusing on a setting where the client\ndatasets are fast changing and highly temporal in nature, we investigate the\ntimeliness of model updates and propose a novel timely communication scheme.\nUnder the proposed scheme, at each iteration, the PS waits for $m$ available\nclients and sends them the current model. Then, the PS uses the local updates\nof the earliest $k$ out of $m$ clients to update the global model at each\niteration. We find the average age of information experienced by each client\nand numerically characterize the age-optimal $m$ and $k$ values for a given\n$n$. Our results indicate that, in addition to ensuring timeliness, the\nproposed communication scheme results in significantly smaller average\niteration times compared to random client selection without hurting the\nconvergence of the global learning task.",
    "text": "Timely Communication in Federated Learning\n\nWe consider a federated learning framework in which a parameter server (PS)\ntrains a global model by using $n$ clients without actually storing the client\ndata centrally at a cloud server. Focusing on a setting where the client\ndatasets are fast changing and highly temporal in nature, we investigate the\ntimeliness of model updates and propose a novel timely communication scheme.\nUnder the proposed scheme, at each iteration, the PS waits for $m$ available\nclients and sends them the current model. Then, the PS uses the local updates\nof the earliest $k$ out of $m$ clients to update the global model at each\niteration. We find the average age of information experienced by each client\nand numerically characterize the age-optimal $m$ and $k$ values for a given\n$n$. Our results indicate that, in addition to ensuring timeliness, the\nproposed communication scheme results in significantly smaller average\niteration times compared to random client selection without hurting the\nconvergence of the global learning task."
  },
  {
    "id": "arxiv-52",
    "title": "A Competitive Deep Neural Network Approach for the ImageCLEFmed Caption\n  2020 Task",
    "abstract": "The aim of ImageCLEFmed Caption task is to develop a system that\nautomatically labels radiology images with relevant medical concepts. We\ndescribe our Deep Neural Network (DNN) based approach for tackling this\nproblem. On the challenge test set of 3,534 radiology images, our system\nachieves an F1 score of 0.375 and ranks high, 12th among all systems that were\nsuccessfully submitted to the challenge, whereby we only rely on the provided\ndata sources and do not use any external medical knowledge or ontologies, or\npretrained models from other medical image repositories or application domains.",
    "text": "A Competitive Deep Neural Network Approach for the ImageCLEFmed Caption\n  2020 Task\n\nThe aim of ImageCLEFmed Caption task is to develop a system that\nautomatically labels radiology images with relevant medical concepts. We\ndescribe our Deep Neural Network (DNN) based approach for tackling this\nproblem. On the challenge test set of 3,534 radiology images, our system\nachieves an F1 score of 0.375 and ranks high, 12th among all systems that were\nsuccessfully submitted to the challenge, whereby we only rely on the provided\ndata sources and do not use any external medical knowledge or ontologies, or\npretrained models from other medical image repositories or application domains."
  },
  {
    "id": "arxiv-53",
    "title": "Minimally distorted Adversarial Examples with a Fast Adaptive Boundary\n  Attack",
    "abstract": "The evaluation of robustness against adversarial manipulation of neural\nnetworks-based classifiers is mainly tested with empirical attacks as methods\nfor the exact computation, even when available, do not scale to large networks.\nWe propose in this paper a new white-box adversarial attack wrt the $l_p$-norms\nfor $p \\in \\{1,2,\\infty\\}$ aiming at finding the minimal perturbation necessary\nto change the class of a given input. It has an intuitive geometric meaning,\nyields quickly high quality results, minimizes the size of the perturbation (so\nthat it returns the robust accuracy at every threshold with a single run). It\nperforms better or similar to state-of-the-art attacks which are partially\nspecialized to one $l_p$-norm, and is robust to the phenomenon of gradient\nmasking.",
    "text": "Minimally distorted Adversarial Examples with a Fast Adaptive Boundary\n  Attack\n\nThe evaluation of robustness against adversarial manipulation of neural\nnetworks-based classifiers is mainly tested with empirical attacks as methods\nfor the exact computation, even when available, do not scale to large networks.\nWe propose in this paper a new white-box adversarial attack wrt the $l_p$-norms\nfor $p \\in \\{1,2,\\infty\\}$ aiming at finding the minimal perturbation necessary\nto change the class of a given input. It has an intuitive geometric meaning,\nyields quickly high quality results, minimizes the size of the perturbation (so\nthat it returns the robust accuracy at every threshold with a single run). It\nperforms better or similar to state-of-the-art attacks which are partially\nspecialized to one $l_p$-norm, and is robust to the phenomenon of gradient\nmasking."
  },
  {
    "id": "arxiv-54",
    "title": "Causal Matrix Completion",
    "abstract": "Matrix completion is the study of recovering an underlying matrix from a\nsparse subset of noisy observations. Traditionally, it is assumed that the\nentries of the matrix are \"missing completely at random\" (MCAR), i.e., each\nentry is revealed at random, independent of everything else, with uniform\nprobability. This is likely unrealistic due to the presence of \"latent\nconfounders\", i.e., unobserved factors that determine both the entries of the\nunderlying matrix and the missingness pattern in the observed matrix. For\nexample, in the context of movie recommender systems -- a canonical application\nfor matrix completion -- a user who vehemently dislikes horror films is\nunlikely to ever watch horror films. In general, these confounders yield\n\"missing not at random\" (MNAR) data, which can severely impact any inference\nprocedure that does not correct for this bias. We develop a formal causal model\nfor matrix completion through the language of potential outcomes, and provide\nnovel identification arguments for a variety of causal estimands of interest.\nWe design a procedure, which we call \"synthetic nearest neighbors\" (SNN), to\nestimate these causal estimands. We prove finite-sample consistency and\nasymptotic normality of our estimator. Our analysis also leads to new\ntheoretical results for the matrix completion literature. In particular, we\nestablish entry-wise, i.e., max-norm, finite-sample consistency and asymptotic\nnormality results for matrix completion with MNAR data. As a special case, this\nalso provides entry-wise bounds for matrix completion with MCAR data. Across\nsimulated and real data, we demonstrate the efficacy of our proposed estimator.",
    "text": "Causal Matrix Completion\n\nMatrix completion is the study of recovering an underlying matrix from a\nsparse subset of noisy observations. Traditionally, it is assumed that the\nentries of the matrix are \"missing completely at random\" (MCAR), i.e., each\nentry is revealed at random, independent of everything else, with uniform\nprobability. This is likely unrealistic due to the presence of \"latent\nconfounders\", i.e., unobserved factors that determine both the entries of the\nunderlying matrix and the missingness pattern in the observed matrix. For\nexample, in the context of movie recommender systems -- a canonical application\nfor matrix completion -- a user who vehemently dislikes horror films is\nunlikely to ever watch horror films. In general, these confounders yield\n\"missing not at random\" (MNAR) data, which can severely impact any inference\nprocedure that does not correct for this bias. We develop a formal causal model\nfor matrix completion through the language of potential outcomes, and provide\nnovel identification arguments for a variety of causal estimands of interest.\nWe design a procedure, which we call \"synthetic nearest neighbors\" (SNN), to\nestimate these causal estimands. We prove finite-sample consistency and\nasymptotic normality of our estimator. Our analysis also leads to new\ntheoretical results for the matrix completion literature. In particular, we\nestablish entry-wise, i.e., max-norm, finite-sample consistency and asymptotic\nnormality results for matrix completion with MNAR data. As a special case, this\nalso provides entry-wise bounds for matrix completion with MCAR data. Across\nsimulated and real data, we demonstrate the efficacy of our proposed estimator."
  },
  {
    "id": "arxiv-55",
    "title": "DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting",
    "abstract": "Recent progress has shown that large-scale pre-training using contrastive\nimage-text pairs can be a promising alternative for high-quality visual\nrepresentation learning from natural language supervision. Benefiting from a\nbroader source of supervision, this new paradigm exhibits impressive\ntransferability to downstream classification tasks and datasets. However, the\nproblem of transferring the knowledge learned from image-text pairs to more\ncomplex dense prediction tasks has barely been visited. In this work, we\npresent a new framework for dense prediction by implicitly and explicitly\nleveraging the pre-trained knowledge from CLIP. Specifically, we convert the\noriginal image-text matching problem in CLIP to a pixel-text matching problem\nand use the pixel-text score maps to guide the learning of dense prediction\nmodels. By further using the contextual information from the image to prompt\nthe language model, we are able to facilitate our model to better exploit the\npre-trained knowledge. Our method is model-agnostic, which can be applied to\narbitrary dense prediction systems and various pre-trained visual backbones\nincluding both CLIP models and ImageNet pre-trained models. Extensive\nexperiments demonstrate the superior performance of our methods on semantic\nsegmentation, object detection, and instance segmentation tasks. Code is\navailable at https://github.com/raoyongming/DenseCLIP",
    "text": "DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting\n\nRecent progress has shown that large-scale pre-training using contrastive\nimage-text pairs can be a promising alternative for high-quality visual\nrepresentation learning from natural language supervision. Benefiting from a\nbroader source of supervision, this new paradigm exhibits impressive\ntransferability to downstream classification tasks and datasets. However, the\nproblem of transferring the knowledge learned from image-text pairs to more\ncomplex dense prediction tasks has barely been visited. In this work, we\npresent a new framework for dense prediction by implicitly and explicitly\nleveraging the pre-trained knowledge from CLIP. Specifically, we convert the\noriginal image-text matching problem in CLIP to a pixel-text matching problem\nand use the pixel-text score maps to guide the learning of dense prediction\nmodels. By further using the contextual information from the image to prompt\nthe language model, we are able to facilitate our model to better exploit the\npre-trained knowledge. Our method is model-agnostic, which can be applied to\narbitrary dense prediction systems and various pre-trained visual backbones\nincluding both CLIP models and ImageNet pre-trained models. Extensive\nexperiments demonstrate the superior performance of our methods on semantic\nsegmentation, object detection, and instance segmentation tasks. Code is\navailable at https://github.com/raoyongming/DenseCLIP"
  },
  {
    "id": "arxiv-56",
    "title": "Learning Wake-Sleep Recurrent Attention Models",
    "abstract": "Despite their success, convolutional neural networks are computationally\nexpensive because they must examine all image locations. Stochastic\nattention-based models have been shown to improve computational efficiency at\ntest time, but they remain difficult to train because of intractable posterior\ninference and high variance in the stochastic gradient estimates. Borrowing\ntechniques from the literature on training deep generative models, we present\nthe Wake-Sleep Recurrent Attention Model, a method for training stochastic\nattention networks which improves posterior inference and which reduces the\nvariability in the stochastic gradients. We show that our method can greatly\nspeed up the training time for stochastic attention networks in the domains of\nimage classification and caption generation.",
    "text": "Learning Wake-Sleep Recurrent Attention Models\n\nDespite their success, convolutional neural networks are computationally\nexpensive because they must examine all image locations. Stochastic\nattention-based models have been shown to improve computational efficiency at\ntest time, but they remain difficult to train because of intractable posterior\ninference and high variance in the stochastic gradient estimates. Borrowing\ntechniques from the literature on training deep generative models, we present\nthe Wake-Sleep Recurrent Attention Model, a method for training stochastic\nattention networks which improves posterior inference and which reduces the\nvariability in the stochastic gradients. We show that our method can greatly\nspeed up the training time for stochastic attention networks in the domains of\nimage classification and caption generation."
  },
  {
    "id": "arxiv-57",
    "title": "Adversarial Domain Adaptation with Domain Mixup",
    "abstract": "Recent works on domain adaptation reveal the effectiveness of adversarial\nlearning on filling the discrepancy between source and target domains. However,\ntwo common limitations exist in current adversarial-learning-based methods.\nFirst, samples from two domains alone are not sufficient to ensure\ndomain-invariance at most part of latent space. Second, the domain\ndiscriminator involved in these methods can only judge real or fake with the\nguidance of hard label, while it is more reasonable to use soft scores to\nevaluate the generated images or features, i.e., to fully utilize the\ninter-domain information. In this paper, we present adversarial domain\nadaptation with domain mixup (DM-ADA), which guarantees domain-invariance in a\nmore continuous latent space and guides the domain discriminator in judging\nsamples' difference relative to source and target domains. Domain mixup is\njointly conducted on pixel and feature level to improve the robustness of\nmodels. Extensive experiments prove that the proposed approach can achieve\nsuperior performance on tasks with various degrees of domain shift and data\ncomplexity.",
    "text": "Adversarial Domain Adaptation with Domain Mixup\n\nRecent works on domain adaptation reveal the effectiveness of adversarial\nlearning on filling the discrepancy between source and target domains. However,\ntwo common limitations exist in current adversarial-learning-based methods.\nFirst, samples from two domains alone are not sufficient to ensure\ndomain-invariance at most part of latent space. Second, the domain\ndiscriminator involved in these methods can only judge real or fake with the\nguidance of hard label, while it is more reasonable to use soft scores to\nevaluate the generated images or features, i.e., to fully utilize the\ninter-domain information. In this paper, we present adversarial domain\nadaptation with domain mixup (DM-ADA), which guarantees domain-invariance in a\nmore continuous latent space and guides the domain discriminator in judging\nsamples' difference relative to source and target domains. Domain mixup is\njointly conducted on pixel and feature level to improve the robustness of\nmodels. Extensive experiments prove that the proposed approach can achieve\nsuperior performance on tasks with various degrees of domain shift and data\ncomplexity."
  },
  {
    "id": "arxiv-58",
    "title": "Learning and Certification under Instance-targeted Poisoning",
    "abstract": "In this paper, we study PAC learnability and certification of predictions\nunder instance-targeted poisoning attacks, where the adversary who knows the\ntest instance may change a fraction of the training set with the goal of\nfooling the learner at the test instance. Our first contribution is to\nformalize the problem in various settings and to explicitly model subtle\naspects such as the proper or improper nature of the learning, learner's\nrandomness, and whether (or not) adversary's attack can depend on it. Our main\nresult shows that when the budget of the adversary scales sublinearly with the\nsample complexity, (improper) PAC learnability and certification are\nachievable; in contrast, when the adversary's budget grows linearly with the\nsample complexity, the adversary can potentially drive up the expected 0-1 loss\nto one. We also study distribution-specific PAC learning in the same attack\nmodel and show that proper learning with certification is possible for learning\nhalf spaces under natural distributions. Finally, we empirically study the\nrobustness of K nearest neighbour, logistic regression, multi-layer perceptron,\nand convolutional neural network on real data sets against targeted-poisoning\nattacks. Our experimental results show that many models, especially\nstate-of-the-art neural networks, are indeed vulnerable to these strong\nattacks. Interestingly, we observe that methods with high standard accuracy\nmight be more vulnerable to instance-targeted poisoning attacks.",
    "text": "Learning and Certification under Instance-targeted Poisoning\n\nIn this paper, we study PAC learnability and certification of predictions\nunder instance-targeted poisoning attacks, where the adversary who knows the\ntest instance may change a fraction of the training set with the goal of\nfooling the learner at the test instance. Our first contribution is to\nformalize the problem in various settings and to explicitly model subtle\naspects such as the proper or improper nature of the learning, learner's\nrandomness, and whether (or not) adversary's attack can depend on it. Our main\nresult shows that when the budget of the adversary scales sublinearly with the\nsample complexity, (improper) PAC learnability and certification are\nachievable; in contrast, when the adversary's budget grows linearly with the\nsample complexity, the adversary can potentially drive up the expected 0-1 loss\nto one. We also study distribution-specific PAC learning in the same attack\nmodel and show that proper learning with certification is possible for learning\nhalf spaces under natural distributions. Finally, we empirically study the\nrobustness of K nearest neighbour, logistic regression, multi-layer perceptron,\nand convolutional neural network on real data sets against targeted-poisoning\nattacks. Our experimental results show that many models, especially\nstate-of-the-art neural networks, are indeed vulnerable to these strong\nattacks. Interestingly, we observe that methods with high standard accuracy\nmight be more vulnerable to instance-targeted poisoning attacks."
  },
  {
    "id": "arxiv-59",
    "title": "Privacy-preserving Active Learning on Sensitive Data for User Intent\n  Classification",
    "abstract": "Active learning holds promise of significantly reducing data annotation costs\nwhile maintaining reasonable model performance. However, it requires sending\ndata to annotators for labeling. This presents a possible privacy leak when the\ntraining set includes sensitive user data. In this paper, we describe an\napproach for carrying out privacy preserving active learning with quantifiable\nguarantees. We evaluate our approach by showing the tradeoff between privacy,\nutility and annotation budget on a binary classification task in a active\nlearning setting.",
    "text": "Privacy-preserving Active Learning on Sensitive Data for User Intent\n  Classification\n\nActive learning holds promise of significantly reducing data annotation costs\nwhile maintaining reasonable model performance. However, it requires sending\ndata to annotators for labeling. This presents a possible privacy leak when the\ntraining set includes sensitive user data. In this paper, we describe an\napproach for carrying out privacy preserving active learning with quantifiable\nguarantees. We evaluate our approach by showing the tradeoff between privacy,\nutility and annotation budget on a binary classification task in a active\nlearning setting."
  },
  {
    "id": "arxiv-60",
    "title": "End-to-End Video-To-Speech Synthesis using Generative Adversarial\n  Networks",
    "abstract": "Video-to-speech is the process of reconstructing the audio speech from a\nvideo of a spoken utterance. Previous approaches to this task have relied on a\ntwo-step process where an intermediate representation is inferred from the\nvideo, and is then decoded into waveform audio using a vocoder or a waveform\nreconstruction algorithm. In this work, we propose a new end-to-end\nvideo-to-speech model based on Generative Adversarial Networks (GANs) which\ntranslates spoken video to waveform end-to-end without using any intermediate\nrepresentation or separate waveform synthesis algorithm. Our model consists of\nan encoder-decoder architecture that receives raw video as input and generates\nspeech, which is then fed to a waveform critic and a power critic. The use of\nan adversarial loss based on these two critics enables the direct synthesis of\nraw audio waveform and ensures its realism. In addition, the use of our three\ncomparative losses helps establish direct correspondence between the generated\naudio and the input video. We show that this model is able to reconstruct\nspeech with remarkable realism for constrained datasets such as GRID, and that\nit is the first end-to-end model to produce intelligible speech for LRW (Lip\nReading in the Wild), featuring hundreds of speakers recorded entirely `in the\nwild'. We evaluate the generated samples in two different scenarios -- seen and\nunseen speakers -- using four objective metrics which measure the quality and\nintelligibility of artificial speech. We demonstrate that the proposed approach\noutperforms all previous works in most metrics on GRID and LRW.",
    "text": "End-to-End Video-To-Speech Synthesis using Generative Adversarial\n  Networks\n\nVideo-to-speech is the process of reconstructing the audio speech from a\nvideo of a spoken utterance. Previous approaches to this task have relied on a\ntwo-step process where an intermediate representation is inferred from the\nvideo, and is then decoded into waveform audio using a vocoder or a waveform\nreconstruction algorithm. In this work, we propose a new end-to-end\nvideo-to-speech model based on Generative Adversarial Networks (GANs) which\ntranslates spoken video to waveform end-to-end without using any intermediate\nrepresentation or separate waveform synthesis algorithm. Our model consists of\nan encoder-decoder architecture that receives raw video as input and generates\nspeech, which is then fed to a waveform critic and a power critic. The use of\nan adversarial loss based on these two critics enables the direct synthesis of\nraw audio waveform and ensures its realism. In addition, the use of our three\ncomparative losses helps establish direct correspondence between the generated\naudio and the input video. We show that this model is able to reconstruct\nspeech with remarkable realism for constrained datasets such as GRID, and that\nit is the first end-to-end model to produce intelligible speech for LRW (Lip\nReading in the Wild), featuring hundreds of speakers recorded entirely `in the\nwild'. We evaluate the generated samples in two different scenarios -- seen and\nunseen speakers -- using four objective metrics which measure the quality and\nintelligibility of artificial speech. We demonstrate that the proposed approach\noutperforms all previous works in most metrics on GRID and LRW."
  },
  {
    "id": "arxiv-61",
    "title": "Estimating Categorical Counterfactuals via Deep Twin Networks",
    "abstract": "Counterfactual inference is a powerful tool, capable of solving challenging\nproblems in high-profile sectors. To perform counterfactual inference, one\nrequires knowledge of the underlying causal mechanisms. However, causal\nmechanisms cannot be uniquely determined from observations and interventions\nalone. This raises the question of how to choose the causal mechanisms so that\nresulting counterfactual inference is trustworthy in a given domain. This\nquestion has been addressed in causal models with binary variables, but the\ncase of categorical variables remains unanswered. We address this challenge by\nintroducing for causal models with categorical variables the notion of\ncounterfactual ordering, a principle that posits desirable properties causal\nmechanisms should posses, and prove that it is equivalent to specific\nfunctional constraints on the causal mechanisms. To learn causal mechanisms\nsatisfying these constraints, and perform counterfactual inference with them,\nwe introduce deep twin networks. These are deep neural networks that, when\ntrained, are capable of twin network counterfactual inference -- an alternative\nto the abduction, action, & prediction method. We empirically test our approach\non diverse real-world and semi-synthetic data from medicine, epidemiology, and\nfinance, reporting accurate estimation of counterfactual probabilities while\ndemonstrating the issues that arise with counterfactual reasoning when\ncounterfactual ordering is not enforced.",
    "text": "Estimating Categorical Counterfactuals via Deep Twin Networks\n\nCounterfactual inference is a powerful tool, capable of solving challenging\nproblems in high-profile sectors. To perform counterfactual inference, one\nrequires knowledge of the underlying causal mechanisms. However, causal\nmechanisms cannot be uniquely determined from observations and interventions\nalone. This raises the question of how to choose the causal mechanisms so that\nresulting counterfactual inference is trustworthy in a given domain. This\nquestion has been addressed in causal models with binary variables, but the\ncase of categorical variables remains unanswered. We address this challenge by\nintroducing for causal models with categorical variables the notion of\ncounterfactual ordering, a principle that posits desirable properties causal\nmechanisms should posses, and prove that it is equivalent to specific\nfunctional constraints on the causal mechanisms. To learn causal mechanisms\nsatisfying these constraints, and perform counterfactual inference with them,\nwe introduce deep twin networks. These are deep neural networks that, when\ntrained, are capable of twin network counterfactual inference -- an alternative\nto the abduction, action, & prediction method. We empirically test our approach\non diverse real-world and semi-synthetic data from medicine, epidemiology, and\nfinance, reporting accurate estimation of counterfactual probabilities while\ndemonstrating the issues that arise with counterfactual reasoning when\ncounterfactual ordering is not enforced."
  },
  {
    "id": "arxiv-62",
    "title": "NOTMAD: Estimating Bayesian Networks with Sample-Specific Structures and\n  Parameters",
    "abstract": "Context-specific Bayesian networks (i.e. directed acyclic graphs, DAGs)\nidentify context-dependent relationships between variables, but the\nnon-convexity induced by the acyclicity requirement makes it difficult to share\ninformation between context-specific estimators (e.g. with graph generator\nfunctions). For this reason, existing methods for inferring context-specific\nBayesian networks have favored breaking datasets into subsamples, limiting\nstatistical power and resolution, and preventing the use of multidimensional\nand latent contexts. To overcome this challenge, we propose NOTEARS-optimized\nMixtures of Archetypal DAGs (NOTMAD). NOTMAD models context-specific Bayesian\nnetworks as the output of a function which learns to mix archetypal networks\naccording to sample context. The archetypal networks are estimated jointly with\nthe context-specific networks and do not require any prior knowledge. We encode\nthe acyclicity constraint as a smooth regularization loss which is\nback-propagated to the mixing function; in this way, NOTMAD shares information\nbetween context-specific acyclic graphs, enabling the estimation of Bayesian\nnetwork structures and parameters at even single-sample resolution. We\ndemonstrate the utility of NOTMAD and sample-specific network inference through\nanalysis and experiments, including patient-specific gene expression networks\nwhich correspond to morphological variation in cancer.",
    "text": "NOTMAD: Estimating Bayesian Networks with Sample-Specific Structures and\n  Parameters\n\nContext-specific Bayesian networks (i.e. directed acyclic graphs, DAGs)\nidentify context-dependent relationships between variables, but the\nnon-convexity induced by the acyclicity requirement makes it difficult to share\ninformation between context-specific estimators (e.g. with graph generator\nfunctions). For this reason, existing methods for inferring context-specific\nBayesian networks have favored breaking datasets into subsamples, limiting\nstatistical power and resolution, and preventing the use of multidimensional\nand latent contexts. To overcome this challenge, we propose NOTEARS-optimized\nMixtures of Archetypal DAGs (NOTMAD). NOTMAD models context-specific Bayesian\nnetworks as the output of a function which learns to mix archetypal networks\naccording to sample context. The archetypal networks are estimated jointly with\nthe context-specific networks and do not require any prior knowledge. We encode\nthe acyclicity constraint as a smooth regularization loss which is\nback-propagated to the mixing function; in this way, NOTMAD shares information\nbetween context-specific acyclic graphs, enabling the estimation of Bayesian\nnetwork structures and parameters at even single-sample resolution. We\ndemonstrate the utility of NOTMAD and sample-specific network inference through\nanalysis and experiments, including patient-specific gene expression networks\nwhich correspond to morphological variation in cancer."
  },
  {
    "id": "arxiv-63",
    "title": "On the Trustworthiness of Tree Ensemble Explainability Methods",
    "abstract": "The recent increase in the deployment of machine learning models in critical\ndomains such as healthcare, criminal justice, and finance has highlighted the\nneed for trustworthy methods that can explain these models to stakeholders.\nFeature importance methods (e.g. gain and SHAP) are among the most popular\nexplainability methods used to address this need. For any explainability\ntechnique to be trustworthy and meaningful, it has to provide an explanation\nthat is accurate and stable. Although the stability of local feature importance\nmethods (explaining individual predictions) has been studied before, there is\nyet a knowledge gap about the stability of global features importance methods\n(explanations for the whole model). Additionally, there is no study that\nevaluates and compares the accuracy of global feature importance methods with\nrespect to feature ordering. In this paper, we evaluate the accuracy and\nstability of global feature importance methods through comprehensive\nexperiments done on simulations as well as four real-world datasets. We focus\non tree-based ensemble methods as they are used widely in industry and measure\nthe accuracy and stability of explanations under two scenarios: 1) when inputs\nare perturbed 2) when models are perturbed. Our findings provide a comparison\nof these methods under a variety of settings and shed light on the limitations\nof global feature importance methods by indicating their lack of accuracy with\nand without noisy inputs, as well as their lack of stability with respect to:\n1) increase in input dimension or noise in the data; 2) perturbations in models\ninitialized by different random seeds or hyperparameter settings.",
    "text": "On the Trustworthiness of Tree Ensemble Explainability Methods\n\nThe recent increase in the deployment of machine learning models in critical\ndomains such as healthcare, criminal justice, and finance has highlighted the\nneed for trustworthy methods that can explain these models to stakeholders.\nFeature importance methods (e.g. gain and SHAP) are among the most popular\nexplainability methods used to address this need. For any explainability\ntechnique to be trustworthy and meaningful, it has to provide an explanation\nthat is accurate and stable. Although the stability of local feature importance\nmethods (explaining individual predictions) has been studied before, there is\nyet a knowledge gap about the stability of global features importance methods\n(explanations for the whole model). Additionally, there is no study that\nevaluates and compares the accuracy of global feature importance methods with\nrespect to feature ordering. In this paper, we evaluate the accuracy and\nstability of global feature importance methods through comprehensive\nexperiments done on simulations as well as four real-world datasets. We focus\non tree-based ensemble methods as they are used widely in industry and measure\nthe accuracy and stability of explanations under two scenarios: 1) when inputs\nare perturbed 2) when models are perturbed. Our findings provide a comparison\nof these methods under a variety of settings and shed light on the limitations\nof global feature importance methods by indicating their lack of accuracy with\nand without noisy inputs, as well as their lack of stability with respect to:\n1) increase in input dimension or noise in the data; 2) perturbations in models\ninitialized by different random seeds or hyperparameter settings."
  },
  {
    "id": "arxiv-64",
    "title": "Deep Learning for Interference Identification: Band, Training SNR, and\n  Sample Selection",
    "abstract": "We study the problem of interference source identification, through the lens\nof recognizing one of 15 different channels that belong to 3 different wireless\ntechnologies: Bluetooth, Zigbee, and WiFi. We employ deep learning algorithms\ntrained on received samples taken from a 10 MHz band in the 2.4 GHz ISM Band.\nWe obtain a classification accuracy of around 89.5% using any of four different\ndeep neural network architectures: CNN, ResNet, CLDNN, and LSTM, which\ndemonstrate the generality of the effectiveness of deep learning at the\nconsidered task. Interestingly, our proposed CNN architecture requires\napproximately 60% of the training time required by the state of the art while\nachieving slightly larger classification accuracy. We then focus on the CNN\narchitecture and further optimize its training time while incurring minimal\nloss in classification accuracy using three different approaches: 1- Band\nSelection, where we only use samples belonging to the lower and uppermost 2 MHz\nbands, 2- SNR Selection, where we only use training samples belonging to a\nsingle SNR value, and 3- Sample Selection, where we try various sub-Nyquist\nsampling methods to select the subset of samples most relevant to the\nclassification task. Our results confirm the feasibility of fast deep learning\nfor wireless interference identification, by showing that the training time can\nbe reduced by as much as 30x with minimal loss in accuracy.",
    "text": "Deep Learning for Interference Identification: Band, Training SNR, and\n  Sample Selection\n\nWe study the problem of interference source identification, through the lens\nof recognizing one of 15 different channels that belong to 3 different wireless\ntechnologies: Bluetooth, Zigbee, and WiFi. We employ deep learning algorithms\ntrained on received samples taken from a 10 MHz band in the 2.4 GHz ISM Band.\nWe obtain a classification accuracy of around 89.5% using any of four different\ndeep neural network architectures: CNN, ResNet, CLDNN, and LSTM, which\ndemonstrate the generality of the effectiveness of deep learning at the\nconsidered task. Interestingly, our proposed CNN architecture requires\napproximately 60% of the training time required by the state of the art while\nachieving slightly larger classification accuracy. We then focus on the CNN\narchitecture and further optimize its training time while incurring minimal\nloss in classification accuracy using three different approaches: 1- Band\nSelection, where we only use samples belonging to the lower and uppermost 2 MHz\nbands, 2- SNR Selection, where we only use training samples belonging to a\nsingle SNR value, and 3- Sample Selection, where we try various sub-Nyquist\nsampling methods to select the subset of samples most relevant to the\nclassification task. Our results confirm the feasibility of fast deep learning\nfor wireless interference identification, by showing that the training time can\nbe reduced by as much as 30x with minimal loss in accuracy."
  },
  {
    "id": "arxiv-65",
    "title": "Disentangling feature and lazy training in deep neural networks",
    "abstract": "Two distinct limits for deep learning have been derived as the network width\n$h\\rightarrow \\infty$, depending on how the weights of the last layer scale\nwith $h$. In the Neural Tangent Kernel (NTK) limit, the dynamics becomes linear\nin the weights and is described by a frozen kernel $\\Theta$. By contrast, in\nthe Mean-Field limit, the dynamics can be expressed in terms of the\ndistribution of the parameters associated with a neuron, that follows a partial\ndifferential equation. In this work we consider deep networks where the weights\nin the last layer scale as $\\alpha h^{-1/2}$ at initialization. By varying\n$\\alpha$ and $h$, we probe the crossover between the two limits. We observe the\npreviously identified regimes of lazy training and feature training. In the\nlazy-training regime, the dynamics is almost linear and the NTK barely changes\nafter initialization. The feature-training regime includes the mean-field\nformulation as a limiting case and is characterized by a kernel that evolves in\ntime, and learns some features. We perform numerical experiments on MNIST,\nFashion-MNIST, EMNIST and CIFAR10 and consider various architectures. We find\nthat (i) The two regimes are separated by an $\\alpha^*$ that scales as\n$h^{-1/2}$. (ii) Network architecture and data structure play an important role\nin determining which regime is better: in our tests, fully-connected networks\nperform generally better in the lazy-training regime, unlike convolutional\nnetworks. (iii) In both regimes, the fluctuations $\\delta F$ induced on the\nlearned function by initial conditions decay as $\\delta F\\sim 1/\\sqrt{h}$,\nleading to a performance that increases with $h$. The same improvement can also\nbe obtained at an intermediate width by ensemble-averaging several networks.\n(iv) In the feature-training regime we identify a time scale\n$t_1\\sim\\sqrt{h}\\alpha$, such that for $t\\ll t_1$ the dynamics is linear.",
    "text": "Disentangling feature and lazy training in deep neural networks\n\nTwo distinct limits for deep learning have been derived as the network width\n$h\\rightarrow \\infty$, depending on how the weights of the last layer scale\nwith $h$. In the Neural Tangent Kernel (NTK) limit, the dynamics becomes linear\nin the weights and is described by a frozen kernel $\\Theta$. By contrast, in\nthe Mean-Field limit, the dynamics can be expressed in terms of the\ndistribution of the parameters associated with a neuron, that follows a partial\ndifferential equation. In this work we consider deep networks where the weights\nin the last layer scale as $\\alpha h^{-1/2}$ at initialization. By varying\n$\\alpha$ and $h$, we probe the crossover between the two limits. We observe the\npreviously identified regimes of lazy training and feature training. In the\nlazy-training regime, the dynamics is almost linear and the NTK barely changes\nafter initialization. The feature-training regime includes the mean-field\nformulation as a limiting case and is characterized by a kernel that evolves in\ntime, and learns some features. We perform numerical experiments on MNIST,\nFashion-MNIST, EMNIST and CIFAR10 and consider various architectures. We find\nthat (i) The two regimes are separated by an $\\alpha^*$ that scales as\n$h^{-1/2}$. (ii) Network architecture and data structure play an important role\nin determining which regime is better: in our tests, fully-connected networks\nperform generally better in the lazy-training regime, unlike convolutional\nnetworks. (iii) In both regimes, the fluctuations $\\delta F$ induced on the\nlearned function by initial conditions decay as $\\delta F\\sim 1/\\sqrt{h}$,\nleading to a performance that increases with $h$. The same improvement can also\nbe obtained at an intermediate width by ensemble-averaging several networks.\n(iv) In the feature-training regime we identify a time scale\n$t_1\\sim\\sqrt{h}\\alpha$, such that for $t\\ll t_1$ the dynamics is linear."
  },
  {
    "id": "arxiv-66",
    "title": "Decentralized learning for wireless communications and networking",
    "abstract": "This chapter deals with decentralized learning algorithms for in-network\nprocessing of graph-valued data. A generic learning problem is formulated and\nrecast into a separable form, which is iteratively minimized using the\nalternating-direction method of multipliers (ADMM) so as to gain the desired\ndegree of parallelization. Without exchanging elements from the distributed\ntraining sets and keeping inter-node communications at affordable levels, the\nlocal (per-node) learners consent to the desired quantity inferred globally,\nmeaning the one obtained if the entire training data set were centrally\navailable. Impact of the decentralized learning framework to contemporary\nwireless communications and networking tasks is illustrated through case\nstudies including target tracking using wireless sensor networks, unveiling\nInternet traffic anomalies, power system state estimation, as well as spectrum\ncartography for wireless cognitive radio networks.",
    "text": "Decentralized learning for wireless communications and networking\n\nThis chapter deals with decentralized learning algorithms for in-network\nprocessing of graph-valued data. A generic learning problem is formulated and\nrecast into a separable form, which is iteratively minimized using the\nalternating-direction method of multipliers (ADMM) so as to gain the desired\ndegree of parallelization. Without exchanging elements from the distributed\ntraining sets and keeping inter-node communications at affordable levels, the\nlocal (per-node) learners consent to the desired quantity inferred globally,\nmeaning the one obtained if the entire training data set were centrally\navailable. Impact of the decentralized learning framework to contemporary\nwireless communications and networking tasks is illustrated through case\nstudies including target tracking using wireless sensor networks, unveiling\nInternet traffic anomalies, power system state estimation, as well as spectrum\ncartography for wireless cognitive radio networks."
  },
  {
    "id": "arxiv-67",
    "title": "Provably Robust Detection of Out-of-distribution Data (almost) for free",
    "abstract": "When applying machine learning in safety-critical systems, a reliable\nassessment of the uncertainy of a classifier is required. However, deep neural\nnetworks are known to produce highly overconfident predictions on\nout-of-distribution (OOD) data and even if trained to be non-confident on OOD\ndata one can still adversarially manipulate OOD data so that the classifer\nagain assigns high confidence to the manipulated samples. In this paper we\npropose a novel method where from first principles we combine a certifiable OOD\ndetector with a standard classifier into an OOD aware classifier. In this way\nwe achieve the best of two worlds: certifiably adversarially robust OOD\ndetection, even for OOD samples close to the in-distribution, without loss in\nprediction accuracy and close to state-of-the-art OOD detection performance for\nnon-manipulated OOD data. Moreover, due to the particular construction our\nclassifier provably avoids the asymptotic overconfidence problem of standard\nneural networks.",
    "text": "Provably Robust Detection of Out-of-distribution Data (almost) for free\n\nWhen applying machine learning in safety-critical systems, a reliable\nassessment of the uncertainy of a classifier is required. However, deep neural\nnetworks are known to produce highly overconfident predictions on\nout-of-distribution (OOD) data and even if trained to be non-confident on OOD\ndata one can still adversarially manipulate OOD data so that the classifer\nagain assigns high confidence to the manipulated samples. In this paper we\npropose a novel method where from first principles we combine a certifiable OOD\ndetector with a standard classifier into an OOD aware classifier. In this way\nwe achieve the best of two worlds: certifiably adversarially robust OOD\ndetection, even for OOD samples close to the in-distribution, without loss in\nprediction accuracy and close to state-of-the-art OOD detection performance for\nnon-manipulated OOD data. Moreover, due to the particular construction our\nclassifier provably avoids the asymptotic overconfidence problem of standard\nneural networks."
  },
  {
    "id": "arxiv-68",
    "title": "Prediction against a limited adversary",
    "abstract": "We study the problem of prediction with expert advice with adversarial\ncorruption where the adversary can at most corrupt one expert. Using tools from\nviscosity theory, we characterize the long-time behavior of the value function\nof the game between the forecaster and the adversary. We provide lower and\nupper bounds for the growth rate of regret without relying on a comparison\nresult. We show that depending on the description of regret, the limiting\nbehavior of the game can significantly differ.",
    "text": "Prediction against a limited adversary\n\nWe study the problem of prediction with expert advice with adversarial\ncorruption where the adversary can at most corrupt one expert. Using tools from\nviscosity theory, we characterize the long-time behavior of the value function\nof the game between the forecaster and the adversary. We provide lower and\nupper bounds for the growth rate of regret without relying on a comparison\nresult. We show that depending on the description of regret, the limiting\nbehavior of the game can significantly differ."
  },
  {
    "id": "arxiv-69",
    "title": "Machine learning a manifold",
    "abstract": "We propose a simple method to identify a continuous Lie algebra symmetry in a\ndataset through regression by an artificial neural network. Our proposal takes\nadvantage of the $ \\mathcal{O}(\\epsilon^2)$ scaling of the output variable\nunder infinitesimal symmetry transformations on the input variables. As\nsymmetry transformations are generated post-training, the methodology does not\nrely on sampling of the full representation space or binning of the dataset,\nand the possibility of false identification is minimised. We demonstrate our\nmethod in the SU(3)-symmetric (non-) linear $\\Sigma$ model.",
    "text": "Machine learning a manifold\n\nWe propose a simple method to identify a continuous Lie algebra symmetry in a\ndataset through regression by an artificial neural network. Our proposal takes\nadvantage of the $ \\mathcal{O}(\\epsilon^2)$ scaling of the output variable\nunder infinitesimal symmetry transformations on the input variables. As\nsymmetry transformations are generated post-training, the methodology does not\nrely on sampling of the full representation space or binning of the dataset,\nand the possibility of false identification is minimised. We demonstrate our\nmethod in the SU(3)-symmetric (non-) linear $\\Sigma$ model."
  },
  {
    "id": "arxiv-70",
    "title": "MECATS: Mixture-of-Experts for Quantile Forecasts of Aggregated Time\n  Series",
    "abstract": "We introduce a mixture of heterogeneous experts framework called\n\\texttt{MECATS}, which simultaneously forecasts the values of a set of time\nseries that are related through an aggregation hierarchy. Different types of\nforecasting models can be employed as individual experts so that the form of\neach model can be tailored to the nature of the corresponding time series.\n\\texttt{MECATS} learns hierarchical relationships during the training stage to\nhelp generalize better across all the time series being modeled and also\nmitigates coherency issues that arise due to constraints imposed by the\nhierarchy. We further build multiple quantile estimators on top of the point\nforecasts. The resulting probabilistic forecasts are nearly coherent,\ndistribution-free, and independent of the choice of forecasting models. We\nconduct a comprehensive evaluation on both point and probabilistic forecasts\nand also formulate an extension for situations where change points exist in\nsequential data. In general, our method is robust, adaptive to datasets with\ndifferent properties, and highly configurable and efficient for large-scale\nforecasting pipelines.",
    "text": "MECATS: Mixture-of-Experts for Quantile Forecasts of Aggregated Time\n  Series\n\nWe introduce a mixture of heterogeneous experts framework called\n\\texttt{MECATS}, which simultaneously forecasts the values of a set of time\nseries that are related through an aggregation hierarchy. Different types of\nforecasting models can be employed as individual experts so that the form of\neach model can be tailored to the nature of the corresponding time series.\n\\texttt{MECATS} learns hierarchical relationships during the training stage to\nhelp generalize better across all the time series being modeled and also\nmitigates coherency issues that arise due to constraints imposed by the\nhierarchy. We further build multiple quantile estimators on top of the point\nforecasts. The resulting probabilistic forecasts are nearly coherent,\ndistribution-free, and independent of the choice of forecasting models. We\nconduct a comprehensive evaluation on both point and probabilistic forecasts\nand also formulate an extension for situations where change points exist in\nsequential data. In general, our method is robust, adaptive to datasets with\ndifferent properties, and highly configurable and efficient for large-scale\nforecasting pipelines."
  },
  {
    "id": "arxiv-71",
    "title": "Predicting Landslides Using Locally Aligned Convolutional Neural\n  Networks",
    "abstract": "Landslides, movement of soil and rock under the influence of gravity, are\ncommon phenomena that cause significant human and economic losses every year.\nExperts use heterogeneous features such as slope, elevation, land cover,\nlithology, rock age, and rock family to predict landslides. To work with such\nfeatures, we adapted convolutional neural networks to consider relative spatial\ninformation for the prediction task. Traditional filters in these networks\neither have a fixed orientation or are rotationally invariant. Intuitively, the\nfilters should orient uphill, but there is not enough data to learn the concept\nof uphill; instead, it can be provided as prior knowledge. We propose a model\ncalled Locally Aligned Convolutional Neural Network, LACNN, that follows the\nground surface at multiple scales to predict possible landslide occurrence for\na single point. To validate our method, we created a standardized dataset of\ngeoreferenced images consisting of the heterogeneous features as inputs, and\ncompared our method to several baselines, including linear regression, a neural\nnetwork, and a convolutional network, using log-likelihood error and Receiver\nOperating Characteristic curves on the test set. Our model achieves 2-7%\nimprovement in terms of accuracy and 2-15% boost in terms of log likelihood\ncompared to the other proposed baselines.",
    "text": "Predicting Landslides Using Locally Aligned Convolutional Neural\n  Networks\n\nLandslides, movement of soil and rock under the influence of gravity, are\ncommon phenomena that cause significant human and economic losses every year.\nExperts use heterogeneous features such as slope, elevation, land cover,\nlithology, rock age, and rock family to predict landslides. To work with such\nfeatures, we adapted convolutional neural networks to consider relative spatial\ninformation for the prediction task. Traditional filters in these networks\neither have a fixed orientation or are rotationally invariant. Intuitively, the\nfilters should orient uphill, but there is not enough data to learn the concept\nof uphill; instead, it can be provided as prior knowledge. We propose a model\ncalled Locally Aligned Convolutional Neural Network, LACNN, that follows the\nground surface at multiple scales to predict possible landslide occurrence for\na single point. To validate our method, we created a standardized dataset of\ngeoreferenced images consisting of the heterogeneous features as inputs, and\ncompared our method to several baselines, including linear regression, a neural\nnetwork, and a convolutional network, using log-likelihood error and Receiver\nOperating Characteristic curves on the test set. Our model achieves 2-7%\nimprovement in terms of accuracy and 2-15% boost in terms of log likelihood\ncompared to the other proposed baselines."
  },
  {
    "id": "arxiv-72",
    "title": "Double cycle-consistent generative adversarial network for unsupervised\n  conditional generation",
    "abstract": "Conditional generative models have achieved considerable success in the past\nfew years, but usually require a lot of labeled data. Recently, ClusterGAN\ncombines GAN with an encoder to achieve remarkable clustering performance via\nunsupervised conditional generation. However, it ignores the real conditional\ndistribution of data, which leads to generating less diverse samples for each\nclass and makes the encoder only achieve sub-optimal clustering performance.\nHere, we propose a new unsupervised conditional generation framework, Double\nCycle-Consistent Conditional GAN (DC3-GAN), which can generate diverse\nclass-conditioned samples. We enforce the encoder and the generator of GAN to\nform an encoder-generator pair in addition to the generator-encoder pair, which\nenables us to avoid the low-diversity generation and the triviality of latent\nfeatures. We train the encoder-generator pair using real data, which can\nindirectly estimate the real conditional distribution. Meanwhile, this\nframework enforces the outputs of the encoder to match the inputs of GAN and\nthe prior noise distribution, which disentangles latent space into two parts:\none-hot discrete and continuous latent variables. The former can be directly\nexpressed as clusters and the latter represents remaining unspecified factors.\nThis work demonstrates that enhancing the diversity of unsupervised conditional\ngenerated samples can improve the clustering performance. Experiments on\ndifferent benchmark datasets show that the proposed method outperforms existing\ngenerative model-based clustering methods, and also achieves the optimal\ndisentanglement performance.",
    "text": "Double cycle-consistent generative adversarial network for unsupervised\n  conditional generation\n\nConditional generative models have achieved considerable success in the past\nfew years, but usually require a lot of labeled data. Recently, ClusterGAN\ncombines GAN with an encoder to achieve remarkable clustering performance via\nunsupervised conditional generation. However, it ignores the real conditional\ndistribution of data, which leads to generating less diverse samples for each\nclass and makes the encoder only achieve sub-optimal clustering performance.\nHere, we propose a new unsupervised conditional generation framework, Double\nCycle-Consistent Conditional GAN (DC3-GAN), which can generate diverse\nclass-conditioned samples. We enforce the encoder and the generator of GAN to\nform an encoder-generator pair in addition to the generator-encoder pair, which\nenables us to avoid the low-diversity generation and the triviality of latent\nfeatures. We train the encoder-generator pair using real data, which can\nindirectly estimate the real conditional distribution. Meanwhile, this\nframework enforces the outputs of the encoder to match the inputs of GAN and\nthe prior noise distribution, which disentangles latent space into two parts:\none-hot discrete and continuous latent variables. The former can be directly\nexpressed as clusters and the latter represents remaining unspecified factors.\nThis work demonstrates that enhancing the diversity of unsupervised conditional\ngenerated samples can improve the clustering performance. Experiments on\ndifferent benchmark datasets show that the proposed method outperforms existing\ngenerative model-based clustering methods, and also achieves the optimal\ndisentanglement performance."
  },
  {
    "id": "arxiv-73",
    "title": "R-SNN: An Analysis and Design Methodology for Robustifying Spiking\n  Neural Networks against Adversarial Attacks through Noise Filters for Dynamic\n  Vision Sensors",
    "abstract": "Spiking Neural Networks (SNNs) aim at providing energy-efficient learning\ncapabilities when implemented on neuromorphic chips with event-based Dynamic\nVision Sensors (DVS). This paper studies the robustness of SNNs against\nadversarial attacks on such DVS-based systems, and proposes R-SNN, a novel\nmethodology for robustifying SNNs through efficient DVS-noise filtering. We are\nthe first to generate adversarial attacks on DVS signals (i.e., frames of\nevents in the spatio-temporal domain) and to apply noise filters for DVS\nsensors in the quest for defending against adversarial attacks. Our results\nshow that the noise filters effectively prevent the SNNs from being fooled. The\nSNNs in our experiments provide more than 90% accuracy on the DVS-Gesture and\nNMNIST datasets under different adversarial threat models.",
    "text": "R-SNN: An Analysis and Design Methodology for Robustifying Spiking\n  Neural Networks against Adversarial Attacks through Noise Filters for Dynamic\n  Vision Sensors\n\nSpiking Neural Networks (SNNs) aim at providing energy-efficient learning\ncapabilities when implemented on neuromorphic chips with event-based Dynamic\nVision Sensors (DVS). This paper studies the robustness of SNNs against\nadversarial attacks on such DVS-based systems, and proposes R-SNN, a novel\nmethodology for robustifying SNNs through efficient DVS-noise filtering. We are\nthe first to generate adversarial attacks on DVS signals (i.e., frames of\nevents in the spatio-temporal domain) and to apply noise filters for DVS\nsensors in the quest for defending against adversarial attacks. Our results\nshow that the noise filters effectively prevent the SNNs from being fooled. The\nSNNs in our experiments provide more than 90% accuracy on the DVS-Gesture and\nNMNIST datasets under different adversarial threat models."
  },
  {
    "id": "arxiv-74",
    "title": "Application-oriented mathematical algorithms for group testing",
    "abstract": "We have a large number of samples and we want to find the infected ones using\nas few number of tests as possible. We can use group testing which tells about\na small group of people whether at least one of them is infected. Group testing\nis particularly efficient if the infection rate is low. The goal of this\narticle is to summarize and extend the mathematical knowledge about the most\nefficient group testing algorithms, focusing on real-life applications instead\nof pure mathematical motivations and approaches.",
    "text": "Application-oriented mathematical algorithms for group testing\n\nWe have a large number of samples and we want to find the infected ones using\nas few number of tests as possible. We can use group testing which tells about\na small group of people whether at least one of them is infected. Group testing\nis particularly efficient if the infection rate is low. The goal of this\narticle is to summarize and extend the mathematical knowledge about the most\nefficient group testing algorithms, focusing on real-life applications instead\nof pure mathematical motivations and approaches."
  },
  {
    "id": "arxiv-75",
    "title": "Towards a Human-like Open-Domain Chatbot",
    "abstract": "We present Meena, a multi-turn open-domain chatbot trained end-to-end on data\nmined and filtered from public domain social media conversations. This 2.6B\nparameter neural network is simply trained to minimize perplexity of the next\ntoken. We also propose a human evaluation metric called Sensibleness and\nSpecificity Average (SSA), which captures key elements of a human-like\nmulti-turn conversation. Our experiments show strong correlation between\nperplexity and SSA. The fact that the best perplexity end-to-end trained Meena\nscores high on SSA (72% on multi-turn evaluation) suggests that a human-level\nSSA of 86% is potentially within reach if we can better optimize perplexity.\nAdditionally, the full version of Meena (with a filtering mechanism and tuned\ndecoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots\nwe evaluated.",
    "text": "Towards a Human-like Open-Domain Chatbot\n\nWe present Meena, a multi-turn open-domain chatbot trained end-to-end on data\nmined and filtered from public domain social media conversations. This 2.6B\nparameter neural network is simply trained to minimize perplexity of the next\ntoken. We also propose a human evaluation metric called Sensibleness and\nSpecificity Average (SSA), which captures key elements of a human-like\nmulti-turn conversation. Our experiments show strong correlation between\nperplexity and SSA. The fact that the best perplexity end-to-end trained Meena\nscores high on SSA (72% on multi-turn evaluation) suggests that a human-level\nSSA of 86% is potentially within reach if we can better optimize perplexity.\nAdditionally, the full version of Meena (with a filtering mechanism and tuned\ndecoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots\nwe evaluated."
  },
  {
    "id": "arxiv-76",
    "title": "Dynamic Mode Decomposition based feature for Image Classification",
    "abstract": "Irrespective of the fact that Machine learning has produced groundbreaking\nresults, it demands an enormous amount of data in order to perform so. Even\nthough data production has been in its all-time high, almost all the data is\nunlabelled, hence making them unsuitable for training the algorithms. This\npaper proposes a novel method of extracting the features using Dynamic Mode\nDecomposition (DMD). The experiment is performed using data samples from\nImagenet. The learning is done using SVM-linear, SVM-RBF, Random Kitchen Sink\napproach (RKS). The results have shown that DMD features with RKS give\ncompeting results.",
    "text": "Dynamic Mode Decomposition based feature for Image Classification\n\nIrrespective of the fact that Machine learning has produced groundbreaking\nresults, it demands an enormous amount of data in order to perform so. Even\nthough data production has been in its all-time high, almost all the data is\nunlabelled, hence making them unsuitable for training the algorithms. This\npaper proposes a novel method of extracting the features using Dynamic Mode\nDecomposition (DMD). The experiment is performed using data samples from\nImagenet. The learning is done using SVM-linear, SVM-RBF, Random Kitchen Sink\napproach (RKS). The results have shown that DMD features with RKS give\ncompeting results."
  },
  {
    "id": "arxiv-77",
    "title": "Field-weighted Factorization Machines for Click-Through Rate Prediction\n  in Display Advertising",
    "abstract": "Click-through rate (CTR) prediction is a critical task in online display\nadvertising. The data involved in CTR prediction are typically multi-field\ncategorical data, i.e., every feature is categorical and belongs to one and\nonly one field. One of the interesting characteristics of such data is that\nfeatures from one field often interact differently with features from different\nother fields. Recently, Field-aware Factorization Machines (FFMs) have been\namong the best performing models for CTR prediction by explicitly modeling such\ndifference. However, the number of parameters in FFMs is in the order of\nfeature number times field number, which is unacceptable in the real-world\nproduction systems. In this paper, we propose Field-weighted Factorization\nMachines (FwFMs) to model the different feature interactions between different\nfields in a much more memory-efficient way. Our experimental evaluations show\nthat FwFMs can achieve competitive prediction performance with only as few as\n4% parameters of FFMs. When using the same number of parameters, FwFMs can\nbring 0.92% and 0.47% AUC lift over FFMs on two real CTR prediction data sets.",
    "text": "Field-weighted Factorization Machines for Click-Through Rate Prediction\n  in Display Advertising\n\nClick-through rate (CTR) prediction is a critical task in online display\nadvertising. The data involved in CTR prediction are typically multi-field\ncategorical data, i.e., every feature is categorical and belongs to one and\nonly one field. One of the interesting characteristics of such data is that\nfeatures from one field often interact differently with features from different\nother fields. Recently, Field-aware Factorization Machines (FFMs) have been\namong the best performing models for CTR prediction by explicitly modeling such\ndifference. However, the number of parameters in FFMs is in the order of\nfeature number times field number, which is unacceptable in the real-world\nproduction systems. In this paper, we propose Field-weighted Factorization\nMachines (FwFMs) to model the different feature interactions between different\nfields in a much more memory-efficient way. Our experimental evaluations show\nthat FwFMs can achieve competitive prediction performance with only as few as\n4% parameters of FFMs. When using the same number of parameters, FwFMs can\nbring 0.92% and 0.47% AUC lift over FFMs on two real CTR prediction data sets."
  },
  {
    "id": "arxiv-78",
    "title": "Variations of Genetic Algorithms",
    "abstract": "The goal of this project is to develop the Genetic Algorithms (GA) for\nsolving the Schaffer F6 function in fewer than 4000 function evaluations on a\ntotal of 30 runs. Four types of Genetic Algorithms (GA) are presented -\nGenerational GA (GGA), Steady-State (mu+1)-GA (SSGA), Steady-Generational\n(mu,mu)-GA (SGGA), and (mu+mu)-GA.",
    "text": "Variations of Genetic Algorithms\n\nThe goal of this project is to develop the Genetic Algorithms (GA) for\nsolving the Schaffer F6 function in fewer than 4000 function evaluations on a\ntotal of 30 runs. Four types of Genetic Algorithms (GA) are presented -\nGenerational GA (GGA), Steady-State (mu+1)-GA (SSGA), Steady-Generational\n(mu,mu)-GA (SGGA), and (mu+mu)-GA."
  },
  {
    "id": "arxiv-79",
    "title": "Low-Memory Neural Network Training: A Technical Report",
    "abstract": "Memory is increasingly often the bottleneck when training neural network\nmodels. Despite this, techniques to lower the overall memory requirements of\ntraining have been less widely studied compared to the extensive literature on\nreducing the memory requirements of inference. In this paper we study a\nfundamental question: How much memory is actually needed to train a neural\nnetwork? To answer this question, we profile the overall memory usage of\ntraining on two representative deep learning benchmarks -- the WideResNet model\nfor image classification and the DynamicConv Transformer model for machine\ntranslation -- and comprehensively evaluate four standard techniques for\nreducing the training memory requirements: (1) imposing sparsity on the model,\n(2) using low precision, (3) microbatching, and (4) gradient checkpointing. We\nexplore how each of these techniques in isolation affects both the peak memory\nusage of training and the quality of the end model, and explore the memory,\naccuracy, and computation tradeoffs incurred when combining these techniques.\nUsing appropriate combinations of these techniques, we show that it is possible\nto the reduce the memory required to train a WideResNet-28-2 on CIFAR-10 by up\nto 60.7x with a 0.4% loss in accuracy, and reduce the memory required to train\na DynamicConv model on IWSLT'14 German to English translation by up to 8.7x\nwith a BLEU score drop of 0.15.",
    "text": "Low-Memory Neural Network Training: A Technical Report\n\nMemory is increasingly often the bottleneck when training neural network\nmodels. Despite this, techniques to lower the overall memory requirements of\ntraining have been less widely studied compared to the extensive literature on\nreducing the memory requirements of inference. In this paper we study a\nfundamental question: How much memory is actually needed to train a neural\nnetwork? To answer this question, we profile the overall memory usage of\ntraining on two representative deep learning benchmarks -- the WideResNet model\nfor image classification and the DynamicConv Transformer model for machine\ntranslation -- and comprehensively evaluate four standard techniques for\nreducing the training memory requirements: (1) imposing sparsity on the model,\n(2) using low precision, (3) microbatching, and (4) gradient checkpointing. We\nexplore how each of these techniques in isolation affects both the peak memory\nusage of training and the quality of the end model, and explore the memory,\naccuracy, and computation tradeoffs incurred when combining these techniques.\nUsing appropriate combinations of these techniques, we show that it is possible\nto the reduce the memory required to train a WideResNet-28-2 on CIFAR-10 by up\nto 60.7x with a 0.4% loss in accuracy, and reduce the memory required to train\na DynamicConv model on IWSLT'14 German to English translation by up to 8.7x\nwith a BLEU score drop of 0.15."
  },
  {
    "id": "arxiv-80",
    "title": "A Machine Learning Approach to Modeling Human Migration",
    "abstract": "Human migration is a type of human mobility, where a trip involves a person\nmoving with the intention of changing their home location. Predicting human\nmigration as accurately as possible is important in city planning applications,\ninternational trade, spread of infectious diseases, conservation planning, and\npublic policy development. Traditional human mobility models, such as gravity\nmodels or the more recent radiation model, predict human mobility flows based\non population and distance features only. These models have been validated on\ncommuting flows, a different type of human mobility, and are mainly used in\nmodeling scenarios where large amounts of prior ground truth mobility data are\nnot available. One downside of these models is that they have a fixed form and\nare therefore not able to capture more complicated migration dynamics. We\npropose machine learning models that are able to incorporate any number of\nexogenous features, to predict origin/destination human migration flows. Our\nmachine learning models outperform traditional human mobility models on a\nvariety of evaluation metrics, both in the task of predicting migrations\nbetween US counties as well as international migrations. In general, predictive\nmachine learning models of human migration will provide a flexible base with\nwhich to model human migration under different what-if conditions, such as\npotential sea level rise or population growth scenarios.",
    "text": "A Machine Learning Approach to Modeling Human Migration\n\nHuman migration is a type of human mobility, where a trip involves a person\nmoving with the intention of changing their home location. Predicting human\nmigration as accurately as possible is important in city planning applications,\ninternational trade, spread of infectious diseases, conservation planning, and\npublic policy development. Traditional human mobility models, such as gravity\nmodels or the more recent radiation model, predict human mobility flows based\non population and distance features only. These models have been validated on\ncommuting flows, a different type of human mobility, and are mainly used in\nmodeling scenarios where large amounts of prior ground truth mobility data are\nnot available. One downside of these models is that they have a fixed form and\nare therefore not able to capture more complicated migration dynamics. We\npropose machine learning models that are able to incorporate any number of\nexogenous features, to predict origin/destination human migration flows. Our\nmachine learning models outperform traditional human mobility models on a\nvariety of evaluation metrics, both in the task of predicting migrations\nbetween US counties as well as international migrations. In general, predictive\nmachine learning models of human migration will provide a flexible base with\nwhich to model human migration under different what-if conditions, such as\npotential sea level rise or population growth scenarios."
  },
  {
    "id": "arxiv-81",
    "title": "A Credibility-aware Swarm-Federated Deep Learning Framework in Internet\n  of Vehicles",
    "abstract": "Federated Deep Learning (FDL) is helping to realize distributed machine\nlearning in the Internet of Vehicles (IoV). However, FDL's global model needs\nmultiple clients to upload learning model parameters, thus still existing\nunavoidable communication overhead and data privacy risks. The recently\nproposed Swarm Learning (SL) provides a decentralized machine-learning approach\nuniting edge computing and blockchain-based coordination without the need for a\ncentral coordinator. This paper proposes a Swarm-Federated Deep Learning\nframework in the IoV system (IoV-SFDL) that integrates SL into the FDL\nframework. The IoV-SFDL organizes vehicles to generate local SL models with\nadjacent vehicles based on the blockchain empowered SL, then aggregates the\nglobal FDL model among different SL groups with a proposed credibility weights\nprediction algorithm. Extensive experimental results demonstrate that compared\nwith the baseline frameworks, the proposed IoV-SFDL framework achieves a 16.72%\nreduction in edge-to-global communication overhead while improving about 5.02%\nin model performance with the same training iterations.",
    "text": "A Credibility-aware Swarm-Federated Deep Learning Framework in Internet\n  of Vehicles\n\nFederated Deep Learning (FDL) is helping to realize distributed machine\nlearning in the Internet of Vehicles (IoV). However, FDL's global model needs\nmultiple clients to upload learning model parameters, thus still existing\nunavoidable communication overhead and data privacy risks. The recently\nproposed Swarm Learning (SL) provides a decentralized machine-learning approach\nuniting edge computing and blockchain-based coordination without the need for a\ncentral coordinator. This paper proposes a Swarm-Federated Deep Learning\nframework in the IoV system (IoV-SFDL) that integrates SL into the FDL\nframework. The IoV-SFDL organizes vehicles to generate local SL models with\nadjacent vehicles based on the blockchain empowered SL, then aggregates the\nglobal FDL model among different SL groups with a proposed credibility weights\nprediction algorithm. Extensive experimental results demonstrate that compared\nwith the baseline frameworks, the proposed IoV-SFDL framework achieves a 16.72%\nreduction in edge-to-global communication overhead while improving about 5.02%\nin model performance with the same training iterations."
  },
  {
    "id": "arxiv-82",
    "title": "LAME: Layout Aware Metadata Extraction Approach for Research Articles",
    "abstract": "The volume of academic literature, such as academic conference papers and\njournals, has increased rapidly worldwide, and research on metadata extraction\nis ongoing. However, high-performing metadata extraction is still challenging\ndue to diverse layout formats according to journal publishers. To accommodate\nthe diversity of the layouts of academic journals, we propose a novel\nLAyout-aware Metadata Extraction (LAME) framework equipped with the three\ncharacteristics (e.g., design of an automatic layout analysis, construction of\na large meta-data training set, and construction of Layout-MetaBERT). We\ndesigned an automatic layout analysis using PDFMiner. Based on the layout\nanalysis, a large volume of metadata-separated training data, including the\ntitle, abstract, author name, author affiliated organization, and keywords,\nwere automatically extracted. Moreover, we constructed Layout-MetaBERT to\nextract the metadata from academic journals with varying layout formats. The\nexperimental results with Layout-MetaBERT exhibited robust performance\n(Macro-F1, 93.27%) in metadata extraction for unseen journals with different\nlayout formats.",
    "text": "LAME: Layout Aware Metadata Extraction Approach for Research Articles\n\nThe volume of academic literature, such as academic conference papers and\njournals, has increased rapidly worldwide, and research on metadata extraction\nis ongoing. However, high-performing metadata extraction is still challenging\ndue to diverse layout formats according to journal publishers. To accommodate\nthe diversity of the layouts of academic journals, we propose a novel\nLAyout-aware Metadata Extraction (LAME) framework equipped with the three\ncharacteristics (e.g., design of an automatic layout analysis, construction of\na large meta-data training set, and construction of Layout-MetaBERT). We\ndesigned an automatic layout analysis using PDFMiner. Based on the layout\nanalysis, a large volume of metadata-separated training data, including the\ntitle, abstract, author name, author affiliated organization, and keywords,\nwere automatically extracted. Moreover, we constructed Layout-MetaBERT to\nextract the metadata from academic journals with varying layout formats. The\nexperimental results with Layout-MetaBERT exhibited robust performance\n(Macro-F1, 93.27%) in metadata extraction for unseen journals with different\nlayout formats."
  },
  {
    "id": "arxiv-83",
    "title": "The Landscape of the Planted Clique Problem: Dense subgraphs and the\n  Overlap Gap Property",
    "abstract": "In this paper we study the computational-statistical gap of the planted\nclique problem, where a clique of size $k$ is planted in an Erdos Renyi graph\n$G(n,\\frac{1}{2})$ resulting in a graph $G\\left(n,\\frac{1}{2},k\\right)$. The\ngoal is to recover the planted clique vertices by observing\n$G\\left(n,\\frac{1}{2},k\\right)$ . It is known that the clique can be recovered\nas long as $k \\geq \\left(2+\\epsilon\\right)\\log n $ for any $\\epsilon>0$, but no\npolynomial-time algorithm is known for this task unless $k=\\Omega\\left(\\sqrt{n}\n\\right)$. Following a statistical-physics inspired point of view as an attempt\nto understand this computational-statistical gap, we study the landscape of the\n\"sufficiently dense\" subgraphs of $G$ and their overlap with the planted\nclique.\n  Using the first moment method, we study the densest subgraph problems for\nsubgraphs with fixed, but arbitrary, overlap size with the planted clique, and\nprovide evidence of a phase transition for the presence of Overlap Gap Property\n(OGP) at $k=\\Theta\\left(\\sqrt{n}\\right)$. OGP is a concept introduced\noriginally in spin glass theory and known to suggest algorithmic hardness when\nit appears. We establish the presence of OGP when $k$ is a small positive power\nof $n$ by using a conditional second moment method. As our main technical tool,\nwe establish the first, to the best of our knowledge, concentration results for\nthe $K$-densest subgraph problem for the Erdos-Renyi model\n$G\\left(n,\\frac{1}{2}\\right)$ when $K=n^{0.5-\\epsilon}$ for arbitrary\n$\\epsilon>0$. Finally, to study the OGP we employ a certain form of\noverparametrization, which is conceptually aligned with a large body of recent\nwork in learning theory and optimization.",
    "text": "The Landscape of the Planted Clique Problem: Dense subgraphs and the\n  Overlap Gap Property\n\nIn this paper we study the computational-statistical gap of the planted\nclique problem, where a clique of size $k$ is planted in an Erdos Renyi graph\n$G(n,\\frac{1}{2})$ resulting in a graph $G\\left(n,\\frac{1}{2},k\\right)$. The\ngoal is to recover the planted clique vertices by observing\n$G\\left(n,\\frac{1}{2},k\\right)$ . It is known that the clique can be recovered\nas long as $k \\geq \\left(2+\\epsilon\\right)\\log n $ for any $\\epsilon>0$, but no\npolynomial-time algorithm is known for this task unless $k=\\Omega\\left(\\sqrt{n}\n\\right)$. Following a statistical-physics inspired point of view as an attempt\nto understand this computational-statistical gap, we study the landscape of the\n\"sufficiently dense\" subgraphs of $G$ and their overlap with the planted\nclique.\n  Using the first moment method, we study the densest subgraph problems for\nsubgraphs with fixed, but arbitrary, overlap size with the planted clique, and\nprovide evidence of a phase transition for the presence of Overlap Gap Property\n(OGP) at $k=\\Theta\\left(\\sqrt{n}\\right)$. OGP is a concept introduced\noriginally in spin glass theory and known to suggest algorithmic hardness when\nit appears. We establish the presence of OGP when $k$ is a small positive power\nof $n$ by using a conditional second moment method. As our main technical tool,\nwe establish the first, to the best of our knowledge, concentration results for\nthe $K$-densest subgraph problem for the Erdos-Renyi model\n$G\\left(n,\\frac{1}{2}\\right)$ when $K=n^{0.5-\\epsilon}$ for arbitrary\n$\\epsilon>0$. Finally, to study the OGP we employ a certain form of\noverparametrization, which is conceptually aligned with a large body of recent\nwork in learning theory and optimization."
  },
  {
    "id": "arxiv-84",
    "title": "Generalisation and the Risk--Entropy Curve",
    "abstract": "In this paper we show that the expected generalisation performance of a\nlearning machine is determined by the distribution of risks or equivalently its\nlogarithm -- a quantity we term the risk entropy -- and the fluctuations in a\nquantity we call the training ratio. We show that the risk entropy can be\nempirically inferred for deep neural network models using Markov Chain Monte\nCarlo techniques. Results are presented for different deep neural networks on a\nvariety of problems. The asymptotic behaviour of the risk entropy acts in an\nanalogous way to the capacity of the learning machine, but the generalisation\nperformance experienced in practical situations is determined by the behaviour\nof the risk entropy before the asymptotic regime is reached. This performance\nis strongly dependent on the distribution of the data (features and targets)\nand not just on the capacity of the learning machine.",
    "text": "Generalisation and the Risk--Entropy Curve\n\nIn this paper we show that the expected generalisation performance of a\nlearning machine is determined by the distribution of risks or equivalently its\nlogarithm -- a quantity we term the risk entropy -- and the fluctuations in a\nquantity we call the training ratio. We show that the risk entropy can be\nempirically inferred for deep neural network models using Markov Chain Monte\nCarlo techniques. Results are presented for different deep neural networks on a\nvariety of problems. The asymptotic behaviour of the risk entropy acts in an\nanalogous way to the capacity of the learning machine, but the generalisation\nperformance experienced in practical situations is determined by the behaviour\nof the risk entropy before the asymptotic regime is reached. This performance\nis strongly dependent on the distribution of the data (features and targets)\nand not just on the capacity of the learning machine."
  },
  {
    "id": "arxiv-85",
    "title": "Gaussian Process Uniform Error Bounds with Unknown Hyperparameters for\n  Safety-Critical Applications",
    "abstract": "Gaussian processes have become a promising tool for various safety-critical\nsettings, since the posterior variance can be used to directly estimate the\nmodel error and quantify risk. However, state-of-the-art techniques for\nsafety-critical settings hinge on the assumption that the kernel\nhyperparameters are known, which does not apply in general. To mitigate this,\nwe introduce robust Gaussian process uniform error bounds in settings with\nunknown hyperparameters. Our approach computes a confidence region in the space\nof hyperparameters, which enables us to obtain a probabilistic upper bound for\nthe model error of a Gaussian process with arbitrary hyperparameters. We do not\nrequire to know any bounds for the hyperparameters a priori, which is an\nassumption commonly found in related work. Instead, we are able to derive\nbounds from data in an intuitive fashion. We additionally employ the proposed\ntechnique to derive performance guarantees for a class of learning-based\ncontrol problems. Experiments show that the bound performs significantly better\nthan vanilla and fully Bayesian Gaussian processes.",
    "text": "Gaussian Process Uniform Error Bounds with Unknown Hyperparameters for\n  Safety-Critical Applications\n\nGaussian processes have become a promising tool for various safety-critical\nsettings, since the posterior variance can be used to directly estimate the\nmodel error and quantify risk. However, state-of-the-art techniques for\nsafety-critical settings hinge on the assumption that the kernel\nhyperparameters are known, which does not apply in general. To mitigate this,\nwe introduce robust Gaussian process uniform error bounds in settings with\nunknown hyperparameters. Our approach computes a confidence region in the space\nof hyperparameters, which enables us to obtain a probabilistic upper bound for\nthe model error of a Gaussian process with arbitrary hyperparameters. We do not\nrequire to know any bounds for the hyperparameters a priori, which is an\nassumption commonly found in related work. Instead, we are able to derive\nbounds from data in an intuitive fashion. We additionally employ the proposed\ntechnique to derive performance guarantees for a class of learning-based\ncontrol problems. Experiments show that the bound performs significantly better\nthan vanilla and fully Bayesian Gaussian processes."
  },
  {
    "id": "arxiv-86",
    "title": "Investigating Under and Overfitting in Wasserstein Generative\n  Adversarial Networks",
    "abstract": "We investigate under and overfitting in Generative Adversarial Networks\n(GANs), using discriminators unseen by the generator to measure generalization.\nWe find that the model capacity of the discriminator has a significant effect\non the generator's model quality, and that the generator's poor performance\ncoincides with the discriminator underfitting. Contrary to our expectations, we\nfind that generators with large model capacities relative to the discriminator\ndo not show evidence of overfitting on CIFAR10, CIFAR100, and CelebA.",
    "text": "Investigating Under and Overfitting in Wasserstein Generative\n  Adversarial Networks\n\nWe investigate under and overfitting in Generative Adversarial Networks\n(GANs), using discriminators unseen by the generator to measure generalization.\nWe find that the model capacity of the discriminator has a significant effect\non the generator's model quality, and that the generator's poor performance\ncoincides with the discriminator underfitting. Contrary to our expectations, we\nfind that generators with large model capacities relative to the discriminator\ndo not show evidence of overfitting on CIFAR10, CIFAR100, and CelebA."
  },
  {
    "id": "arxiv-87",
    "title": "The Role of Linear Layers in Nonlinear Interpolating Networks",
    "abstract": "This paper explores the implicit bias of overparameterized neural networks of\ndepth greater than two layers. Our framework considers a family of networks of\nvarying depth that all have the same capacity but different implicitly defined\nrepresentation costs. The representation cost of a function induced by a neural\nnetwork architecture is the minimum sum of squared weights needed for the\nnetwork to represent the function; it reflects the function space bias\nassociated with the architecture. Our results show that adding linear layers to\na ReLU network yields a representation cost that reflects a complex interplay\nbetween the alignment and sparsity of ReLU units. Specifically, using a neural\nnetwork to fit training data with minimum representation cost yields an\ninterpolating function that is constant in directions perpendicular to a\nlow-dimensional subspace on which a parsimonious interpolant exists.",
    "text": "The Role of Linear Layers in Nonlinear Interpolating Networks\n\nThis paper explores the implicit bias of overparameterized neural networks of\ndepth greater than two layers. Our framework considers a family of networks of\nvarying depth that all have the same capacity but different implicitly defined\nrepresentation costs. The representation cost of a function induced by a neural\nnetwork architecture is the minimum sum of squared weights needed for the\nnetwork to represent the function; it reflects the function space bias\nassociated with the architecture. Our results show that adding linear layers to\na ReLU network yields a representation cost that reflects a complex interplay\nbetween the alignment and sparsity of ReLU units. Specifically, using a neural\nnetwork to fit training data with minimum representation cost yields an\ninterpolating function that is constant in directions perpendicular to a\nlow-dimensional subspace on which a parsimonious interpolant exists."
  },
  {
    "id": "arxiv-88",
    "title": "Identity Matters in Deep Learning",
    "abstract": "An emerging design principle in deep learning is that each layer of a deep\nartificial neural network should be able to easily express the identity\ntransformation. This idea not only motivated various normalization techniques,\nsuch as \\emph{batch normalization}, but was also key to the immense success of\n\\emph{residual networks}.\n  In this work, we put the principle of \\emph{identity parameterization} on a\nmore solid theoretical footing alongside further empirical progress. We first\ngive a strikingly simple proof that arbitrarily deep linear residual networks\nhave no spurious local optima. The same result for linear feed-forward networks\nin their standard parameterization is substantially more delicate. Second, we\nshow that residual networks with ReLu activations have universal finite-sample\nexpressivity in the sense that the network can represent any function of its\nsample provided that the model has more parameters than the sample size.\n  Directly inspired by our theory, we experiment with a radically simple\nresidual architecture consisting of only residual convolutional layers and ReLu\nactivations, but no batch normalization, dropout, or max pool. Our model\nimproves significantly on previous all-convolutional networks on the CIFAR10,\nCIFAR100, and ImageNet classification benchmarks.",
    "text": "Identity Matters in Deep Learning\n\nAn emerging design principle in deep learning is that each layer of a deep\nartificial neural network should be able to easily express the identity\ntransformation. This idea not only motivated various normalization techniques,\nsuch as \\emph{batch normalization}, but was also key to the immense success of\n\\emph{residual networks}.\n  In this work, we put the principle of \\emph{identity parameterization} on a\nmore solid theoretical footing alongside further empirical progress. We first\ngive a strikingly simple proof that arbitrarily deep linear residual networks\nhave no spurious local optima. The same result for linear feed-forward networks\nin their standard parameterization is substantially more delicate. Second, we\nshow that residual networks with ReLu activations have universal finite-sample\nexpressivity in the sense that the network can represent any function of its\nsample provided that the model has more parameters than the sample size.\n  Directly inspired by our theory, we experiment with a radically simple\nresidual architecture consisting of only residual convolutional layers and ReLu\nactivations, but no batch normalization, dropout, or max pool. Our model\nimproves significantly on previous all-convolutional networks on the CIFAR10,\nCIFAR100, and ImageNet classification benchmarks."
  },
  {
    "id": "arxiv-89",
    "title": "Challenges and approaches to privacy preserving post-click conversion\n  prediction",
    "abstract": "Online advertising has typically been more personalized than offline\nadvertising, through the use of machine learning models and real-time auctions\nfor ad targeting. One specific task, predicting the likelihood of conversion\n(i.e.\\ the probability a user will purchase the advertised product), is crucial\nto the advertising ecosystem for both targeting and pricing ads. Currently,\nthese models are often trained by observing individual user behavior, but,\nincreasingly, regulatory and technical constraints are requiring\nprivacy-preserving approaches. For example, major platforms are moving to\nrestrict tracking individual user events across multiple applications, and\ngovernments around the world have shown steadily more interest in regulating\nthe use of personal data. Instead of receiving data about individual user\nbehavior, advertisers may receive privacy-preserving feedback, such as the\nnumber of installs of an advertised app that resulted from a group of users. In\nthis paper we outline the recent privacy-related changes in the online\nadvertising ecosystem from a machine learning perspective. We provide an\noverview of the challenges and constraints when learning conversion models in\nthis setting. We introduce a novel approach for training these models that\nmakes use of post-ranking signals. We show using offline experiments on real\nworld data that it outperforms a model relying on opt-in data alone, and\nsignificantly reduces model degradation when no individual labels are\navailable. Finally, we discuss future directions for research in this evolving\narea.",
    "text": "Challenges and approaches to privacy preserving post-click conversion\n  prediction\n\nOnline advertising has typically been more personalized than offline\nadvertising, through the use of machine learning models and real-time auctions\nfor ad targeting. One specific task, predicting the likelihood of conversion\n(i.e.\\ the probability a user will purchase the advertised product), is crucial\nto the advertising ecosystem for both targeting and pricing ads. Currently,\nthese models are often trained by observing individual user behavior, but,\nincreasingly, regulatory and technical constraints are requiring\nprivacy-preserving approaches. For example, major platforms are moving to\nrestrict tracking individual user events across multiple applications, and\ngovernments around the world have shown steadily more interest in regulating\nthe use of personal data. Instead of receiving data about individual user\nbehavior, advertisers may receive privacy-preserving feedback, such as the\nnumber of installs of an advertised app that resulted from a group of users. In\nthis paper we outline the recent privacy-related changes in the online\nadvertising ecosystem from a machine learning perspective. We provide an\noverview of the challenges and constraints when learning conversion models in\nthis setting. We introduce a novel approach for training these models that\nmakes use of post-ranking signals. We show using offline experiments on real\nworld data that it outperforms a model relying on opt-in data alone, and\nsignificantly reduces model degradation when no individual labels are\navailable. Finally, we discuss future directions for research in this evolving\narea."
  },
  {
    "id": "arxiv-90",
    "title": "Accelerated MRI with Un-trained Neural Networks",
    "abstract": "Convolutional Neural Networks (CNNs) are highly effective for image\nreconstruction problems. Typically, CNNs are trained on large amounts of\ntraining images. Recently, however, un-trained CNNs such as the Deep Image\nPrior and Deep Decoder have achieved excellent performance for image\nreconstruction problems such as denoising and inpainting, \\emph{without using\nany training data}. Motivated by this development, we address the\nreconstruction problem arising in accelerated MRI with un-trained neural\nnetworks. We propose a highly optimized un-trained recovery approach based on a\nvariation of the Deep Decoder and show that it significantly outperforms other\nun-trained methods, in particular sparsity-based classical compressed sensing\nmethods and naive applications of un-trained neural networks. We also compare\nperformance (both in terms of reconstruction accuracy and computational cost)\nin an ideal setup for trained methods, specifically on the fastMRI dataset,\nwhere the training and test data come from the same distribution. We find that\nour un-trained algorithm achieves similar performance to a baseline trained\nneural network, but a state-of-the-art trained network outperforms the\nun-trained one. Finally, we perform a comparison on a non-ideal setup where the\ntrain and test distributions are slightly different, and find that our\nun-trained method achieves similar performance to a state-of-the-art\naccelerated MRI reconstruction method.",
    "text": "Accelerated MRI with Un-trained Neural Networks\n\nConvolutional Neural Networks (CNNs) are highly effective for image\nreconstruction problems. Typically, CNNs are trained on large amounts of\ntraining images. Recently, however, un-trained CNNs such as the Deep Image\nPrior and Deep Decoder have achieved excellent performance for image\nreconstruction problems such as denoising and inpainting, \\emph{without using\nany training data}. Motivated by this development, we address the\nreconstruction problem arising in accelerated MRI with un-trained neural\nnetworks. We propose a highly optimized un-trained recovery approach based on a\nvariation of the Deep Decoder and show that it significantly outperforms other\nun-trained methods, in particular sparsity-based classical compressed sensing\nmethods and naive applications of un-trained neural networks. We also compare\nperformance (both in terms of reconstruction accuracy and computational cost)\nin an ideal setup for trained methods, specifically on the fastMRI dataset,\nwhere the training and test data come from the same distribution. We find that\nour un-trained algorithm achieves similar performance to a baseline trained\nneural network, but a state-of-the-art trained network outperforms the\nun-trained one. Finally, we perform a comparison on a non-ideal setup where the\ntrain and test distributions are slightly different, and find that our\nun-trained method achieves similar performance to a state-of-the-art\naccelerated MRI reconstruction method."
  },
  {
    "id": "arxiv-91",
    "title": "Multi Expression Programming -- an in-depth description",
    "abstract": "Multi Expression Programming (MEP) is a Genetic Programming variant that uses\na linear representation of chromosomes. MEP individuals are strings of genes\nencoding complex computer programs. When MEP individuals encode expressions,\ntheir representation is similar to the way in which compilers translate $C$ or\n$Pascal$ expressions into machine code. A unique MEP feature is the ability to\nstore multiple solutions of a problem in a single chromosome. Usually, the best\nsolution is chosen for fitness assignment. When solving symbolic regression or\nclassification problems (or any other problems for which the training set is\nknown before the problem is solved) MEP has the same complexity as other\ntechniques storing a single solution in a chromosome (such as GP, CGP, GEP or\nGE). Evaluation of the expressions encoded into an MEP individual can be\nperformed by a single parsing of the chromosome. Offspring obtained by\ncrossover and mutation is always syntactically correct MEP individuals\n(computer programs). Thus, no extra processing for repairing newly obtained\nindividuals is needed.",
    "text": "Multi Expression Programming -- an in-depth description\n\nMulti Expression Programming (MEP) is a Genetic Programming variant that uses\na linear representation of chromosomes. MEP individuals are strings of genes\nencoding complex computer programs. When MEP individuals encode expressions,\ntheir representation is similar to the way in which compilers translate $C$ or\n$Pascal$ expressions into machine code. A unique MEP feature is the ability to\nstore multiple solutions of a problem in a single chromosome. Usually, the best\nsolution is chosen for fitness assignment. When solving symbolic regression or\nclassification problems (or any other problems for which the training set is\nknown before the problem is solved) MEP has the same complexity as other\ntechniques storing a single solution in a chromosome (such as GP, CGP, GEP or\nGE). Evaluation of the expressions encoded into an MEP individual can be\nperformed by a single parsing of the chromosome. Offspring obtained by\ncrossover and mutation is always syntactically correct MEP individuals\n(computer programs). Thus, no extra processing for repairing newly obtained\nindividuals is needed."
  },
  {
    "id": "arxiv-92",
    "title": "Understanding and Accelerating Neural Architecture Search with\n  Training-Free and Theory-Grounded Metrics",
    "abstract": "This work targets designing a principled and unified training-free framework\nfor Neural Architecture Search (NAS), with high performance, low cost, and\nin-depth interpretation. NAS has been explosively studied to automate the\ndiscovery of top-performer neural networks, but suffers from heavy resource\nconsumption and often incurs search bias due to truncated training or\napproximations. Recent NAS works start to explore indicators that can predict a\nnetwork's performance without training. However, they either leveraged limited\nproperties of deep networks, or the benefits of their training-free indicators\nare not applied to more extensive search methods. By rigorous correlation\nanalysis, we present a unified framework to understand and accelerate NAS, by\ndisentangling \"TEG\" characteristics of searched networks - Trainability,\nExpressivity, Generalization - all assessed in a training-free manner. The TEG\nindicators could be scaled up and integrated with various NAS search methods,\nincluding both supernet and single-path approaches. Extensive studies validate\nthe effective and efficient guidance from our TEG-NAS framework, leading to\nboth improved search accuracy and over 2.3x reduction in search time cost.\nMoreover, we visualize search trajectories on three landscapes of \"TEG\"\ncharacteristics, observing that while a good local minimum is easier to find on\nNAS-Bench-201 given its simple topology, balancing \"TEG\" characteristics is\nmuch harder on the DARTS search space due to its complex landscape geometry.\nOur code is available at https://github.com/VITA-Group/TEGNAS.",
    "text": "Understanding and Accelerating Neural Architecture Search with\n  Training-Free and Theory-Grounded Metrics\n\nThis work targets designing a principled and unified training-free framework\nfor Neural Architecture Search (NAS), with high performance, low cost, and\nin-depth interpretation. NAS has been explosively studied to automate the\ndiscovery of top-performer neural networks, but suffers from heavy resource\nconsumption and often incurs search bias due to truncated training or\napproximations. Recent NAS works start to explore indicators that can predict a\nnetwork's performance without training. However, they either leveraged limited\nproperties of deep networks, or the benefits of their training-free indicators\nare not applied to more extensive search methods. By rigorous correlation\nanalysis, we present a unified framework to understand and accelerate NAS, by\ndisentangling \"TEG\" characteristics of searched networks - Trainability,\nExpressivity, Generalization - all assessed in a training-free manner. The TEG\nindicators could be scaled up and integrated with various NAS search methods,\nincluding both supernet and single-path approaches. Extensive studies validate\nthe effective and efficient guidance from our TEG-NAS framework, leading to\nboth improved search accuracy and over 2.3x reduction in search time cost.\nMoreover, we visualize search trajectories on three landscapes of \"TEG\"\ncharacteristics, observing that while a good local minimum is easier to find on\nNAS-Bench-201 given its simple topology, balancing \"TEG\" characteristics is\nmuch harder on the DARTS search space due to its complex landscape geometry.\nOur code is available at https://github.com/VITA-Group/TEGNAS."
  },
  {
    "id": "arxiv-93",
    "title": "Guitar Effects Recognition and Parameter Estimation with Convolutional\n  Neural Networks",
    "abstract": "Despite the popularity of guitar effects, there is very little existing\nresearch on classification and parameter estimation of specific plugins or\neffect units from guitar recordings. In this paper, convolutional neural\nnetworks were used for classification and parameter estimation for 13\noverdrive, distortion and fuzz guitar effects. A novel dataset of processed\nelectric guitar samples was assembled, with four sub-datasets consisting of\nmonophonic or polyphonic samples and discrete or continuous settings values,\nfor a total of about 250 hours of processed samples. Results were compared for\nnetworks trained and tested on the same or on a different sub-dataset. We found\nthat discrete datasets could lead to equally high performance as continuous\nones, whilst being easier to design, analyse and modify. Classification\naccuracy was above 80\\%, with confusion matrices reflecting similarities in the\neffects timbre and circuits design. With parameter values between 0.0 and 1.0,\nthe mean absolute error is in most cases below 0.05, while the root mean square\nerror is below 0.1 in all cases but one.",
    "text": "Guitar Effects Recognition and Parameter Estimation with Convolutional\n  Neural Networks\n\nDespite the popularity of guitar effects, there is very little existing\nresearch on classification and parameter estimation of specific plugins or\neffect units from guitar recordings. In this paper, convolutional neural\nnetworks were used for classification and parameter estimation for 13\noverdrive, distortion and fuzz guitar effects. A novel dataset of processed\nelectric guitar samples was assembled, with four sub-datasets consisting of\nmonophonic or polyphonic samples and discrete or continuous settings values,\nfor a total of about 250 hours of processed samples. Results were compared for\nnetworks trained and tested on the same or on a different sub-dataset. We found\nthat discrete datasets could lead to equally high performance as continuous\nones, whilst being easier to design, analyse and modify. Classification\naccuracy was above 80\\%, with confusion matrices reflecting similarities in the\neffects timbre and circuits design. With parameter values between 0.0 and 1.0,\nthe mean absolute error is in most cases below 0.05, while the root mean square\nerror is below 0.1 in all cases but one."
  },
  {
    "id": "arxiv-94",
    "title": "Neural Networks with Physics-Informed Architectures and Constraints for\n  Dynamical Systems Modeling",
    "abstract": "Effective inclusion of physics-based knowledge into deep neural network\nmodels of dynamical systems can greatly improve data efficiency and\ngeneralization. Such a-priori knowledge might arise from physical principles\n(e.g., conservation laws) or from the system's design (e.g., the Jacobian\nmatrix of a robot), even if large portions of the system dynamics remain\nunknown. We develop a framework to learn dynamics models from trajectory data\nwhile incorporating a-priori system knowledge as inductive bias. More\nspecifically, the proposed framework uses physics-based side information to\ninform the structure of the neural network itself, and to place constraints on\nthe values of the outputs and the internal states of the model. It represents\nthe system's vector field as a composition of known and unknown functions, the\nlatter of which are parametrized by neural networks. The physics-informed\nconstraints are enforced via the augmented Lagrangian method during the model's\ntraining. We experimentally demonstrate the benefits of the proposed approach\non a variety of dynamical systems -- including a benchmark suite of robotics\nenvironments featuring large state spaces, non-linear dynamics, external\nforces, contact forces, and control inputs. By exploiting a-priori system\nknowledge during training, the proposed approach learns to predict the system\ndynamics two orders of magnitude more accurately than a baseline approach that\ndoes not include prior knowledge, given the same training dataset.",
    "text": "Neural Networks with Physics-Informed Architectures and Constraints for\n  Dynamical Systems Modeling\n\nEffective inclusion of physics-based knowledge into deep neural network\nmodels of dynamical systems can greatly improve data efficiency and\ngeneralization. Such a-priori knowledge might arise from physical principles\n(e.g., conservation laws) or from the system's design (e.g., the Jacobian\nmatrix of a robot), even if large portions of the system dynamics remain\nunknown. We develop a framework to learn dynamics models from trajectory data\nwhile incorporating a-priori system knowledge as inductive bias. More\nspecifically, the proposed framework uses physics-based side information to\ninform the structure of the neural network itself, and to place constraints on\nthe values of the outputs and the internal states of the model. It represents\nthe system's vector field as a composition of known and unknown functions, the\nlatter of which are parametrized by neural networks. The physics-informed\nconstraints are enforced via the augmented Lagrangian method during the model's\ntraining. We experimentally demonstrate the benefits of the proposed approach\non a variety of dynamical systems -- including a benchmark suite of robotics\nenvironments featuring large state spaces, non-linear dynamics, external\nforces, contact forces, and control inputs. By exploiting a-priori system\nknowledge during training, the proposed approach learns to predict the system\ndynamics two orders of magnitude more accurately than a baseline approach that\ndoes not include prior knowledge, given the same training dataset."
  },
  {
    "id": "arxiv-95",
    "title": "MPC-guided Imitation Learning of Neural Network Policies for the\n  Artificial Pancreas",
    "abstract": "Even though model predictive control (MPC) is currently the main algorithm\nfor insulin control in the artificial pancreas (AP), it usually requires\ncomplex online optimizations, which are infeasible for resource-constrained\nmedical devices. MPC also typically relies on state estimation, an error-prone\nprocess. In this paper, we introduce a novel approach to AP control that uses\nImitation Learning to synthesize neural-network insulin policies from\nMPC-computed demonstrations. Such policies are computationally efficient and,\nby instrumenting MPC at training time with full state information, they can\ndirectly map measurements into optimal therapy decisions, thus bypassing state\nestimation. We apply Bayesian inference via Monte Carlo Dropout to learn\npolicies, which allows us to quantify prediction uncertainty and thereby derive\nsafer therapy decisions. We show that our control policies trained under a\nspecific patient model readily generalize (in terms of model parameters and\ndisturbance distributions) to patient cohorts, consistently outperforming\ntraditional MPC with state estimation.",
    "text": "MPC-guided Imitation Learning of Neural Network Policies for the\n  Artificial Pancreas\n\nEven though model predictive control (MPC) is currently the main algorithm\nfor insulin control in the artificial pancreas (AP), it usually requires\ncomplex online optimizations, which are infeasible for resource-constrained\nmedical devices. MPC also typically relies on state estimation, an error-prone\nprocess. In this paper, we introduce a novel approach to AP control that uses\nImitation Learning to synthesize neural-network insulin policies from\nMPC-computed demonstrations. Such policies are computationally efficient and,\nby instrumenting MPC at training time with full state information, they can\ndirectly map measurements into optimal therapy decisions, thus bypassing state\nestimation. We apply Bayesian inference via Monte Carlo Dropout to learn\npolicies, which allows us to quantify prediction uncertainty and thereby derive\nsafer therapy decisions. We show that our control policies trained under a\nspecific patient model readily generalize (in terms of model parameters and\ndisturbance distributions) to patient cohorts, consistently outperforming\ntraditional MPC with state estimation."
  },
  {
    "id": "arxiv-96",
    "title": "Draw your Neural Networks",
    "abstract": "Deep Neural Networks are the basic building blocks of modern Artificial\nIntelligence. They are increasingly replacing or augmenting existing software\nsystems due to their ability to learn directly from the data and superior\naccuracy on variety of tasks. Existing Software Development Life Cycle (SDLC)\nmethodologies fall short on representing the unique capabilities and\nrequirements of AI Development and must be replaced with Artificial\nIntelligence Development Life Cycle (AIDLC) methodologies. In this paper, we\ndiscuss an alternative and more natural approach to develop neural networks\nthat involves intuitive GUI elements such as blocks and lines to draw them\ninstead of complex computer programming. We present Sketch framework, that uses\nthis GUI-based approach to design and modify the neural networks and provides\ninteroperability with traditional frameworks. The system provides popular\nlayers and operations out-of-the-box and could import any supported pre-trained\nmodel making it a faster method to design and train complex neural networks and\nultimately democratizing the AI by removing the learning curve.",
    "text": "Draw your Neural Networks\n\nDeep Neural Networks are the basic building blocks of modern Artificial\nIntelligence. They are increasingly replacing or augmenting existing software\nsystems due to their ability to learn directly from the data and superior\naccuracy on variety of tasks. Existing Software Development Life Cycle (SDLC)\nmethodologies fall short on representing the unique capabilities and\nrequirements of AI Development and must be replaced with Artificial\nIntelligence Development Life Cycle (AIDLC) methodologies. In this paper, we\ndiscuss an alternative and more natural approach to develop neural networks\nthat involves intuitive GUI elements such as blocks and lines to draw them\ninstead of complex computer programming. We present Sketch framework, that uses\nthis GUI-based approach to design and modify the neural networks and provides\ninteroperability with traditional frameworks. The system provides popular\nlayers and operations out-of-the-box and could import any supported pre-trained\nmodel making it a faster method to design and train complex neural networks and\nultimately democratizing the AI by removing the learning curve."
  },
  {
    "id": "arxiv-97",
    "title": "A Methodology for Creating Question Answering Corpora Using Inverse Data\n  Annotation",
    "abstract": "In this paper, we introduce a novel methodology to efficiently construct a\ncorpus for question answering over structured data. For this, we introduce an\nintermediate representation that is based on the logical query plan in a\ndatabase called Operation Trees (OT). This representation allows us to invert\nthe annotation process without losing flexibility in the types of queries that\nwe generate. Furthermore, it allows for fine-grained alignment of query tokens\nto OT operations. In our method, we randomly generate OTs from a context-free\ngrammar. Afterwards, annotators have to write the appropriate natural language\nquestion that is represented by the OT. Finally, the annotators assign the\ntokens to the OT operations. We apply the method to create a new corpus OTTA\n(Operation Trees and Token Assignment), a large semantic parsing corpus for\nevaluating natural language interfaces to databases. We compare OTTA to Spider\nand LC-QuaD 2.0 and show that our methodology more than triples the annotation\nspeed while maintaining the complexity of the queries. Finally, we train a\nstate-of-the-art semantic parsing model on our data and show that our corpus is\na challenging dataset and that the token alignment can be leveraged to increase\nthe performance significantly.",
    "text": "A Methodology for Creating Question Answering Corpora Using Inverse Data\n  Annotation\n\nIn this paper, we introduce a novel methodology to efficiently construct a\ncorpus for question answering over structured data. For this, we introduce an\nintermediate representation that is based on the logical query plan in a\ndatabase called Operation Trees (OT). This representation allows us to invert\nthe annotation process without losing flexibility in the types of queries that\nwe generate. Furthermore, it allows for fine-grained alignment of query tokens\nto OT operations. In our method, we randomly generate OTs from a context-free\ngrammar. Afterwards, annotators have to write the appropriate natural language\nquestion that is represented by the OT. Finally, the annotators assign the\ntokens to the OT operations. We apply the method to create a new corpus OTTA\n(Operation Trees and Token Assignment), a large semantic parsing corpus for\nevaluating natural language interfaces to databases. We compare OTTA to Spider\nand LC-QuaD 2.0 and show that our methodology more than triples the annotation\nspeed while maintaining the complexity of the queries. Finally, we train a\nstate-of-the-art semantic parsing model on our data and show that our corpus is\na challenging dataset and that the token alignment can be leveraged to increase\nthe performance significantly."
  },
  {
    "id": "arxiv-98",
    "title": "Structured Sparse Convolutional Autoencoder",
    "abstract": "This paper aims to improve the feature learning in Convolutional Networks\n(Convnet) by capturing the structure of objects. A new sparsity function is\nimposed on the extracted featuremap to capture the structure and shape of the\nlearned object, extracting interpretable features to improve the prediction\nperformance. The proposed algorithm is based on organizing the activation\nwithin and across featuremap by constraining the node activities through\n$\\ell_{2}$ and $\\ell_{1}$ normalization in a structured form.",
    "text": "Structured Sparse Convolutional Autoencoder\n\nThis paper aims to improve the feature learning in Convolutional Networks\n(Convnet) by capturing the structure of objects. A new sparsity function is\nimposed on the extracted featuremap to capture the structure and shape of the\nlearned object, extracting interpretable features to improve the prediction\nperformance. The proposed algorithm is based on organizing the activation\nwithin and across featuremap by constraining the node activities through\n$\\ell_{2}$ and $\\ell_{1}$ normalization in a structured form."
  },
  {
    "id": "arxiv-99",
    "title": "Online Optimization of Stimulation Speed in an Auditory Brain-Computer\n  Interface under Time Constraints",
    "abstract": "The decoding of brain signals recorded via, e.g., an electroencephalogram,\nusing machine learning is key to brain-computer interfaces (BCIs). Stimulation\nparameters or other experimental settings of the BCI protocol typically are\nchosen according to the literature. The decoding performance directly depends\non the choice of parameters, as they influence the elicited brain signals and\noptimal parameters are subject-dependent. Thus a fast and automated selection\nprocedure for experimental parameters could greatly improve the usability of\nBCIs.\n  We evaluate a standalone random search and a combined Bayesian optimization\nwith random search in a closed-loop auditory event-related potential protocol.\nWe aimed at finding the individually best stimulation speed -- also known as\nstimulus onset asynchrony (SOA) -- that maximizes the classification\nperformance of a regularized linear discriminant analysis. To make the Bayesian\noptimization feasible under noise and the time pressure posed by an online BCI\nexperiment, we first used offline simulations to initialize and constrain the\ninternal optimization model. Then we evaluated our approach online with 13\nhealthy subjects.\n  We could show that for 8 out of 13 subjects, the proposed approach using\nBayesian optimization succeeded to select the individually optimal SOA out of\nmultiple evaluated SOA values. Our data suggests, however, that subjects were\ninfluenced to very different degrees by the SOA parameter. This makes the\nautomatic parameter selection infeasible for subjects where the influence is\nlimited.\n  Our work proposes an approach to exploit the benefits of individualized\nexperimental protocols and evaluated it in an auditory BCI. When applied to\nother experimental parameters our approach could enhance the usability of BCI\nfor different target groups -- specifically if an individual disease progress\nmay prevent the use of standard parameters.",
    "text": "Online Optimization of Stimulation Speed in an Auditory Brain-Computer\n  Interface under Time Constraints\n\nThe decoding of brain signals recorded via, e.g., an electroencephalogram,\nusing machine learning is key to brain-computer interfaces (BCIs). Stimulation\nparameters or other experimental settings of the BCI protocol typically are\nchosen according to the literature. The decoding performance directly depends\non the choice of parameters, as they influence the elicited brain signals and\noptimal parameters are subject-dependent. Thus a fast and automated selection\nprocedure for experimental parameters could greatly improve the usability of\nBCIs.\n  We evaluate a standalone random search and a combined Bayesian optimization\nwith random search in a closed-loop auditory event-related potential protocol.\nWe aimed at finding the individually best stimulation speed -- also known as\nstimulus onset asynchrony (SOA) -- that maximizes the classification\nperformance of a regularized linear discriminant analysis. To make the Bayesian\noptimization feasible under noise and the time pressure posed by an online BCI\nexperiment, we first used offline simulations to initialize and constrain the\ninternal optimization model. Then we evaluated our approach online with 13\nhealthy subjects.\n  We could show that for 8 out of 13 subjects, the proposed approach using\nBayesian optimization succeeded to select the individually optimal SOA out of\nmultiple evaluated SOA values. Our data suggests, however, that subjects were\ninfluenced to very different degrees by the SOA parameter. This makes the\nautomatic parameter selection infeasible for subjects where the influence is\nlimited.\n  Our work proposes an approach to exploit the benefits of individualized\nexperimental protocols and evaluated it in an auditory BCI. When applied to\nother experimental parameters our approach could enhance the usability of BCI\nfor different target groups -- specifically if an individual disease progress\nmay prevent the use of standard parameters."
  },
  {
    "id": "arxiv-100",
    "title": "The leap to ordinal: detailed functional prognosis after traumatic brain\n  injury with a flexible modelling approach",
    "abstract": "When a patient is admitted to the intensive care unit (ICU) after a traumatic\nbrain injury (TBI), an early prognosis is essential for baseline risk\nadjustment and shared decision making. TBI outcomes are commonly categorised by\nthe Glasgow Outcome Scale-Extended (GOSE) into 8, ordered levels of functional\nrecovery at 6 months after injury. Existing ICU prognostic models predict\nbinary outcomes at a certain threshold of GOSE (e.g., prediction of survival\n[GOSE>1] or functional independence [GOSE>4]). We aimed to develop ordinal\nprediction models that concurrently predict probabilities of each GOSE score.\nFrom a prospective cohort (n=1,550, 65 centres) in the ICU stratum of the\nCollaborative European NeuroTrauma Effectiveness Research in TBI (CENTER-TBI)\npatient dataset, we extracted all clinical information within 24 hours of ICU\nadmission (1,151 predictors) and 6-month GOSE scores. We analysed the effect of\n2 design elements on ordinal model performance: (1) the baseline predictor set,\nranging from a concise set of 10 validated predictors to a token-embedded\nrepresentation of all possible predictors, and (2) the modelling strategy, from\nordinal logistic regression to multinomial deep learning. With repeated k-fold\ncross-validation, we found that expanding the baseline predictor set\nsignificantly improved ordinal prediction performance while increasing\nanalytical complexity did not. Half of these gains could be achieved with the\naddition of 8 high-impact predictors (2 demographic variables, 4 protein\nbiomarkers, and 2 severity assessments) to the concise set. At best, ordinal\nmodels achieved 0.76 (95% CI: 0.74-0.77) ordinal discrimination ability\n(ordinal c-index) and 57% (95% CI: 54%-60%) explanation of ordinal variation in\n6-month GOSE (Somers' D). Our results motivate the search for informative\npredictors for higher GOSE and the development of ordinal dynamic prediction\nmodels.",
    "text": "The leap to ordinal: detailed functional prognosis after traumatic brain\n  injury with a flexible modelling approach\n\nWhen a patient is admitted to the intensive care unit (ICU) after a traumatic\nbrain injury (TBI), an early prognosis is essential for baseline risk\nadjustment and shared decision making. TBI outcomes are commonly categorised by\nthe Glasgow Outcome Scale-Extended (GOSE) into 8, ordered levels of functional\nrecovery at 6 months after injury. Existing ICU prognostic models predict\nbinary outcomes at a certain threshold of GOSE (e.g., prediction of survival\n[GOSE>1] or functional independence [GOSE>4]). We aimed to develop ordinal\nprediction models that concurrently predict probabilities of each GOSE score.\nFrom a prospective cohort (n=1,550, 65 centres) in the ICU stratum of the\nCollaborative European NeuroTrauma Effectiveness Research in TBI (CENTER-TBI)\npatient dataset, we extracted all clinical information within 24 hours of ICU\nadmission (1,151 predictors) and 6-month GOSE scores. We analysed the effect of\n2 design elements on ordinal model performance: (1) the baseline predictor set,\nranging from a concise set of 10 validated predictors to a token-embedded\nrepresentation of all possible predictors, and (2) the modelling strategy, from\nordinal logistic regression to multinomial deep learning. With repeated k-fold\ncross-validation, we found that expanding the baseline predictor set\nsignificantly improved ordinal prediction performance while increasing\nanalytical complexity did not. Half of these gains could be achieved with the\naddition of 8 high-impact predictors (2 demographic variables, 4 protein\nbiomarkers, and 2 severity assessments) to the concise set. At best, ordinal\nmodels achieved 0.76 (95% CI: 0.74-0.77) ordinal discrimination ability\n(ordinal c-index) and 57% (95% CI: 54%-60%) explanation of ordinal variation in\n6-month GOSE (Somers' D). Our results motivate the search for informative\npredictors for higher GOSE and the development of ordinal dynamic prediction\nmodels."
  },
  {
    "id": "arxiv-101",
    "title": "Joint Classification and Prediction CNN Framework for Automatic Sleep\n  Stage Classification",
    "abstract": "Correctly identifying sleep stages is important in diagnosing and treating\nsleep disorders. This work proposes a joint classification-and-prediction\nframework based on CNNs for automatic sleep staging, and, subsequently,\nintroduces a simple yet efficient CNN architecture to power the framework.\nGiven a single input epoch, the novel framework jointly determines its label\n(classification) and its neighboring epochs' labels (prediction) in the\ncontextual output. While the proposed framework is orthogonal to the widely\nadopted classification schemes, which take one or multiple epochs as contextual\ninputs and produce a single classification decision on the target epoch, we\ndemonstrate its advantages in several ways. First, it leverages the dependency\namong consecutive sleep epochs while surpassing the problems experienced with\nthe common classification schemes. Second, even with a single model, the\nframework has the capacity to produce multiple decisions, which are essential\nin obtaining a good performance as in ensemble-of-models methods, with very\nlittle induced computational overhead. Probabilistic aggregation techniques are\nthen proposed to leverage the availability of multiple decisions. We conducted\nexperiments on two public datasets: Sleep-EDF Expanded with 20 subjects, and\nMontreal Archive of Sleep Studies dataset with 200 subjects. The proposed\nframework yields an overall classification accuracy of 82.3% and 83.6%,\nrespectively. We also show that the proposed framework not only is superior to\nthe baselines based on the common classification schemes but also outperforms\nexisting deep-learning approaches. To our knowledge, this is the first work\ngoing beyond the standard single-output classification to consider multitask\nneural networks for automatic sleep staging. This framework provides avenues\nfor further studies of different neural-network architectures for automatic\nsleep staging.",
    "text": "Joint Classification and Prediction CNN Framework for Automatic Sleep\n  Stage Classification\n\nCorrectly identifying sleep stages is important in diagnosing and treating\nsleep disorders. This work proposes a joint classification-and-prediction\nframework based on CNNs for automatic sleep staging, and, subsequently,\nintroduces a simple yet efficient CNN architecture to power the framework.\nGiven a single input epoch, the novel framework jointly determines its label\n(classification) and its neighboring epochs' labels (prediction) in the\ncontextual output. While the proposed framework is orthogonal to the widely\nadopted classification schemes, which take one or multiple epochs as contextual\ninputs and produce a single classification decision on the target epoch, we\ndemonstrate its advantages in several ways. First, it leverages the dependency\namong consecutive sleep epochs while surpassing the problems experienced with\nthe common classification schemes. Second, even with a single model, the\nframework has the capacity to produce multiple decisions, which are essential\nin obtaining a good performance as in ensemble-of-models methods, with very\nlittle induced computational overhead. Probabilistic aggregation techniques are\nthen proposed to leverage the availability of multiple decisions. We conducted\nexperiments on two public datasets: Sleep-EDF Expanded with 20 subjects, and\nMontreal Archive of Sleep Studies dataset with 200 subjects. The proposed\nframework yields an overall classification accuracy of 82.3% and 83.6%,\nrespectively. We also show that the proposed framework not only is superior to\nthe baselines based on the common classification schemes but also outperforms\nexisting deep-learning approaches. To our knowledge, this is the first work\ngoing beyond the standard single-output classification to consider multitask\nneural networks for automatic sleep staging. This framework provides avenues\nfor further studies of different neural-network architectures for automatic\nsleep staging."
  },
  {
    "id": "arxiv-102",
    "title": "RL4RS: A Real-World Benchmark for Reinforcement Learning based\n  Recommender System",
    "abstract": "Reinforcement learning based recommender systems (RL-based RS) aim at\nlearning a good policy from a batch of collected data, by casting sequential\nrecommendations to multi-step decision-making tasks. However, current RL-based\nRS benchmarks commonly have a large reality gap, because they involve\nartificial RL datasets or semi-simulated RS datasets, and the trained policy is\ndirectly evaluated in the simulation environment. In real-world situations, not\nall recommendation problems are suitable to be transformed into reinforcement\nlearning problems. Unlike previous academic RL research, RL-based RS suffers\nfrom extrapolation error and the difficulties of being well-validated before\ndeployment. In this paper, we introduce the RL4RS (Reinforcement Learning for\nRecommender Systems) benchmark - a new resource fully collected from industrial\napplications to train and evaluate RL algorithms with special concerns on the\nabove issues. It contains two datasets, tuned simulation environments, related\nadvanced RL baselines, data understanding tools, and counterfactual policy\nevaluation algorithms. The RL4RS suit can be found at\nhttps://github.com/fuxiAIlab/RL4RS. In addition to the RL-based recommender\nsystems, we expect the resource to contribute to research in reinforcement\nlearning and neural combinatorial optimization.",
    "text": "RL4RS: A Real-World Benchmark for Reinforcement Learning based\n  Recommender System\n\nReinforcement learning based recommender systems (RL-based RS) aim at\nlearning a good policy from a batch of collected data, by casting sequential\nrecommendations to multi-step decision-making tasks. However, current RL-based\nRS benchmarks commonly have a large reality gap, because they involve\nartificial RL datasets or semi-simulated RS datasets, and the trained policy is\ndirectly evaluated in the simulation environment. In real-world situations, not\nall recommendation problems are suitable to be transformed into reinforcement\nlearning problems. Unlike previous academic RL research, RL-based RS suffers\nfrom extrapolation error and the difficulties of being well-validated before\ndeployment. In this paper, we introduce the RL4RS (Reinforcement Learning for\nRecommender Systems) benchmark - a new resource fully collected from industrial\napplications to train and evaluate RL algorithms with special concerns on the\nabove issues. It contains two datasets, tuned simulation environments, related\nadvanced RL baselines, data understanding tools, and counterfactual policy\nevaluation algorithms. The RL4RS suit can be found at\nhttps://github.com/fuxiAIlab/RL4RS. In addition to the RL-based recommender\nsystems, we expect the resource to contribute to research in reinforcement\nlearning and neural combinatorial optimization."
  },
  {
    "id": "arxiv-103",
    "title": "Adversarial defenses via a mixture of generators",
    "abstract": "In spite of the enormous success of neural networks, adversarial examples\nremain a relatively weakly understood feature of deep learning systems. There\nis a considerable effort in both building more powerful adversarial attacks and\ndesigning methods to counter the effects of adversarial examples. We propose a\nmethod to transform the adversarial input data through a mixture of generators\nin order to recover the correct class obfuscated by the adversarial attack. A\ncanonical set of images is used to generate adversarial examples through\npotentially multiple attacks. Such transformed images are processed by a set of\ngenerators, which are trained adversarially as a whole to compete in inverting\nthe initial transformations. To our knowledge, this is the first use of a\nmixture-based adversarially trained system as a defense mechanism. We show that\nit is possible to train such a system without supervision, simultaneously on\nmultiple adversarial attacks. Our system is able to recover class information\nfor previously-unseen examples with neither attack nor data labels on the MNIST\ndataset. The results demonstrate that this multi-attack approach is competitive\nwith adversarial defenses tested in single-attack settings.",
    "text": "Adversarial defenses via a mixture of generators\n\nIn spite of the enormous success of neural networks, adversarial examples\nremain a relatively weakly understood feature of deep learning systems. There\nis a considerable effort in both building more powerful adversarial attacks and\ndesigning methods to counter the effects of adversarial examples. We propose a\nmethod to transform the adversarial input data through a mixture of generators\nin order to recover the correct class obfuscated by the adversarial attack. A\ncanonical set of images is used to generate adversarial examples through\npotentially multiple attacks. Such transformed images are processed by a set of\ngenerators, which are trained adversarially as a whole to compete in inverting\nthe initial transformations. To our knowledge, this is the first use of a\nmixture-based adversarially trained system as a defense mechanism. We show that\nit is possible to train such a system without supervision, simultaneously on\nmultiple adversarial attacks. Our system is able to recover class information\nfor previously-unseen examples with neither attack nor data labels on the MNIST\ndataset. The results demonstrate that this multi-attack approach is competitive\nwith adversarial defenses tested in single-attack settings."
  },
  {
    "id": "arxiv-104",
    "title": "An Evaluation of Support Vector Machines as a Pattern Recognition Tool",
    "abstract": "The purpose of this report is in examining the generalization performance of\nSupport Vector Machines (SVM) as a tool for pattern recognition and object\nclassification. The work is motivated by the growing popularity of the method\nthat is claimed to guarantee a good generalization performance for the task in\nhand. The method is implemented in MATLAB. SVMs based on various kernels are\ntested for classifying data from various domains.",
    "text": "An Evaluation of Support Vector Machines as a Pattern Recognition Tool\n\nThe purpose of this report is in examining the generalization performance of\nSupport Vector Machines (SVM) as a tool for pattern recognition and object\nclassification. The work is motivated by the growing popularity of the method\nthat is claimed to guarantee a good generalization performance for the task in\nhand. The method is implemented in MATLAB. SVMs based on various kernels are\ntested for classifying data from various domains."
  },
  {
    "id": "arxiv-105",
    "title": "Feature Selection for Microarray Gene Expression Data using Simulated\n  Annealing guided by the Multivariate Joint Entropy",
    "abstract": "In this work a new way to calculate the multivariate joint entropy is\npresented. This measure is the basis for a fast information-theoretic based\nevaluation of gene relevance in a Microarray Gene Expression data context. Its\nlow complexity is based on the reuse of previous computations to calculate\ncurrent feature relevance. The mu-TAFS algorithm --named as such to\ndifferentiate it from previous TAFS algorithms-- implements a simulated\nannealing technique specially designed for feature subset selection. The\nalgorithm is applied to the maximization of gene subset relevance in several\npublic-domain microarray data sets. The experimental results show a notoriously\nhigh classification performance and low size subsets formed by biologically\nmeaningful genes.",
    "text": "Feature Selection for Microarray Gene Expression Data using Simulated\n  Annealing guided by the Multivariate Joint Entropy\n\nIn this work a new way to calculate the multivariate joint entropy is\npresented. This measure is the basis for a fast information-theoretic based\nevaluation of gene relevance in a Microarray Gene Expression data context. Its\nlow complexity is based on the reuse of previous computations to calculate\ncurrent feature relevance. The mu-TAFS algorithm --named as such to\ndifferentiate it from previous TAFS algorithms-- implements a simulated\nannealing technique specially designed for feature subset selection. The\nalgorithm is applied to the maximization of gene subset relevance in several\npublic-domain microarray data sets. The experimental results show a notoriously\nhigh classification performance and low size subsets formed by biologically\nmeaningful genes."
  },
  {
    "id": "arxiv-106",
    "title": "Toward a general, scaleable framework for Bayesian teaching with\n  applications to topic models",
    "abstract": "Machines, not humans, are the world's dominant knowledge accumulators but\nhumans remain the dominant decision makers. Interpreting and disseminating the\nknowledge accumulated by machines requires expertise, time, and is prone to\nfailure. The problem of how best to convey accumulated knowledge from computers\nto humans is a critical bottleneck in the broader application of machine\nlearning. We propose an approach based on human teaching where the problem is\nformalized as selecting a small subset of the data that will, with high\nprobability, lead the human user to the correct inference. This approach,\nthough successful for modeling human learning in simple laboratory experiments,\nhas failed to achieve broader relevance due to challenges in formulating\ngeneral and scalable algorithms. We propose general-purpose teaching via\npseudo-marginal sampling and demonstrate the algorithm by teaching topic\nmodels. Simulation results show our sampling-based approach: effectively\napproximates the probability where ground-truth is possible via enumeration,\nresults in data that are markedly different from those expected by random\nsampling, and speeds learning especially for small amounts of data. Application\nto movie synopsis data illustrates differences between teaching and random\nsampling for teaching distributions and specific topics, and demonstrates gains\nin scalability and applicability to real-world problems.",
    "text": "Toward a general, scaleable framework for Bayesian teaching with\n  applications to topic models\n\nMachines, not humans, are the world's dominant knowledge accumulators but\nhumans remain the dominant decision makers. Interpreting and disseminating the\nknowledge accumulated by machines requires expertise, time, and is prone to\nfailure. The problem of how best to convey accumulated knowledge from computers\nto humans is a critical bottleneck in the broader application of machine\nlearning. We propose an approach based on human teaching where the problem is\nformalized as selecting a small subset of the data that will, with high\nprobability, lead the human user to the correct inference. This approach,\nthough successful for modeling human learning in simple laboratory experiments,\nhas failed to achieve broader relevance due to challenges in formulating\ngeneral and scalable algorithms. We propose general-purpose teaching via\npseudo-marginal sampling and demonstrate the algorithm by teaching topic\nmodels. Simulation results show our sampling-based approach: effectively\napproximates the probability where ground-truth is possible via enumeration,\nresults in data that are markedly different from those expected by random\nsampling, and speeds learning especially for small amounts of data. Application\nto movie synopsis data illustrates differences between teaching and random\nsampling for teaching distributions and specific topics, and demonstrates gains\nin scalability and applicability to real-world problems."
  },
  {
    "id": "arxiv-107",
    "title": "How to Learn when Data Reacts to Your Model: Performative Gradient\n  Descent",
    "abstract": "Performative distribution shift captures the setting where the choice of\nwhich ML model is deployed changes the data distribution. For example, a bank\nwhich uses the number of open credit lines to determine a customer's risk of\ndefault on a loan may induce customers to open more credit lines in order to\nimprove their chances of being approved. Because of the interactions between\nthe model and data distribution, finding the optimal model parameters is\nchallenging. Works in this area have focused on finding stable points, which\ncan be far from optimal. Here we introduce performative gradient descent\n(PerfGD), which is the first algorithm which provably converges to the\nperformatively optimal point. PerfGD explicitly captures how changes in the\nmodel affects the data distribution and is simple to use. We support our\nfindings with theory and experiments.",
    "text": "How to Learn when Data Reacts to Your Model: Performative Gradient\n  Descent\n\nPerformative distribution shift captures the setting where the choice of\nwhich ML model is deployed changes the data distribution. For example, a bank\nwhich uses the number of open credit lines to determine a customer's risk of\ndefault on a loan may induce customers to open more credit lines in order to\nimprove their chances of being approved. Because of the interactions between\nthe model and data distribution, finding the optimal model parameters is\nchallenging. Works in this area have focused on finding stable points, which\ncan be far from optimal. Here we introduce performative gradient descent\n(PerfGD), which is the first algorithm which provably converges to the\nperformatively optimal point. PerfGD explicitly captures how changes in the\nmodel affects the data distribution and is simple to use. We support our\nfindings with theory and experiments."
  },
  {
    "id": "arxiv-108",
    "title": "Single-Molecule Protein Identification by Sub-Nanopore Sensors",
    "abstract": "Recent advances in top-down mass spectrometry enabled identification of\nintact proteins, but this technology still faces challenges. For example,\ntop-down mass spectrometry suffers from a lack of sensitivity since the ion\ncounts for a single fragmentation event are often low. In contrast, nanopore\ntechnology is exquisitely sensitive to single intact molecules, but it has only\nbeen successfully applied to DNA sequencing, so far. Here, we explore the\npotential of sub-nanopores for single-molecule protein identification (SMPI)\nand describe an algorithm for identification of the electrical current blockade\nsignal (nanospectrum) resulting from the translocation of a denaturated,\nlinearly charged protein through a sub-nanopore. The analysis of identification\np-values suggests that the current technology is already sufficient for\nmatching nanospectra against small protein databases, e.g., protein\nidentification in bacterial proteomes.",
    "text": "Single-Molecule Protein Identification by Sub-Nanopore Sensors\n\nRecent advances in top-down mass spectrometry enabled identification of\nintact proteins, but this technology still faces challenges. For example,\ntop-down mass spectrometry suffers from a lack of sensitivity since the ion\ncounts for a single fragmentation event are often low. In contrast, nanopore\ntechnology is exquisitely sensitive to single intact molecules, but it has only\nbeen successfully applied to DNA sequencing, so far. Here, we explore the\npotential of sub-nanopores for single-molecule protein identification (SMPI)\nand describe an algorithm for identification of the electrical current blockade\nsignal (nanospectrum) resulting from the translocation of a denaturated,\nlinearly charged protein through a sub-nanopore. The analysis of identification\np-values suggests that the current technology is already sufficient for\nmatching nanospectra against small protein databases, e.g., protein\nidentification in bacterial proteomes."
  },
  {
    "id": "arxiv-109",
    "title": "Adaptive Initialization Method for K-means Algorithm",
    "abstract": "The K-means algorithm is a widely used clustering algorithm that offers\nsimplicity and efficiency. However, the traditional K-means algorithm uses the\nrandom method to determine the initial cluster centers, which make clustering\nresults prone to local optima and then result in worse clustering performance.\nMany initialization methods have been proposed, but none of them can\ndynamically adapt to datasets with various characteristics. In our previous\nresearch, an initialization method for K-means based on hybrid distance was\nproposed, and this algorithm can adapt to datasets with different\ncharacteristics. However, it has the following drawbacks: (a) When calculating\ndensity, the threshold cannot be uniquely determined, resulting in unstable\nresults. (b) Heavily depending on adjusting the parameter, the parameter must\nbe adjusted five times to obtain better clustering results. (c) The time\ncomplexity of the algorithm is quadratic, which is difficult to apply to large\ndatasets. In the current paper, we proposed an adaptive initialization method\nfor the K-means algorithm (AIMK) to improve our previous work. AIMK can not\nonly adapt to datasets with various characteristics but also obtain better\nclustering results within two interactions. In addition, we then leverage\nrandom sampling in AIMK, which is named as AIMK-RS, to reduce the time\ncomplexity. AIMK-RS is easily applied to large and high-dimensional datasets.\nWe compared AIMK and AIMK-RS with 10 different algorithms on 16 normal and six\nextra-large datasets. The experimental results show that AIMK and AIMK-RS\noutperform the current initialization methods and several well-known clustering\nalgorithms. Furthermore, AIMK-RS can significantly reduce the complexity of\napplying it to extra-large datasets with high dimensions. The time complexity\nof AIMK-RS is O(n).",
    "text": "Adaptive Initialization Method for K-means Algorithm\n\nThe K-means algorithm is a widely used clustering algorithm that offers\nsimplicity and efficiency. However, the traditional K-means algorithm uses the\nrandom method to determine the initial cluster centers, which make clustering\nresults prone to local optima and then result in worse clustering performance.\nMany initialization methods have been proposed, but none of them can\ndynamically adapt to datasets with various characteristics. In our previous\nresearch, an initialization method for K-means based on hybrid distance was\nproposed, and this algorithm can adapt to datasets with different\ncharacteristics. However, it has the following drawbacks: (a) When calculating\ndensity, the threshold cannot be uniquely determined, resulting in unstable\nresults. (b) Heavily depending on adjusting the parameter, the parameter must\nbe adjusted five times to obtain better clustering results. (c) The time\ncomplexity of the algorithm is quadratic, which is difficult to apply to large\ndatasets. In the current paper, we proposed an adaptive initialization method\nfor the K-means algorithm (AIMK) to improve our previous work. AIMK can not\nonly adapt to datasets with various characteristics but also obtain better\nclustering results within two interactions. In addition, we then leverage\nrandom sampling in AIMK, which is named as AIMK-RS, to reduce the time\ncomplexity. AIMK-RS is easily applied to large and high-dimensional datasets.\nWe compared AIMK and AIMK-RS with 10 different algorithms on 16 normal and six\nextra-large datasets. The experimental results show that AIMK and AIMK-RS\noutperform the current initialization methods and several well-known clustering\nalgorithms. Furthermore, AIMK-RS can significantly reduce the complexity of\napplying it to extra-large datasets with high dimensions. The time complexity\nof AIMK-RS is O(n)."
  },
  {
    "id": "arxiv-110",
    "title": "Hybrid Deep Learning Model using SPCAGAN Augmentation for Insider Threat\n  Analysis",
    "abstract": "Cyberattacks from within an organization's trusted entities are known as\ninsider threats. Anomaly detection using deep learning requires comprehensive\ndata, but insider threat data is not readily available due to confidentiality\nconcerns of organizations. Therefore, there arises demand to generate synthetic\ndata to explore enhanced approaches for threat analysis. We propose a linear\nmanifold learning-based generative adversarial network, SPCAGAN, that takes\ninput from heterogeneous data sources and adds a novel loss function to train\nthe generator to produce high-quality data that closely resembles the original\ndata distribution. Furthermore, we introduce a deep learning-based hybrid model\nfor insider threat analysis. We provide extensive experiments for data\nsynthesis, anomaly detection, adversarial robustness, and synthetic data\nquality analysis using benchmark datasets. In this context, empirical\ncomparisons show that GAN-based oversampling is competitive with numerous\ntypical oversampling regimes. For synthetic data generation, our SPCAGAN model\novercame the problem of mode collapse and converged faster than previous GAN\nmodels. Results demonstrate that our proposed approach has a lower error, is\nmore accurate, and generates substantially superior synthetic insider threat\ndata than previous models.",
    "text": "Hybrid Deep Learning Model using SPCAGAN Augmentation for Insider Threat\n  Analysis\n\nCyberattacks from within an organization's trusted entities are known as\ninsider threats. Anomaly detection using deep learning requires comprehensive\ndata, but insider threat data is not readily available due to confidentiality\nconcerns of organizations. Therefore, there arises demand to generate synthetic\ndata to explore enhanced approaches for threat analysis. We propose a linear\nmanifold learning-based generative adversarial network, SPCAGAN, that takes\ninput from heterogeneous data sources and adds a novel loss function to train\nthe generator to produce high-quality data that closely resembles the original\ndata distribution. Furthermore, we introduce a deep learning-based hybrid model\nfor insider threat analysis. We provide extensive experiments for data\nsynthesis, anomaly detection, adversarial robustness, and synthetic data\nquality analysis using benchmark datasets. In this context, empirical\ncomparisons show that GAN-based oversampling is competitive with numerous\ntypical oversampling regimes. For synthetic data generation, our SPCAGAN model\novercame the problem of mode collapse and converged faster than previous GAN\nmodels. Results demonstrate that our proposed approach has a lower error, is\nmore accurate, and generates substantially superior synthetic insider threat\ndata than previous models."
  },
  {
    "id": "arxiv-111",
    "title": "Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines",
    "abstract": "This paper introduces the Metric-Free Natural Gradient (MFNG) algorithm for\ntraining Boltzmann Machines. Similar in spirit to the Hessian-Free method of\nMartens [8], our algorithm belongs to the family of truncated Newton methods\nand exploits an efficient matrix-vector product to avoid explicitely storing\nthe natural gradient metric $L$. This metric is shown to be the expected second\nderivative of the log-partition function (under the model distribution), or\nequivalently, the variance of the vector of partial derivatives of the energy\nfunction. We evaluate our method on the task of joint-training a 3-layer Deep\nBoltzmann Machine and show that MFNG does indeed have faster per-epoch\nconvergence compared to Stochastic Maximum Likelihood with centering, though\nwall-clock performance is currently not competitive.",
    "text": "Metric-Free Natural Gradient for Joint-Training of Boltzmann Machines\n\nThis paper introduces the Metric-Free Natural Gradient (MFNG) algorithm for\ntraining Boltzmann Machines. Similar in spirit to the Hessian-Free method of\nMartens [8], our algorithm belongs to the family of truncated Newton methods\nand exploits an efficient matrix-vector product to avoid explicitely storing\nthe natural gradient metric $L$. This metric is shown to be the expected second\nderivative of the log-partition function (under the model distribution), or\nequivalently, the variance of the vector of partial derivatives of the energy\nfunction. We evaluate our method on the task of joint-training a 3-layer Deep\nBoltzmann Machine and show that MFNG does indeed have faster per-epoch\nconvergence compared to Stochastic Maximum Likelihood with centering, though\nwall-clock performance is currently not competitive."
  },
  {
    "id": "arxiv-112",
    "title": "Learning excursion sets of vector-valued Gaussian random fields for\n  autonomous ocean sampling",
    "abstract": "Improving and optimizing oceanographic sampling is a crucial task for marine\nscience and maritime resource management. Faced with limited resources in\nunderstanding processes in the water-column, the combination of statistics and\nautonomous systems provide new opportunities for experimental design. In this\nwork we develop efficient spatial sampling methods for characterizing regions\ndefined by simultaneous exceedances above prescribed thresholds of several\nresponses, with an application focus on mapping coastal ocean phenomena based\non temperature and salinity measurements. Specifically, we define a design\ncriterion based on uncertainty in the excursions of vector-valued Gaussian\nrandom fields, and derive tractable expressions for the expected integrated\nBernoulli variance reduction in such a framework. We demonstrate how this\ncriterion can be used to prioritize sampling efforts at locations that are\nambiguous, making exploration more effective. We use simulations to study and\ncompare properties of the considered approaches, followed by results from field\ndeployments with an autonomous underwater vehicle as part of a study mapping\nthe boundary of a river plume. The results demonstrate the potential of\ncombining statistical methods and robotic platforms to effectively inform and\nexecute data-driven environmental sampling.",
    "text": "Learning excursion sets of vector-valued Gaussian random fields for\n  autonomous ocean sampling\n\nImproving and optimizing oceanographic sampling is a crucial task for marine\nscience and maritime resource management. Faced with limited resources in\nunderstanding processes in the water-column, the combination of statistics and\nautonomous systems provide new opportunities for experimental design. In this\nwork we develop efficient spatial sampling methods for characterizing regions\ndefined by simultaneous exceedances above prescribed thresholds of several\nresponses, with an application focus on mapping coastal ocean phenomena based\non temperature and salinity measurements. Specifically, we define a design\ncriterion based on uncertainty in the excursions of vector-valued Gaussian\nrandom fields, and derive tractable expressions for the expected integrated\nBernoulli variance reduction in such a framework. We demonstrate how this\ncriterion can be used to prioritize sampling efforts at locations that are\nambiguous, making exploration more effective. We use simulations to study and\ncompare properties of the considered approaches, followed by results from field\ndeployments with an autonomous underwater vehicle as part of a study mapping\nthe boundary of a river plume. The results demonstrate the potential of\ncombining statistical methods and robotic platforms to effectively inform and\nexecute data-driven environmental sampling."
  },
  {
    "id": "arxiv-113",
    "title": "Human Pose Estimation using Deep Consensus Voting",
    "abstract": "In this paper we consider the problem of human pose estimation from a single\nstill image. We propose a novel approach where each location in the image votes\nfor the position of each keypoint using a convolutional neural net. The voting\nscheme allows us to utilize information from the whole image, rather than rely\non a sparse set of keypoint locations. Using dense, multi-target votes, not\nonly produces good keypoint predictions, but also enables us to compute\nimage-dependent joint keypoint probabilities by looking at consensus voting.\nThis differs from most previous methods where joint probabilities are learned\nfrom relative keypoint locations and are independent of the image. We finally\ncombine the keypoints votes and joint probabilities in order to identify the\noptimal pose configuration. We show our competitive performance on the MPII\nHuman Pose and Leeds Sports Pose datasets.",
    "text": "Human Pose Estimation using Deep Consensus Voting\n\nIn this paper we consider the problem of human pose estimation from a single\nstill image. We propose a novel approach where each location in the image votes\nfor the position of each keypoint using a convolutional neural net. The voting\nscheme allows us to utilize information from the whole image, rather than rely\non a sparse set of keypoint locations. Using dense, multi-target votes, not\nonly produces good keypoint predictions, but also enables us to compute\nimage-dependent joint keypoint probabilities by looking at consensus voting.\nThis differs from most previous methods where joint probabilities are learned\nfrom relative keypoint locations and are independent of the image. We finally\ncombine the keypoints votes and joint probabilities in order to identify the\noptimal pose configuration. We show our competitive performance on the MPII\nHuman Pose and Leeds Sports Pose datasets."
  },
  {
    "id": "arxiv-114",
    "title": "Conifer Seedling Detection in UAV-Imagery with RGB-Depth Information",
    "abstract": "Monitoring of reforestation is currently being considerably streamlined\nthrough the use of drones and image recognition algorithms, which have already\nproven to be effective on colour imagery. In addition to colour imagery,\nelevation data is often also available. The primary aim of this work was to\nimprove the performance of the faster-RCNN object detection algorithm by\nintegrating this height information, which showed itself to notably improve\nperformance. Interestingly, the structure of the network played a key role,\nwith direct addition of the height information as a fourth image channel\nshowing no improvement, while integration after the backbone network and before\nthe region proposal network led to marked improvements. This effect persisted\nwith very long training regimes. Increasing the resolution of this height\ninformation also showed little effect.",
    "text": "Conifer Seedling Detection in UAV-Imagery with RGB-Depth Information\n\nMonitoring of reforestation is currently being considerably streamlined\nthrough the use of drones and image recognition algorithms, which have already\nproven to be effective on colour imagery. In addition to colour imagery,\nelevation data is often also available. The primary aim of this work was to\nimprove the performance of the faster-RCNN object detection algorithm by\nintegrating this height information, which showed itself to notably improve\nperformance. Interestingly, the structure of the network played a key role,\nwith direct addition of the height information as a fourth image channel\nshowing no improvement, while integration after the backbone network and before\nthe region proposal network led to marked improvements. This effect persisted\nwith very long training regimes. Increasing the resolution of this height\ninformation also showed little effect."
  },
  {
    "id": "arxiv-115",
    "title": "Finding the Stochastic Shortest Path with Low Regret: The Adversarial\n  Cost and Unknown Transition Case",
    "abstract": "We make significant progress toward the stochastic shortest path problem with\nadversarial costs and unknown transition. Specifically, we develop algorithms\nthat achieve $\\widetilde{O}(\\sqrt{S^2ADT_\\star K})$ regret for the\nfull-information setting and $\\widetilde{O}(\\sqrt{S^3A^2DT_\\star K})$ regret\nfor the bandit feedback setting, where $D$ is the diameter, $T_\\star$ is the\nexpected hitting time of the optimal policy, $S$ is the number of states, $A$\nis the number of actions, and $K$ is the number of episodes. Our work strictly\nimproves (Rosenberg and Mansour, 2020) in the full information setting, extends\n(Chen et al., 2020) from known transition to unknown transition, and is also\nthe first to consider the most challenging combination: bandit feedback with\nadversarial costs and unknown transition. To remedy the gap between our upper\nbounds and the current best lower bounds constructed via a stochastically\noblivious adversary, we also propose algorithms with near-optimal regret for\nthis special case.",
    "text": "Finding the Stochastic Shortest Path with Low Regret: The Adversarial\n  Cost and Unknown Transition Case\n\nWe make significant progress toward the stochastic shortest path problem with\nadversarial costs and unknown transition. Specifically, we develop algorithms\nthat achieve $\\widetilde{O}(\\sqrt{S^2ADT_\\star K})$ regret for the\nfull-information setting and $\\widetilde{O}(\\sqrt{S^3A^2DT_\\star K})$ regret\nfor the bandit feedback setting, where $D$ is the diameter, $T_\\star$ is the\nexpected hitting time of the optimal policy, $S$ is the number of states, $A$\nis the number of actions, and $K$ is the number of episodes. Our work strictly\nimproves (Rosenberg and Mansour, 2020) in the full information setting, extends\n(Chen et al., 2020) from known transition to unknown transition, and is also\nthe first to consider the most challenging combination: bandit feedback with\nadversarial costs and unknown transition. To remedy the gap between our upper\nbounds and the current best lower bounds constructed via a stochastically\noblivious adversary, we also propose algorithms with near-optimal regret for\nthis special case."
  },
  {
    "id": "arxiv-116",
    "title": "Posture Recognition in the Critical Care Settings using Wearable Devices",
    "abstract": "Low physical activity levels in the intensive care units (ICU) patients have\nbeen linked to adverse clinical outcomes. Therefore, there is a need for\ncontinuous and objective measurement of physical activity in the ICU to\nquantify the association between physical activity and patient outcomes. This\nmeasurement would also help clinicians evaluate the efficacy of proposed\nrehabilitation and physical therapy regimens in improving physical activity. In\nthis study, we examined the feasibility of posture recognition in an ICU\npopulation using data from wearable sensors.",
    "text": "Posture Recognition in the Critical Care Settings using Wearable Devices\n\nLow physical activity levels in the intensive care units (ICU) patients have\nbeen linked to adverse clinical outcomes. Therefore, there is a need for\ncontinuous and objective measurement of physical activity in the ICU to\nquantify the association between physical activity and patient outcomes. This\nmeasurement would also help clinicians evaluate the efficacy of proposed\nrehabilitation and physical therapy regimens in improving physical activity. In\nthis study, we examined the feasibility of posture recognition in an ICU\npopulation using data from wearable sensors."
  },
  {
    "id": "arxiv-117",
    "title": "Description and Discussion on DCASE 2022 Challenge Task 2: Unsupervised Anomalous Sound Detection for Machine Condition Monitoring Applying Domain Generalization Techniques",
    "abstract": "We present the task description of the Detection and Classification of\nAcoustic Scenes and Events (DCASE) 2022 Challenge Task 2: \"Unsupervised\nanomalous sound detection (ASD) for machine condition monitoring applying\ndomain generalization techniques\". Domain shifts are a critical problem for the\napplication of ASD systems. Because domain shifts can change the acoustic\ncharacteristics of data, a model trained in a source domain performs poorly for\na target domain. In DCASE 2021 Challenge Task 2, we organized an ASD task for\nhandling domain shifts. In this task, it was assumed that the occurrences of\ndomain shifts are known. However, in practice, the domain of each sample may\nnot be given, and the domain shifts can occur implicitly. In 2022 Task 2, we\nfocus on domain generalization techniques that detects anomalies regardless of\nthe domain shifts. Specifically, the domain of each sample is not given in the\ntest data and only one threshold is allowed for all domains. We will add\nchallenge results and analysis of the submissions after the challenge\nsubmission deadline.",
    "text": "Description and Discussion on DCASE 2022 Challenge Task 2: Unsupervised Anomalous Sound Detection for Machine Condition Monitoring Applying Domain Generalization Techniques\n\nWe present the task description of the Detection and Classification of\nAcoustic Scenes and Events (DCASE) 2022 Challenge Task 2: \"Unsupervised\nanomalous sound detection (ASD) for machine condition monitoring applying\ndomain generalization techniques\". Domain shifts are a critical problem for the\napplication of ASD systems. Because domain shifts can change the acoustic\ncharacteristics of data, a model trained in a source domain performs poorly for\na target domain. In DCASE 2021 Challenge Task 2, we organized an ASD task for\nhandling domain shifts. In this task, it was assumed that the occurrences of\ndomain shifts are known. However, in practice, the domain of each sample may\nnot be given, and the domain shifts can occur implicitly. In 2022 Task 2, we\nfocus on domain generalization techniques that detects anomalies regardless of\nthe domain shifts. Specifically, the domain of each sample is not given in the\ntest data and only one threshold is allowed for all domains. We will add\nchallenge results and analysis of the submissions after the challenge\nsubmission deadline."
  },
  {
    "id": "arxiv-118",
    "title": "Attention-Augmented End-to-End Multi-Task Learning for Emotion\n  Prediction from Speech",
    "abstract": "Despite the increasing research interest in end-to-end learning systems for\nspeech emotion recognition, conventional systems either suffer from the\noverfitting due in part to the limited training data, or do not explicitly\nconsider the different contributions of automatically learnt representations\nfor a specific task. In this contribution, we propose a novel end-to-end\nframework which is enhanced by learning other auxiliary tasks and an attention\nmechanism. That is, we jointly train an end-to-end network with several\ndifferent but related emotion prediction tasks, i.e., arousal, valence, and\ndominance predictions, to extract more robust representations shared among\nvarious tasks than traditional systems with the hope that it is able to relieve\nthe overfitting problem. Meanwhile, an attention layer is implemented on top of\nthe layers for each task, with the aim to capture the contribution distribution\nof different segment parts for each individual task. To evaluate the\neffectiveness of the proposed system, we conducted a set of experiments on the\nwidely used database IEMOCAP. The empirical results show that the proposed\nsystems significantly outperform corresponding baseline systems.",
    "text": "Attention-Augmented End-to-End Multi-Task Learning for Emotion\n  Prediction from Speech\n\nDespite the increasing research interest in end-to-end learning systems for\nspeech emotion recognition, conventional systems either suffer from the\noverfitting due in part to the limited training data, or do not explicitly\nconsider the different contributions of automatically learnt representations\nfor a specific task. In this contribution, we propose a novel end-to-end\nframework which is enhanced by learning other auxiliary tasks and an attention\nmechanism. That is, we jointly train an end-to-end network with several\ndifferent but related emotion prediction tasks, i.e., arousal, valence, and\ndominance predictions, to extract more robust representations shared among\nvarious tasks than traditional systems with the hope that it is able to relieve\nthe overfitting problem. Meanwhile, an attention layer is implemented on top of\nthe layers for each task, with the aim to capture the contribution distribution\nof different segment parts for each individual task. To evaluate the\neffectiveness of the proposed system, we conducted a set of experiments on the\nwidely used database IEMOCAP. The empirical results show that the proposed\nsystems significantly outperform corresponding baseline systems."
  },
  {
    "id": "arxiv-119",
    "title": "ManiSkill: Generalizable Manipulation Skill Benchmark with Large-Scale\n  Demonstrations",
    "abstract": "Object manipulation from 3D visual inputs poses many challenges on building\ngeneralizable perception and policy models. However, 3D assets in existing\nbenchmarks mostly lack the diversity of 3D shapes that align with real-world\nintra-class complexity in topology and geometry. Here we propose SAPIEN\nManipulation Skill Benchmark (ManiSkill) to benchmark manipulation skills over\ndiverse objects in a full-physics simulator. 3D assets in ManiSkill include\nlarge intra-class topological and geometric variations. Tasks are carefully\nchosen to cover distinct types of manipulation challenges. Latest progress in\n3D vision also makes us believe that we should customize the benchmark so that\nthe challenge is inviting to researchers working on 3D deep learning. To this\nend, we simulate a moving panoramic camera that returns ego-centric point\nclouds or RGB-D images. In addition, we would like ManiSkill to serve a broad\nset of researchers interested in manipulation research. Besides supporting the\nlearning of policies from interactions, we also support\nlearning-from-demonstrations (LfD) methods, by providing a large number of\nhigh-quality demonstrations (~36,000 successful trajectories, ~1.5M point\ncloud/RGB-D frames in total). We provide baselines using 3D deep learning and\nLfD algorithms. All code of our benchmark (simulator, environment, SDK, and\nbaselines) is open-sourced, and a challenge facing interdisciplinary\nresearchers will be held based on the benchmark.",
    "text": "ManiSkill: Generalizable Manipulation Skill Benchmark with Large-Scale\n  Demonstrations\n\nObject manipulation from 3D visual inputs poses many challenges on building\ngeneralizable perception and policy models. However, 3D assets in existing\nbenchmarks mostly lack the diversity of 3D shapes that align with real-world\nintra-class complexity in topology and geometry. Here we propose SAPIEN\nManipulation Skill Benchmark (ManiSkill) to benchmark manipulation skills over\ndiverse objects in a full-physics simulator. 3D assets in ManiSkill include\nlarge intra-class topological and geometric variations. Tasks are carefully\nchosen to cover distinct types of manipulation challenges. Latest progress in\n3D vision also makes us believe that we should customize the benchmark so that\nthe challenge is inviting to researchers working on 3D deep learning. To this\nend, we simulate a moving panoramic camera that returns ego-centric point\nclouds or RGB-D images. In addition, we would like ManiSkill to serve a broad\nset of researchers interested in manipulation research. Besides supporting the\nlearning of policies from interactions, we also support\nlearning-from-demonstrations (LfD) methods, by providing a large number of\nhigh-quality demonstrations (~36,000 successful trajectories, ~1.5M point\ncloud/RGB-D frames in total). We provide baselines using 3D deep learning and\nLfD algorithms. All code of our benchmark (simulator, environment, SDK, and\nbaselines) is open-sourced, and a challenge facing interdisciplinary\nresearchers will be held based on the benchmark."
  },
  {
    "id": "arxiv-120",
    "title": "Invariant Causal Prediction for Block MDPs",
    "abstract": "Generalization across environments is critical to the successful application\nof reinforcement learning algorithms to real-world challenges. In this paper,\nwe consider the problem of learning abstractions that generalize in block MDPs,\nfamilies of environments with a shared latent state space and dynamics\nstructure over that latent space, but varying observations. We leverage tools\nfrom causal inference to propose a method of invariant prediction to learn\nmodel-irrelevance state abstractions (MISA) that generalize to novel\nobservations in the multi-environment setting. We prove that for certain\nclasses of environments, this approach outputs with high probability a state\nabstraction corresponding to the causal feature set with respect to the return.\nWe further provide more general bounds on model error and generalization error\nin the multi-environment setting, in the process showing a connection between\ncausal variable selection and the state abstraction framework for MDPs. We give\nempirical evidence that our methods work in both linear and nonlinear settings,\nattaining improved generalization over single- and multi-task baselines.",
    "text": "Invariant Causal Prediction for Block MDPs\n\nGeneralization across environments is critical to the successful application\nof reinforcement learning algorithms to real-world challenges. In this paper,\nwe consider the problem of learning abstractions that generalize in block MDPs,\nfamilies of environments with a shared latent state space and dynamics\nstructure over that latent space, but varying observations. We leverage tools\nfrom causal inference to propose a method of invariant prediction to learn\nmodel-irrelevance state abstractions (MISA) that generalize to novel\nobservations in the multi-environment setting. We prove that for certain\nclasses of environments, this approach outputs with high probability a state\nabstraction corresponding to the causal feature set with respect to the return.\nWe further provide more general bounds on model error and generalization error\nin the multi-environment setting, in the process showing a connection between\ncausal variable selection and the state abstraction framework for MDPs. We give\nempirical evidence that our methods work in both linear and nonlinear settings,\nattaining improved generalization over single- and multi-task baselines."
  },
  {
    "id": "arxiv-121",
    "title": "Fairness in Recommendation Ranking through Pairwise Comparisons",
    "abstract": "Recommender systems are one of the most pervasive applications of machine\nlearning in industry, with many services using them to match users to products\nor information. As such it is important to ask: what are the possible fairness\nrisks, how can we quantify them, and how should we address them? In this paper\nwe offer a set of novel metrics for evaluating algorithmic fairness concerns in\nrecommender systems. In particular we show how measuring fairness based on\npairwise comparisons from randomized experiments provides a tractable means to\nreason about fairness in rankings from recommender systems. Building on this\nmetric, we offer a new regularizer to encourage improving this metric during\nmodel training and thus improve fairness in the resulting rankings. We apply\nthis pairwise regularization to a large-scale, production recommender system\nand show that we are able to significantly improve the system's pairwise\nfairness.",
    "text": "Fairness in Recommendation Ranking through Pairwise Comparisons\n\nRecommender systems are one of the most pervasive applications of machine\nlearning in industry, with many services using them to match users to products\nor information. As such it is important to ask: what are the possible fairness\nrisks, how can we quantify them, and how should we address them? In this paper\nwe offer a set of novel metrics for evaluating algorithmic fairness concerns in\nrecommender systems. In particular we show how measuring fairness based on\npairwise comparisons from randomized experiments provides a tractable means to\nreason about fairness in rankings from recommender systems. Building on this\nmetric, we offer a new regularizer to encourage improving this metric during\nmodel training and thus improve fairness in the resulting rankings. We apply\nthis pairwise regularization to a large-scale, production recommender system\nand show that we are able to significantly improve the system's pairwise\nfairness."
  },
  {
    "id": "arxiv-122",
    "title": "Efficient Memory Management for GPU-based Deep Learning Systems",
    "abstract": "GPU (graphics processing unit) has been used for many data-intensive\napplications. Among them, deep learning systems are one of the most important\nconsumer systems for GPU nowadays. As deep learning applications impose deeper\nand larger models in order to achieve higher accuracy, memory management\nbecomes an important research topic for deep learning systems, given that GPU\nhas limited memory size. Many approaches have been proposed towards this issue,\ne.g., model compression and memory swapping. However, they either degrade the\nmodel accuracy or require a lot of manual intervention. In this paper, we\npropose two orthogonal approaches to reduce the memory cost from the system\nperspective. Our approaches are transparent to the models, and thus do not\naffect the model accuracy. They are achieved by exploiting the iterative nature\nof the training algorithm of deep learning to derive the lifetime and\nread/write order of all variables. With the lifetime semantics, we are able to\nimplement a memory pool with minimal fragments. However, the optimization\nproblem is NP-complete. We propose a heuristic algorithm that reduces up to\n13.3% of memory compared with Nvidia's default memory pool with equal time\ncomplexity. With the read/write semantics, the variables that are not in use\ncan be swapped out from GPU to CPU to reduce the memory footprint. We propose\nmultiple swapping strategies to automatically decide which variable to swap and\nwhen to swap out (in), which reduces the memory cost by up to 34.2% without\ncommunication overhead.",
    "text": "Efficient Memory Management for GPU-based Deep Learning Systems\n\nGPU (graphics processing unit) has been used for many data-intensive\napplications. Among them, deep learning systems are one of the most important\nconsumer systems for GPU nowadays. As deep learning applications impose deeper\nand larger models in order to achieve higher accuracy, memory management\nbecomes an important research topic for deep learning systems, given that GPU\nhas limited memory size. Many approaches have been proposed towards this issue,\ne.g., model compression and memory swapping. However, they either degrade the\nmodel accuracy or require a lot of manual intervention. In this paper, we\npropose two orthogonal approaches to reduce the memory cost from the system\nperspective. Our approaches are transparent to the models, and thus do not\naffect the model accuracy. They are achieved by exploiting the iterative nature\nof the training algorithm of deep learning to derive the lifetime and\nread/write order of all variables. With the lifetime semantics, we are able to\nimplement a memory pool with minimal fragments. However, the optimization\nproblem is NP-complete. We propose a heuristic algorithm that reduces up to\n13.3% of memory compared with Nvidia's default memory pool with equal time\ncomplexity. With the read/write semantics, the variables that are not in use\ncan be swapped out from GPU to CPU to reduce the memory footprint. We propose\nmultiple swapping strategies to automatically decide which variable to swap and\nwhen to swap out (in), which reduces the memory cost by up to 34.2% without\ncommunication overhead."
  },
  {
    "id": "arxiv-123",
    "title": "Learning Rich Nearest Neighbor Representations from Self-supervised\n  Ensembles",
    "abstract": "Pretraining convolutional neural networks via self-supervision, and applying\nthem in transfer learning, is an incredibly fast-growing field that is rapidly\nand iteratively improving performance across practically all image domains.\nMeanwhile, model ensembling is one of the most universally applicable\ntechniques in supervised learning literature and practice, offering a simple\nsolution to reliably improve performance. But how to optimally combine\nself-supervised models to maximize representation quality has largely remained\nunaddressed. In this work, we provide a framework to perform self-supervised\nmodel ensembling via a novel method of learning representations directly\nthrough gradient descent at inference time. This technique improves\nrepresentation quality, as measured by k-nearest neighbors, both on the\nin-domain dataset and in the transfer setting, with models transferable from\nthe former setting to the latter. Additionally, this direct learning of feature\nthrough backpropagation improves representations from even a single model,\nechoing the improvements found in self-distillation.",
    "text": "Learning Rich Nearest Neighbor Representations from Self-supervised\n  Ensembles\n\nPretraining convolutional neural networks via self-supervision, and applying\nthem in transfer learning, is an incredibly fast-growing field that is rapidly\nand iteratively improving performance across practically all image domains.\nMeanwhile, model ensembling is one of the most universally applicable\ntechniques in supervised learning literature and practice, offering a simple\nsolution to reliably improve performance. But how to optimally combine\nself-supervised models to maximize representation quality has largely remained\nunaddressed. In this work, we provide a framework to perform self-supervised\nmodel ensembling via a novel method of learning representations directly\nthrough gradient descent at inference time. This technique improves\nrepresentation quality, as measured by k-nearest neighbors, both on the\nin-domain dataset and in the transfer setting, with models transferable from\nthe former setting to the latter. Additionally, this direct learning of feature\nthrough backpropagation improves representations from even a single model,\nechoing the improvements found in self-distillation."
  },
  {
    "id": "arxiv-124",
    "title": "Robust Semi-Supervised Classification for Multi-Relational Graphs",
    "abstract": "Graph-regularized semi-supervised learning has been used effectively for\nclassification when (i) instances are connected through a graph, and (ii)\nlabeled data is scarce. If available, using multiple relations (or graphs)\nbetween the instances can improve the prediction performance. On the other\nhand, when these relations have varying levels of veracity and exhibit varying\nrelevance for the task, very noisy and/or irrelevant relations may deteriorate\nthe performance. As a result, an effective weighing scheme needs to be put in\nplace. In this work, we propose a robust and scalable approach for\nmulti-relational graph-regularized semi-supervised classification. Under a\nconvex optimization scheme, we simultaneously infer weights for the multiple\ngraphs as well as a solution. We provide a careful analysis of the inferred\nweights, based on which we devise an algorithm that filters out irrelevant and\nnoisy graphs and produces weights proportional to the informativeness of the\nremaining graphs. Moreover, the proposed method is linearly scalable w.r.t. the\nnumber of edges in the union of the multiple graphs. Through extensive\nexperiments we show that our method yields superior results under different\nnoise models, and under increasing number of noisy graphs and intensity of\nnoise, as compared to a list of baselines and state-of-the-art approaches.",
    "text": "Robust Semi-Supervised Classification for Multi-Relational Graphs\n\nGraph-regularized semi-supervised learning has been used effectively for\nclassification when (i) instances are connected through a graph, and (ii)\nlabeled data is scarce. If available, using multiple relations (or graphs)\nbetween the instances can improve the prediction performance. On the other\nhand, when these relations have varying levels of veracity and exhibit varying\nrelevance for the task, very noisy and/or irrelevant relations may deteriorate\nthe performance. As a result, an effective weighing scheme needs to be put in\nplace. In this work, we propose a robust and scalable approach for\nmulti-relational graph-regularized semi-supervised classification. Under a\nconvex optimization scheme, we simultaneously infer weights for the multiple\ngraphs as well as a solution. We provide a careful analysis of the inferred\nweights, based on which we devise an algorithm that filters out irrelevant and\nnoisy graphs and produces weights proportional to the informativeness of the\nremaining graphs. Moreover, the proposed method is linearly scalable w.r.t. the\nnumber of edges in the union of the multiple graphs. Through extensive\nexperiments we show that our method yields superior results under different\nnoise models, and under increasing number of noisy graphs and intensity of\nnoise, as compared to a list of baselines and state-of-the-art approaches."
  },
  {
    "id": "arxiv-125",
    "title": "A Graph-Based Platform for Customer Behavior Analysis using\n  Applications' Clickstream Data",
    "abstract": "Clickstream analysis is getting more attention since the increase of usage in\ne-commerce and applications. Beside customers' purchase behavior analysis,\nthere is also attempt to analyze the customer behavior in relation to the\nquality of web or application design. In general, clickstream data can be\nconsidered as a sequence of log events collected at different levels of web/app\nusage. The analysis of clickstream data can be performed directly as sequence\nanalysis or by extracting features from sequences. In this work, we show how\nrepresenting and saving the sequences with their underlying graph structures\ncan induce a platform for customer behavior analysis. Our main idea is that\nclickstream data containing sequences of actions of an application, are walks\nof the corresponding finite state automaton (FSA) of that application. Our\nhypothesis is that the customers of an application normally do not use all\npossible walks through that FSA and the number of actual walks is much smaller\nthan total number of possible walks through the FSA. Sequences of such a walk\nnormally consist of a finite number of cycles on FSA graphs. Identifying and\nmatching these cycles in the classical sequence analysis is not straight\nforward. We show that representing the sequences through their underlying graph\nstructures not only groups the sequences automatically but also provides a\ncompressed data representation of the original sequences.",
    "text": "A Graph-Based Platform for Customer Behavior Analysis using\n  Applications' Clickstream Data\n\nClickstream analysis is getting more attention since the increase of usage in\ne-commerce and applications. Beside customers' purchase behavior analysis,\nthere is also attempt to analyze the customer behavior in relation to the\nquality of web or application design. In general, clickstream data can be\nconsidered as a sequence of log events collected at different levels of web/app\nusage. The analysis of clickstream data can be performed directly as sequence\nanalysis or by extracting features from sequences. In this work, we show how\nrepresenting and saving the sequences with their underlying graph structures\ncan induce a platform for customer behavior analysis. Our main idea is that\nclickstream data containing sequences of actions of an application, are walks\nof the corresponding finite state automaton (FSA) of that application. Our\nhypothesis is that the customers of an application normally do not use all\npossible walks through that FSA and the number of actual walks is much smaller\nthan total number of possible walks through the FSA. Sequences of such a walk\nnormally consist of a finite number of cycles on FSA graphs. Identifying and\nmatching these cycles in the classical sequence analysis is not straight\nforward. We show that representing the sequences through their underlying graph\nstructures not only groups the sequences automatically but also provides a\ncompressed data representation of the original sequences."
  },
  {
    "id": "arxiv-126",
    "title": "OMPQ: Orthogonal Mixed Precision Quantization",
    "abstract": "To bridge the ever increasing gap between deep neural networks' complexity\nand hardware capability, network quantization has attracted more and more\nresearch attention. The latest trend of mixed precision quantization takes\nadvantage of hardware's multiple bit-width arithmetic operations to unleash the\nfull potential of network quantization. However, this also results in a\ndifficult integer programming formulation, and forces most existing approaches\nto use an extremely time-consuming search process even with various\nrelaxations. Instead of solving a problem of the original integer programming,\nwe propose to optimize a proxy metric, the concept of network orthogonality,\nwhich is highly correlated with the loss of the integer programming but also\neasy to optimize with linear programming. This approach reduces the search time\nand required data amount by orders of magnitude, with little compromise on\nquantization accuracy. Specifically, we achieve 72.08% Top-1 accuracy on\nResNet-18 with 6.7Mb, which does not require any searching iterations. Given\nthe high efficiency and low data dependency of our algorithm, we used it for\nthe post-training quantization, which achieve 71.27% Top-1 accuracy on\nMobileNetV2 with only 1.5Mb. Our code is available at\nhttps://github.com/MAC-AutoML/OMPQ.",
    "text": "OMPQ: Orthogonal Mixed Precision Quantization\n\nTo bridge the ever increasing gap between deep neural networks' complexity\nand hardware capability, network quantization has attracted more and more\nresearch attention. The latest trend of mixed precision quantization takes\nadvantage of hardware's multiple bit-width arithmetic operations to unleash the\nfull potential of network quantization. However, this also results in a\ndifficult integer programming formulation, and forces most existing approaches\nto use an extremely time-consuming search process even with various\nrelaxations. Instead of solving a problem of the original integer programming,\nwe propose to optimize a proxy metric, the concept of network orthogonality,\nwhich is highly correlated with the loss of the integer programming but also\neasy to optimize with linear programming. This approach reduces the search time\nand required data amount by orders of magnitude, with little compromise on\nquantization accuracy. Specifically, we achieve 72.08% Top-1 accuracy on\nResNet-18 with 6.7Mb, which does not require any searching iterations. Given\nthe high efficiency and low data dependency of our algorithm, we used it for\nthe post-training quantization, which achieve 71.27% Top-1 accuracy on\nMobileNetV2 with only 1.5Mb. Our code is available at\nhttps://github.com/MAC-AutoML/OMPQ."
  },
  {
    "id": "arxiv-127",
    "title": "Pattern Decomposition with Complex Combinatorial Constraints:\n  Application to Materials Discovery",
    "abstract": "Identifying important components or factors in large amounts of noisy data is\na key problem in machine learning and data mining. Motivated by a pattern\ndecomposition problem in materials discovery, aimed at discovering new\nmaterials for renewable energy, e.g. for fuel and solar cells, we introduce\nCombiFD, a framework for factor based pattern decomposition that allows the\nincorporation of a-priori knowledge as constraints, including complex\ncombinatorial constraints. In addition, we propose a new pattern decomposition\nalgorithm, called AMIQO, based on solving a sequence of (mixed-integer)\nquadratic programs. Our approach considerably outperforms the state of the art\non the materials discovery problem, scaling to larger datasets and recovering\nmore precise and physically meaningful decompositions. We also show the\neffectiveness of our approach for enforcing background knowledge on other\napplication domains.",
    "text": "Pattern Decomposition with Complex Combinatorial Constraints:\n  Application to Materials Discovery\n\nIdentifying important components or factors in large amounts of noisy data is\na key problem in machine learning and data mining. Motivated by a pattern\ndecomposition problem in materials discovery, aimed at discovering new\nmaterials for renewable energy, e.g. for fuel and solar cells, we introduce\nCombiFD, a framework for factor based pattern decomposition that allows the\nincorporation of a-priori knowledge as constraints, including complex\ncombinatorial constraints. In addition, we propose a new pattern decomposition\nalgorithm, called AMIQO, based on solving a sequence of (mixed-integer)\nquadratic programs. Our approach considerably outperforms the state of the art\non the materials discovery problem, scaling to larger datasets and recovering\nmore precise and physically meaningful decompositions. We also show the\neffectiveness of our approach for enforcing background knowledge on other\napplication domains."
  },
  {
    "id": "arxiv-128",
    "title": "Toward Multiple Federated Learning Services Resource Sharing in Mobile\n  Edge Networks",
    "abstract": "Federated Learning is a new learning scheme for collaborative training a\nshared prediction model while keeping data locally on participating devices. In\nthis paper, we study a new model of multiple federated learning services at the\nmulti-access edge computing server. Accordingly, the sharing of CPU resources\namong learning services at each mobile device for the local training process\nand allocating communication resources among mobile devices for exchanging\nlearning information must be considered. Furthermore, the convergence\nperformance of different learning services depends on the hyper-learning rate\nparameter that needs to be precisely decided. Towards this end, we propose a\njoint resource optimization and hyper-learning rate control problem, namely\nMS-FEDL, regarding the energy consumption of mobile devices and overall\nlearning time. We design a centralized algorithm based on the block coordinate\ndescent method and a decentralized JP-miADMM algorithm for solving the MS-FEDL\nproblem. Different from the centralized approach, the decentralized approach\nrequires many iterations to obtain but it allows each learning service to\nindependently manage the local resource and learning process without revealing\nthe learning service information. Our simulation results demonstrate the\nconvergence performance of our proposed algorithms and the superior performance\nof our proposed algorithms compared to the heuristic strategy.",
    "text": "Toward Multiple Federated Learning Services Resource Sharing in Mobile\n  Edge Networks\n\nFederated Learning is a new learning scheme for collaborative training a\nshared prediction model while keeping data locally on participating devices. In\nthis paper, we study a new model of multiple federated learning services at the\nmulti-access edge computing server. Accordingly, the sharing of CPU resources\namong learning services at each mobile device for the local training process\nand allocating communication resources among mobile devices for exchanging\nlearning information must be considered. Furthermore, the convergence\nperformance of different learning services depends on the hyper-learning rate\nparameter that needs to be precisely decided. Towards this end, we propose a\njoint resource optimization and hyper-learning rate control problem, namely\nMS-FEDL, regarding the energy consumption of mobile devices and overall\nlearning time. We design a centralized algorithm based on the block coordinate\ndescent method and a decentralized JP-miADMM algorithm for solving the MS-FEDL\nproblem. Different from the centralized approach, the decentralized approach\nrequires many iterations to obtain but it allows each learning service to\nindependently manage the local resource and learning process without revealing\nthe learning service information. Our simulation results demonstrate the\nconvergence performance of our proposed algorithms and the superior performance\nof our proposed algorithms compared to the heuristic strategy."
  },
  {
    "id": "arxiv-129",
    "title": "A Frobenius norm regularization method for convolutional kernels to\n  avoid unstable gradient problem",
    "abstract": "Convolutional neural network is a very important model of deep learning. It\ncan help avoid the exploding/vanishing gradient problem and improve the\ngeneralizability of a neural network if the singular values of the Jacobian of\na layer are bounded around $1$ in the training process. We propose a new\npenalty function for a convolutional kernel to let the singular values of the\ncorresponding transformation matrix are bounded around $1$. We show how to\ncarry out the gradient type methods. The penalty is about the structured\ntransformation matrix corresponding to a convolutional kernel. This provides a\nnew regularization method about the weights of convolutional layers.",
    "text": "A Frobenius norm regularization method for convolutional kernels to\n  avoid unstable gradient problem\n\nConvolutional neural network is a very important model of deep learning. It\ncan help avoid the exploding/vanishing gradient problem and improve the\ngeneralizability of a neural network if the singular values of the Jacobian of\na layer are bounded around $1$ in the training process. We propose a new\npenalty function for a convolutional kernel to let the singular values of the\ncorresponding transformation matrix are bounded around $1$. We show how to\ncarry out the gradient type methods. The penalty is about the structured\ntransformation matrix corresponding to a convolutional kernel. This provides a\nnew regularization method about the weights of convolutional layers."
  },
  {
    "id": "arxiv-130",
    "title": "A giant with feet of clay: on the validity of the data that feed machine\n  learning in medicine",
    "abstract": "This paper considers the use of Machine Learning (ML) in medicine by focusing\non the main problem that this computational approach has been aimed at solving\nor at least minimizing: uncertainty. To this aim, we point out how uncertainty\nis so ingrained in medicine that it biases also the representation of clinical\nphenomena, that is the very input of ML models, thus undermining the clinical\nsignificance of their output. Recognizing this can motivate both medical\ndoctors, in taking more responsibility in the development and use of these\ndecision aids, and the researchers, in pursuing different ways to assess the\nvalue of these systems. In so doing, both designers and users could take this\nintrinsic characteristic of medicine more seriously and consider alternative\napproaches that do not \"sweep uncertainty under the rug\" within an objectivist\nfiction, which everyone can come up by believing as true.",
    "text": "A giant with feet of clay: on the validity of the data that feed machine\n  learning in medicine\n\nThis paper considers the use of Machine Learning (ML) in medicine by focusing\non the main problem that this computational approach has been aimed at solving\nor at least minimizing: uncertainty. To this aim, we point out how uncertainty\nis so ingrained in medicine that it biases also the representation of clinical\nphenomena, that is the very input of ML models, thus undermining the clinical\nsignificance of their output. Recognizing this can motivate both medical\ndoctors, in taking more responsibility in the development and use of these\ndecision aids, and the researchers, in pursuing different ways to assess the\nvalue of these systems. In so doing, both designers and users could take this\nintrinsic characteristic of medicine more seriously and consider alternative\napproaches that do not \"sweep uncertainty under the rug\" within an objectivist\nfiction, which everyone can come up by believing as true."
  },
  {
    "id": "arxiv-131",
    "title": "Low-Rank Modeling and Its Applications in Image Analysis",
    "abstract": "Low-rank modeling generally refers to a class of methods that solve problems\nby representing variables of interest as low-rank matrices. It has achieved\ngreat success in various fields including computer vision, data mining, signal\nprocessing and bioinformatics. Recently, much progress has been made in\ntheories, algorithms and applications of low-rank modeling, such as exact\nlow-rank matrix recovery via convex programming and matrix completion applied\nto collaborative filtering. These advances have brought more and more\nattentions to this topic. In this paper, we review the recent advance of\nlow-rank modeling, the state-of-the-art algorithms, and related applications in\nimage analysis. We first give an overview to the concept of low-rank modeling\nand challenging problems in this area. Then, we summarize the models and\nalgorithms for low-rank matrix recovery and illustrate their advantages and\nlimitations with numerical experiments. Next, we introduce a few applications\nof low-rank modeling in the context of image analysis. Finally, we conclude\nthis paper with some discussions.",
    "text": "Low-Rank Modeling and Its Applications in Image Analysis\n\nLow-rank modeling generally refers to a class of methods that solve problems\nby representing variables of interest as low-rank matrices. It has achieved\ngreat success in various fields including computer vision, data mining, signal\nprocessing and bioinformatics. Recently, much progress has been made in\ntheories, algorithms and applications of low-rank modeling, such as exact\nlow-rank matrix recovery via convex programming and matrix completion applied\nto collaborative filtering. These advances have brought more and more\nattentions to this topic. In this paper, we review the recent advance of\nlow-rank modeling, the state-of-the-art algorithms, and related applications in\nimage analysis. We first give an overview to the concept of low-rank modeling\nand challenging problems in this area. Then, we summarize the models and\nalgorithms for low-rank matrix recovery and illustrate their advantages and\nlimitations with numerical experiments. Next, we introduce a few applications\nof low-rank modeling in the context of image analysis. Finally, we conclude\nthis paper with some discussions."
  },
  {
    "id": "arxiv-132",
    "title": "Bidirectional Learning for Robust Neural Networks",
    "abstract": "A multilayer perceptron can behave as a generative classifier by applying\nbidirectional learning (BL). It consists of training an undirected neural\nnetwork to map input to output and vice-versa; therefore it can produce a\nclassifier in one direction, and a generator in the opposite direction for the\nsame data. The learning process of BL tries to reproduce the neuroplasticity\nstated in Hebbian theory using only backward propagation of errors. In this\npaper, two novel learning techniques are introduced which use BL for improving\nrobustness to white noise static and adversarial examples. The first method is\nbidirectional propagation of errors, which the error propagation occurs in\nbackward and forward directions. Motivated by the fact that its generative\nmodel receives as input a constant vector per class, we introduce as a second\nmethod the hybrid adversarial networks (HAN). Its generative model receives a\nrandom vector as input and its training is based on generative adversarial\nnetworks (GAN). To assess the performance of BL, we perform experiments using\nseveral architectures with fully and convolutional layers, with and without\nbias. Experimental results show that both methods improve robustness to white\nnoise static and adversarial examples, and even increase accuracy, but have\ndifferent behavior depending on the architecture and task, being more\nbeneficial to use the one or the other. Nevertheless, HAN using a convolutional\narchitecture with batch normalization presents outstanding robustness, reaching\nstate-of-the-art accuracy on adversarial examples of hand-written digits.",
    "text": "Bidirectional Learning for Robust Neural Networks\n\nA multilayer perceptron can behave as a generative classifier by applying\nbidirectional learning (BL). It consists of training an undirected neural\nnetwork to map input to output and vice-versa; therefore it can produce a\nclassifier in one direction, and a generator in the opposite direction for the\nsame data. The learning process of BL tries to reproduce the neuroplasticity\nstated in Hebbian theory using only backward propagation of errors. In this\npaper, two novel learning techniques are introduced which use BL for improving\nrobustness to white noise static and adversarial examples. The first method is\nbidirectional propagation of errors, which the error propagation occurs in\nbackward and forward directions. Motivated by the fact that its generative\nmodel receives as input a constant vector per class, we introduce as a second\nmethod the hybrid adversarial networks (HAN). Its generative model receives a\nrandom vector as input and its training is based on generative adversarial\nnetworks (GAN). To assess the performance of BL, we perform experiments using\nseveral architectures with fully and convolutional layers, with and without\nbias. Experimental results show that both methods improve robustness to white\nnoise static and adversarial examples, and even increase accuracy, but have\ndifferent behavior depending on the architecture and task, being more\nbeneficial to use the one or the other. Nevertheless, HAN using a convolutional\narchitecture with batch normalization presents outstanding robustness, reaching\nstate-of-the-art accuracy on adversarial examples of hand-written digits."
  },
  {
    "id": "arxiv-133",
    "title": "Environment Probing Interaction Policies",
    "abstract": "A key challenge in reinforcement learning (RL) is environment generalization:\na policy trained to solve a task in one environment often fails to solve the\nsame task in a slightly different test environment. A common approach to\nimprove inter-environment transfer is to learn policies that are invariant to\nthe distribution of testing environments. However, we argue that instead of\nbeing invariant, the policy should identify the specific nuances of an\nenvironment and exploit them to achieve better performance. In this work, we\npropose the 'Environment-Probing' Interaction (EPI) policy, a policy that\nprobes a new environment to extract an implicit understanding of that\nenvironment's behavior. Once this environment-specific information is obtained,\nit is used as an additional input to a task-specific policy that can now\nperform environment-conditioned actions to solve a task. To learn these\nEPI-policies, we present a reward function based on transition predictability.\nSpecifically, a higher reward is given if the trajectory generated by the\nEPI-policy can be used to better predict transitions. We experimentally show\nthat EPI-conditioned task-specific policies significantly outperform commonly\nused policy generalization methods on novel testing environments.",
    "text": "Environment Probing Interaction Policies\n\nA key challenge in reinforcement learning (RL) is environment generalization:\na policy trained to solve a task in one environment often fails to solve the\nsame task in a slightly different test environment. A common approach to\nimprove inter-environment transfer is to learn policies that are invariant to\nthe distribution of testing environments. However, we argue that instead of\nbeing invariant, the policy should identify the specific nuances of an\nenvironment and exploit them to achieve better performance. In this work, we\npropose the 'Environment-Probing' Interaction (EPI) policy, a policy that\nprobes a new environment to extract an implicit understanding of that\nenvironment's behavior. Once this environment-specific information is obtained,\nit is used as an additional input to a task-specific policy that can now\nperform environment-conditioned actions to solve a task. To learn these\nEPI-policies, we present a reward function based on transition predictability.\nSpecifically, a higher reward is given if the trajectory generated by the\nEPI-policy can be used to better predict transitions. We experimentally show\nthat EPI-conditioned task-specific policies significantly outperform commonly\nused policy generalization methods on novel testing environments."
  },
  {
    "id": "arxiv-134",
    "title": "Learning Deep Temporal Representations for Brain Decoding",
    "abstract": "Functional magnetic resonance imaging produces high dimensional data, with a\nless then ideal number of labelled samples for brain decoding tasks (predicting\nbrain states). In this study, we propose a new deep temporal convolutional\nneural network architecture with spatial pooling for brain decoding which aims\nto reduce dimensionality of feature space along with improved classification\nperformance. Temporal representations (filters) for each layer of the\nconvolutional model are learned by leveraging unlabelled fMRI data in an\nunsupervised fashion with regularized autoencoders. Learned temporal\nrepresentations in multiple levels capture the regularities in the temporal\ndomain and are observed to be a rich bank of activation patterns which also\nexhibit similarities to the actual hemodynamic responses. Further, spatial\npooling layers in the convolutional architecture reduce the dimensionality\nwithout losing excessive information. By employing the proposed temporal\nconvolutional architecture with spatial pooling, raw input fMRI data is mapped\nto a non-linear, highly-expressive and low-dimensional feature space where the\nfinal classification is conducted. In addition, we propose a simple heuristic\napproach for hyper-parameter tuning when no validation data is available.\nProposed method is tested on a ten class recognition memory experiment with\nnine subjects. The results support the efficiency and potential of the proposed\nmodel, compared to the baseline multi-voxel pattern analysis techniques.",
    "text": "Learning Deep Temporal Representations for Brain Decoding\n\nFunctional magnetic resonance imaging produces high dimensional data, with a\nless then ideal number of labelled samples for brain decoding tasks (predicting\nbrain states). In this study, we propose a new deep temporal convolutional\nneural network architecture with spatial pooling for brain decoding which aims\nto reduce dimensionality of feature space along with improved classification\nperformance. Temporal representations (filters) for each layer of the\nconvolutional model are learned by leveraging unlabelled fMRI data in an\nunsupervised fashion with regularized autoencoders. Learned temporal\nrepresentations in multiple levels capture the regularities in the temporal\ndomain and are observed to be a rich bank of activation patterns which also\nexhibit similarities to the actual hemodynamic responses. Further, spatial\npooling layers in the convolutional architecture reduce the dimensionality\nwithout losing excessive information. By employing the proposed temporal\nconvolutional architecture with spatial pooling, raw input fMRI data is mapped\nto a non-linear, highly-expressive and low-dimensional feature space where the\nfinal classification is conducted. In addition, we propose a simple heuristic\napproach for hyper-parameter tuning when no validation data is available.\nProposed method is tested on a ten class recognition memory experiment with\nnine subjects. The results support the efficiency and potential of the proposed\nmodel, compared to the baseline multi-voxel pattern analysis techniques."
  },
  {
    "id": "arxiv-135",
    "title": "Local Communication Protocols for Learning Complex Swarm Behaviors with\n  Deep Reinforcement Learning",
    "abstract": "Swarm systems constitute a challenging problem for reinforcement learning\n(RL) as the algorithm needs to learn decentralized control policies that can\ncope with limited local sensing and communication abilities of the agents.\nWhile it is often difficult to directly define the behavior of the agents,\nsimple communication protocols can be defined more easily using prior knowledge\nabout the given task. In this paper, we propose a number of simple\ncommunication protocols that can be exploited by deep reinforcement learning to\nfind decentralized control policies in a multi-robot swarm environment. The\nprotocols are based on histograms that encode the local neighborhood relations\nof the agents and can also transmit task-specific information, such as the\nshortest distance and direction to a desired target. In our framework, we use\nan adaptation of Trust Region Policy Optimization to learn complex\ncollaborative tasks, such as formation building and building a communication\nlink. We evaluate our findings in a simulated 2D-physics environment, and\ncompare the implications of different communication protocols.",
    "text": "Local Communication Protocols for Learning Complex Swarm Behaviors with\n  Deep Reinforcement Learning\n\nSwarm systems constitute a challenging problem for reinforcement learning\n(RL) as the algorithm needs to learn decentralized control policies that can\ncope with limited local sensing and communication abilities of the agents.\nWhile it is often difficult to directly define the behavior of the agents,\nsimple communication protocols can be defined more easily using prior knowledge\nabout the given task. In this paper, we propose a number of simple\ncommunication protocols that can be exploited by deep reinforcement learning to\nfind decentralized control policies in a multi-robot swarm environment. The\nprotocols are based on histograms that encode the local neighborhood relations\nof the agents and can also transmit task-specific information, such as the\nshortest distance and direction to a desired target. In our framework, we use\nan adaptation of Trust Region Policy Optimization to learn complex\ncollaborative tasks, such as formation building and building a communication\nlink. We evaluate our findings in a simulated 2D-physics environment, and\ncompare the implications of different communication protocols."
  },
  {
    "id": "arxiv-136",
    "title": "Selective Transfer with Reinforced Transfer Network for Partial Domain\n  Adaptation",
    "abstract": "One crucial aspect of partial domain adaptation (PDA) is how to select the\nrelevant source samples in the shared classes for knowledge transfer. Previous\nPDA methods tackle this problem by re-weighting the source samples based on\ntheir high-level information (deep features). However, since the domain shift\nbetween source and target domains, only using the deep features for sample\nselection is defective. We argue that it is more reasonable to additionally\nexploit the pixel-level information for PDA problem, as the appearance\ndifference between outlier source classes and target classes is significantly\nlarge. In this paper, we propose a reinforced transfer network (RTNet), which\nutilizes both high-level and pixel-level information for PDA problem. Our RTNet\nis composed of a reinforced data selector (RDS) based on reinforcement learning\n(RL), which filters out the outlier source samples, and a domain adaptation\nmodel which minimizes the domain discrepancy in the shared label space.\nSpecifically, in the RDS, we design a novel reward based on the reconstruct\nerrors of selected source samples on the target generator, which introduces the\npixel-level information to guide the learning of RDS. Besides, we develope a\nstate containing high-level information, which used by the RDS for sample\nselection. The proposed RDS is a general module, which can be easily integrated\ninto existing DA models to make them fit the PDA situation. Extensive\nexperiments indicate that RTNet can achieve state-of-the-art performance for\nPDA tasks on several benchmark datasets.",
    "text": "Selective Transfer with Reinforced Transfer Network for Partial Domain\n  Adaptation\n\nOne crucial aspect of partial domain adaptation (PDA) is how to select the\nrelevant source samples in the shared classes for knowledge transfer. Previous\nPDA methods tackle this problem by re-weighting the source samples based on\ntheir high-level information (deep features). However, since the domain shift\nbetween source and target domains, only using the deep features for sample\nselection is defective. We argue that it is more reasonable to additionally\nexploit the pixel-level information for PDA problem, as the appearance\ndifference between outlier source classes and target classes is significantly\nlarge. In this paper, we propose a reinforced transfer network (RTNet), which\nutilizes both high-level and pixel-level information for PDA problem. Our RTNet\nis composed of a reinforced data selector (RDS) based on reinforcement learning\n(RL), which filters out the outlier source samples, and a domain adaptation\nmodel which minimizes the domain discrepancy in the shared label space.\nSpecifically, in the RDS, we design a novel reward based on the reconstruct\nerrors of selected source samples on the target generator, which introduces the\npixel-level information to guide the learning of RDS. Besides, we develope a\nstate containing high-level information, which used by the RDS for sample\nselection. The proposed RDS is a general module, which can be easily integrated\ninto existing DA models to make them fit the PDA situation. Extensive\nexperiments indicate that RTNet can achieve state-of-the-art performance for\nPDA tasks on several benchmark datasets."
  },
  {
    "id": "arxiv-137",
    "title": "Top-$k$ eXtreme Contextual Bandits with Arm Hierarchy",
    "abstract": "Motivated by modern applications, such as online advertisement and\nrecommender systems, we study the top-$k$ extreme contextual bandits problem,\nwhere the total number of arms can be enormous, and the learner is allowed to\nselect $k$ arms and observe all or some of the rewards for the chosen arms. We\nfirst propose an algorithm for the non-extreme realizable setting, utilizing\nthe Inverse Gap Weighting strategy for selecting multiple arms. We show that\nour algorithm has a regret guarantee of $O(k\\sqrt{(A-k+1)T \\log\n(|\\mathcal{F}|T)})$, where $A$ is the total number of arms and $\\mathcal{F}$ is\nthe class containing the regression function, while only requiring\n$\\tilde{O}(A)$ computation per time step. In the extreme setting, where the\ntotal number of arms can be in the millions, we propose a practically-motivated\narm hierarchy model that induces a certain structure in mean rewards to ensure\nstatistical and computational efficiency. The hierarchical structure allows for\nan exponential reduction in the number of relevant arms for each context, thus\nresulting in a regret guarantee of $O(k\\sqrt{(\\log A-k+1)T \\log\n(|\\mathcal{F}|T)})$. Finally, we implement our algorithm using a hierarchical\nlinear function class and show superior performance with respect to well-known\nbenchmarks on simulated bandit feedback experiments using extreme multi-label\nclassification datasets. On a dataset with three million arms, our reduction\nscheme has an average inference time of only 7.9 milliseconds, which is a 100x\nimprovement.",
    "text": "Top-$k$ eXtreme Contextual Bandits with Arm Hierarchy\n\nMotivated by modern applications, such as online advertisement and\nrecommender systems, we study the top-$k$ extreme contextual bandits problem,\nwhere the total number of arms can be enormous, and the learner is allowed to\nselect $k$ arms and observe all or some of the rewards for the chosen arms. We\nfirst propose an algorithm for the non-extreme realizable setting, utilizing\nthe Inverse Gap Weighting strategy for selecting multiple arms. We show that\nour algorithm has a regret guarantee of $O(k\\sqrt{(A-k+1)T \\log\n(|\\mathcal{F}|T)})$, where $A$ is the total number of arms and $\\mathcal{F}$ is\nthe class containing the regression function, while only requiring\n$\\tilde{O}(A)$ computation per time step. In the extreme setting, where the\ntotal number of arms can be in the millions, we propose a practically-motivated\narm hierarchy model that induces a certain structure in mean rewards to ensure\nstatistical and computational efficiency. The hierarchical structure allows for\nan exponential reduction in the number of relevant arms for each context, thus\nresulting in a regret guarantee of $O(k\\sqrt{(\\log A-k+1)T \\log\n(|\\mathcal{F}|T)})$. Finally, we implement our algorithm using a hierarchical\nlinear function class and show superior performance with respect to well-known\nbenchmarks on simulated bandit feedback experiments using extreme multi-label\nclassification datasets. On a dataset with three million arms, our reduction\nscheme has an average inference time of only 7.9 milliseconds, which is a 100x\nimprovement."
  },
  {
    "id": "arxiv-138",
    "title": "Characterization of Gradient Dominance and Regularity Conditions for\n  Neural Networks",
    "abstract": "The past decade has witnessed a successful application of deep learning to\nsolving many challenging problems in machine learning and artificial\nintelligence. However, the loss functions of deep neural networks (especially\nnonlinear networks) are still far from being well understood from a theoretical\naspect. In this paper, we enrich the current understanding of the landscape of\nthe square loss functions for three types of neural networks. Specifically,\nwhen the parameter matrices are square, we provide an explicit characterization\nof the global minimizers for linear networks, linear residual networks, and\nnonlinear networks with one hidden layer. Then, we establish two quadratic\ntypes of landscape properties for the square loss of these neural networks,\ni.e., the gradient dominance condition within the neighborhood of their full\nrank global minimizers, and the regularity condition along certain directions\nand within the neighborhood of their global minimizers. These two landscape\nproperties are desirable for the optimization around the global minimizers of\nthe loss function for these neural networks.",
    "text": "Characterization of Gradient Dominance and Regularity Conditions for\n  Neural Networks\n\nThe past decade has witnessed a successful application of deep learning to\nsolving many challenging problems in machine learning and artificial\nintelligence. However, the loss functions of deep neural networks (especially\nnonlinear networks) are still far from being well understood from a theoretical\naspect. In this paper, we enrich the current understanding of the landscape of\nthe square loss functions for three types of neural networks. Specifically,\nwhen the parameter matrices are square, we provide an explicit characterization\nof the global minimizers for linear networks, linear residual networks, and\nnonlinear networks with one hidden layer. Then, we establish two quadratic\ntypes of landscape properties for the square loss of these neural networks,\ni.e., the gradient dominance condition within the neighborhood of their full\nrank global minimizers, and the regularity condition along certain directions\nand within the neighborhood of their global minimizers. These two landscape\nproperties are desirable for the optimization around the global minimizers of\nthe loss function for these neural networks."
  },
  {
    "id": "arxiv-139",
    "title": "Online Structured Laplace Approximations For Overcoming Catastrophic\n  Forgetting",
    "abstract": "We introduce the Kronecker factored online Laplace approximation for\novercoming catastrophic forgetting in neural networks. The method is grounded\nin a Bayesian online learning framework, where we recursively approximate the\nposterior after every task with a Gaussian, leading to a quadratic penalty on\nchanges to the weights. The Laplace approximation requires calculating the\nHessian around a mode, which is typically intractable for modern architectures.\nIn order to make our method scalable, we leverage recent block-diagonal\nKronecker factored approximations to the curvature. Our algorithm achieves over\n90% test accuracy across a sequence of 50 instantiations of the permuted MNIST\ndataset, substantially outperforming related methods for overcoming\ncatastrophic forgetting.",
    "text": "Online Structured Laplace Approximations For Overcoming Catastrophic\n  Forgetting\n\nWe introduce the Kronecker factored online Laplace approximation for\novercoming catastrophic forgetting in neural networks. The method is grounded\nin a Bayesian online learning framework, where we recursively approximate the\nposterior after every task with a Gaussian, leading to a quadratic penalty on\nchanges to the weights. The Laplace approximation requires calculating the\nHessian around a mode, which is typically intractable for modern architectures.\nIn order to make our method scalable, we leverage recent block-diagonal\nKronecker factored approximations to the curvature. Our algorithm achieves over\n90% test accuracy across a sequence of 50 instantiations of the permuted MNIST\ndataset, substantially outperforming related methods for overcoming\ncatastrophic forgetting."
  },
  {
    "id": "arxiv-140",
    "title": "Wireless Sensing With Deep Spectrogram Network and Primitive Based\n  Autoregressive Hybrid Channel Model",
    "abstract": "Human motion recognition (HMR) based on wireless sensing is a low-cost\ntechnique for scene understanding. Current HMR systems adopt support vector\nmachines (SVMs) and convolutional neural networks (CNNs) to classify radar\nsignals. However, whether a deeper learning model could improve the system\nperformance is currently not known. On the other hand, training a machine\nlearning model requires a large dataset, but data gathering from experiment is\ncost-expensive and time-consuming. Although wireless channel models can be\nadopted for dataset generation, current channel models are mostly designed for\ncommunication rather than sensing. To address the above problems, this paper\nproposes a deep spectrogram network (DSN) by leveraging the residual mapping\ntechnique to enhance the HMR performance. Furthermore, a primitive based\nautoregressive hybrid (PBAH) channel model is developed, which facilitates\nefficient training and testing dataset generation for HMR in a virtual\nenvironment. Experimental results demonstrate that the proposed PBAH channel\nmodel matches the actual experimental data very well and the proposed DSN\nachieves significantly smaller recognition error than that of CNN.",
    "text": "Wireless Sensing With Deep Spectrogram Network and Primitive Based\n  Autoregressive Hybrid Channel Model\n\nHuman motion recognition (HMR) based on wireless sensing is a low-cost\ntechnique for scene understanding. Current HMR systems adopt support vector\nmachines (SVMs) and convolutional neural networks (CNNs) to classify radar\nsignals. However, whether a deeper learning model could improve the system\nperformance is currently not known. On the other hand, training a machine\nlearning model requires a large dataset, but data gathering from experiment is\ncost-expensive and time-consuming. Although wireless channel models can be\nadopted for dataset generation, current channel models are mostly designed for\ncommunication rather than sensing. To address the above problems, this paper\nproposes a deep spectrogram network (DSN) by leveraging the residual mapping\ntechnique to enhance the HMR performance. Furthermore, a primitive based\nautoregressive hybrid (PBAH) channel model is developed, which facilitates\nefficient training and testing dataset generation for HMR in a virtual\nenvironment. Experimental results demonstrate that the proposed PBAH channel\nmodel matches the actual experimental data very well and the proposed DSN\nachieves significantly smaller recognition error than that of CNN."
  },
  {
    "id": "arxiv-141",
    "title": "HSCoNAS: Hardware-Software Co-Design of Efficient DNNs via Neural\n  Architecture Search",
    "abstract": "In this paper, we present a novel multi-objective hardware-aware neural\narchitecture search (NAS) framework, namely HSCoNAS, to automate the design of\ndeep neural networks (DNNs) with high accuracy but low latency upon target\nhardware. To accomplish this goal, we first propose an effective hardware\nperformance modeling method to approximate the runtime latency of DNNs on\ntarget hardware, which will be integrated into HSCoNAS to avoid the tedious\non-device measurements. Besides, we propose two novel techniques, i.e., dynamic\nchannel scaling to maximize the accuracy under the specified latency and\nprogressive space shrinking to refine the search space towards target hardware\nas well as alleviate the search overheads. These two techniques jointly work to\nallow HSCoNAS to perform fine-grained and efficient explorations. Finally, an\nevolutionary algorithm (EA) is incorporated to conduct the architecture search.\nExtensive experiments on ImageNet are conducted upon diverse target hardware,\ni.e., GPU, CPU, and edge device to demonstrate the superiority of HSCoNAS over\nrecent state-of-the-art approaches.",
    "text": "HSCoNAS: Hardware-Software Co-Design of Efficient DNNs via Neural\n  Architecture Search\n\nIn this paper, we present a novel multi-objective hardware-aware neural\narchitecture search (NAS) framework, namely HSCoNAS, to automate the design of\ndeep neural networks (DNNs) with high accuracy but low latency upon target\nhardware. To accomplish this goal, we first propose an effective hardware\nperformance modeling method to approximate the runtime latency of DNNs on\ntarget hardware, which will be integrated into HSCoNAS to avoid the tedious\non-device measurements. Besides, we propose two novel techniques, i.e., dynamic\nchannel scaling to maximize the accuracy under the specified latency and\nprogressive space shrinking to refine the search space towards target hardware\nas well as alleviate the search overheads. These two techniques jointly work to\nallow HSCoNAS to perform fine-grained and efficient explorations. Finally, an\nevolutionary algorithm (EA) is incorporated to conduct the architecture search.\nExtensive experiments on ImageNet are conducted upon diverse target hardware,\ni.e., GPU, CPU, and edge device to demonstrate the superiority of HSCoNAS over\nrecent state-of-the-art approaches."
  },
  {
    "id": "arxiv-142",
    "title": "CIDER: Exploiting Hyperspherical Embeddings for Out-of-Distribution\n  Detection",
    "abstract": "Out-of-distribution (OOD) detection is a critical task for reliable machine\nlearning. Recent advances in representation learning give rise to developments\nin distance-based OOD detection, where testing samples are detected as OOD if\nthey are relatively far away from the centroids or prototypes of\nin-distribution (ID) classes. However, prior methods directly take\noff-the-shelf loss functions that suffice for classifying ID samples, but are\nnot optimally designed for OOD detection. In this paper, we propose CIDER, a\nsimple and effective representation learning framework by exploiting\nhyperspherical embeddings for OOD detection. CIDER jointly optimizes two losses\nto promote strong ID-OOD separability: (1) a dispersion loss that promotes\nlarge angular distances among different class prototypes, and (2) a compactness\nloss that encourages samples to be close to their class prototypes. We show\nthat CIDER is effective under various settings and establishes state-of-the-art\nperformance. On a hard OOD detection task CIFAR-100 vs. CIFAR-10, our method\nsubstantially improves the AUROC by 14.20% compared to the embeddings learned\nby the cross-entropy loss.",
    "text": "CIDER: Exploiting Hyperspherical Embeddings for Out-of-Distribution\n  Detection\n\nOut-of-distribution (OOD) detection is a critical task for reliable machine\nlearning. Recent advances in representation learning give rise to developments\nin distance-based OOD detection, where testing samples are detected as OOD if\nthey are relatively far away from the centroids or prototypes of\nin-distribution (ID) classes. However, prior methods directly take\noff-the-shelf loss functions that suffice for classifying ID samples, but are\nnot optimally designed for OOD detection. In this paper, we propose CIDER, a\nsimple and effective representation learning framework by exploiting\nhyperspherical embeddings for OOD detection. CIDER jointly optimizes two losses\nto promote strong ID-OOD separability: (1) a dispersion loss that promotes\nlarge angular distances among different class prototypes, and (2) a compactness\nloss that encourages samples to be close to their class prototypes. We show\nthat CIDER is effective under various settings and establishes state-of-the-art\nperformance. On a hard OOD detection task CIFAR-100 vs. CIFAR-10, our method\nsubstantially improves the AUROC by 14.20% compared to the embeddings learned\nby the cross-entropy loss."
  },
  {
    "id": "arxiv-143",
    "title": "Precision and Recall for Time Series",
    "abstract": "Classical anomaly detection is principally concerned with point-based\nanomalies, those anomalies that occur at a single point in time. Yet, many\nreal-world anomalies are range-based, meaning they occur over a period of time.\nMotivated by this observation, we present a new mathematical model to evaluate\nthe accuracy of time series classification algorithms. Our model expands the\nwell-known Precision and Recall metrics to measure ranges, while simultaneously\nenabling customization support for domain-specific preferences.",
    "text": "Precision and Recall for Time Series\n\nClassical anomaly detection is principally concerned with point-based\nanomalies, those anomalies that occur at a single point in time. Yet, many\nreal-world anomalies are range-based, meaning they occur over a period of time.\nMotivated by this observation, we present a new mathematical model to evaluate\nthe accuracy of time series classification algorithms. Our model expands the\nwell-known Precision and Recall metrics to measure ranges, while simultaneously\nenabling customization support for domain-specific preferences."
  },
  {
    "id": "arxiv-144",
    "title": "ClimART: A Benchmark Dataset for Emulating Atmospheric Radiative\n  Transfer in Weather and Climate Models",
    "abstract": "Numerical simulations of Earth's weather and climate require substantial\namounts of computation. This has led to a growing interest in replacing\nsubroutines that explicitly compute physical processes with approximate machine\nlearning (ML) methods that are fast at inference time. Within weather and\nclimate models, atmospheric radiative transfer (RT) calculations are especially\nexpensive. This has made them a popular target for neural network-based\nemulators. However, prior work is hard to compare due to the lack of a\ncomprehensive dataset and standardized best practices for ML benchmarking. To\nfill this gap, we build a large dataset, ClimART, with more than \\emph{10\nmillion samples from present, pre-industrial, and future climate conditions},\nbased on the Canadian Earth System Model. ClimART poses several methodological\nchallenges for the ML community, such as multiple out-of-distribution test\nsets, underlying domain physics, and a trade-off between accuracy and inference\nspeed. We also present several novel baselines that indicate shortcomings of\ndatasets and network architectures used in prior work. Download instructions,\nbaselines, and code are available at: https://github.com/RolnickLab/climart",
    "text": "ClimART: A Benchmark Dataset for Emulating Atmospheric Radiative\n  Transfer in Weather and Climate Models\n\nNumerical simulations of Earth's weather and climate require substantial\namounts of computation. This has led to a growing interest in replacing\nsubroutines that explicitly compute physical processes with approximate machine\nlearning (ML) methods that are fast at inference time. Within weather and\nclimate models, atmospheric radiative transfer (RT) calculations are especially\nexpensive. This has made them a popular target for neural network-based\nemulators. However, prior work is hard to compare due to the lack of a\ncomprehensive dataset and standardized best practices for ML benchmarking. To\nfill this gap, we build a large dataset, ClimART, with more than \\emph{10\nmillion samples from present, pre-industrial, and future climate conditions},\nbased on the Canadian Earth System Model. ClimART poses several methodological\nchallenges for the ML community, such as multiple out-of-distribution test\nsets, underlying domain physics, and a trade-off between accuracy and inference\nspeed. We also present several novel baselines that indicate shortcomings of\ndatasets and network architectures used in prior work. Download instructions,\nbaselines, and code are available at: https://github.com/RolnickLab/climart"
  },
  {
    "id": "arxiv-145",
    "title": "Trustworthy AI Inference Systems: An Industry Research View",
    "abstract": "In this work, we provide an industry research view for approaching the\ndesign, deployment, and operation of trustworthy Artificial Intelligence (AI)\ninference systems. Such systems provide customers with timely, informed, and\ncustomized inferences to aid their decision, while at the same time utilizing\nappropriate security protection mechanisms for AI models. Additionally, such\nsystems should also use Privacy-Enhancing Technologies (PETs) to protect\ncustomers' data at any time.\n  To approach the subject, we start by introducing trends in AI inference\nsystems. We continue by elaborating on the relationship between Intellectual\nProperty (IP) and private data protection in such systems. Regarding the\nprotection mechanisms, we survey the security and privacy building blocks\ninstrumental in designing, building, deploying, and operating private AI\ninference systems. For example, we highlight opportunities and challenges in AI\nsystems using trusted execution environments combined with more recent advances\nin cryptographic techniques to protect data in use. Finally, we outline areas\nof further development that require the global collective attention of\nindustry, academia, and government researchers to sustain the operation of\ntrustworthy AI inference systems.",
    "text": "Trustworthy AI Inference Systems: An Industry Research View\n\nIn this work, we provide an industry research view for approaching the\ndesign, deployment, and operation of trustworthy Artificial Intelligence (AI)\ninference systems. Such systems provide customers with timely, informed, and\ncustomized inferences to aid their decision, while at the same time utilizing\nappropriate security protection mechanisms for AI models. Additionally, such\nsystems should also use Privacy-Enhancing Technologies (PETs) to protect\ncustomers' data at any time.\n  To approach the subject, we start by introducing trends in AI inference\nsystems. We continue by elaborating on the relationship between Intellectual\nProperty (IP) and private data protection in such systems. Regarding the\nprotection mechanisms, we survey the security and privacy building blocks\ninstrumental in designing, building, deploying, and operating private AI\ninference systems. For example, we highlight opportunities and challenges in AI\nsystems using trusted execution environments combined with more recent advances\nin cryptographic techniques to protect data in use. Finally, we outline areas\nof further development that require the global collective attention of\nindustry, academia, and government researchers to sustain the operation of\ntrustworthy AI inference systems."
  },
  {
    "id": "arxiv-146",
    "title": "DAIR: Data Augmented Invariant Regularization",
    "abstract": "While deep learning through empirical risk minimization (ERM) has succeeded\nat achieving human-level performance at a variety of complex tasks, ERM\ngeneralizes poorly to distribution shift. This is partly explained by\noverfitting to spurious features such as background in images or named entities\nin natural language. Synthetic data augmentation followed by empirical risk\nminimization (DA-ERM) is a simple and widely used solution to remedy this\nproblem. In addition, consistency regularization could be applied to further\npromote model performance to be consistent on the augmented sample and the\noriginal one. In this paper, we propose data augmented invariant regularization\n(DAIR), a simple form of consistency regularization that is applied directly on\nthe loss function rather than intermediate features, making it widely\napplicable regardless of network architecture or problem setup. We apply DAIR\nto multiple real-world learning problems, namely robust regression, visual\nquestion answering, robust deep neural network training, and neural\ntask-oriented dialog modeling. Our experiments show that DAIR consistently\noutperforms ERM and DA-ERM with little marginal cost and sets new\nstate-of-the-art results in several benchmarks.",
    "text": "DAIR: Data Augmented Invariant Regularization\n\nWhile deep learning through empirical risk minimization (ERM) has succeeded\nat achieving human-level performance at a variety of complex tasks, ERM\ngeneralizes poorly to distribution shift. This is partly explained by\noverfitting to spurious features such as background in images or named entities\nin natural language. Synthetic data augmentation followed by empirical risk\nminimization (DA-ERM) is a simple and widely used solution to remedy this\nproblem. In addition, consistency regularization could be applied to further\npromote model performance to be consistent on the augmented sample and the\noriginal one. In this paper, we propose data augmented invariant regularization\n(DAIR), a simple form of consistency regularization that is applied directly on\nthe loss function rather than intermediate features, making it widely\napplicable regardless of network architecture or problem setup. We apply DAIR\nto multiple real-world learning problems, namely robust regression, visual\nquestion answering, robust deep neural network training, and neural\ntask-oriented dialog modeling. Our experiments show that DAIR consistently\noutperforms ERM and DA-ERM with little marginal cost and sets new\nstate-of-the-art results in several benchmarks."
  },
  {
    "id": "arxiv-147",
    "title": "Programming with Neural Surrogates of Programs",
    "abstract": "Surrogates, models that mimic the behavior of programs, form the basis of a\nvariety of development workflows. We study three surrogate-based design\npatterns, evaluating each in case studies on a large-scale CPU simulator.\n  With surrogate compilation, programmers develop a surrogate that mimics the\nbehavior of a program to deploy to end-users in place of the original program.\nSurrogate compilation accelerates the CPU simulator under study by $1.6\\times$.\nWith surrogate adaptation, programmers develop a surrogate of a program then\nretrain that surrogate on a different task. Surrogate adaptation decreases the\nsimulator's error by up to $50\\%$. With surrogate optimization, programmers\ndevelop a surrogate of a program, optimize input parameters of the surrogate,\nthen plug the optimized input parameters back into the original program.\nSurrogate optimization finds simulation parameters that decrease the\nsimulator's error by $5\\%$ compared to the error induced by expert-set\nparameters.\n  In this paper we formalize this taxonomy of surrogate-based design patterns.\nWe further describe the programming methodology common to all three design\npatterns. Our work builds a foundation for the emerging class of workflows\nbased on programming with surrogates of programs.",
    "text": "Programming with Neural Surrogates of Programs\n\nSurrogates, models that mimic the behavior of programs, form the basis of a\nvariety of development workflows. We study three surrogate-based design\npatterns, evaluating each in case studies on a large-scale CPU simulator.\n  With surrogate compilation, programmers develop a surrogate that mimics the\nbehavior of a program to deploy to end-users in place of the original program.\nSurrogate compilation accelerates the CPU simulator under study by $1.6\\times$.\nWith surrogate adaptation, programmers develop a surrogate of a program then\nretrain that surrogate on a different task. Surrogate adaptation decreases the\nsimulator's error by up to $50\\%$. With surrogate optimization, programmers\ndevelop a surrogate of a program, optimize input parameters of the surrogate,\nthen plug the optimized input parameters back into the original program.\nSurrogate optimization finds simulation parameters that decrease the\nsimulator's error by $5\\%$ compared to the error induced by expert-set\nparameters.\n  In this paper we formalize this taxonomy of surrogate-based design patterns.\nWe further describe the programming methodology common to all three design\npatterns. Our work builds a foundation for the emerging class of workflows\nbased on programming with surrogates of programs."
  },
  {
    "id": "arxiv-148",
    "title": "Comparison three methods of clustering: k-means, spectral clustering and\n  hierarchical clustering",
    "abstract": "Comparison of three kind of the clustering and find cost function and loss\nfunction and calculate them. Error rate of the clustering methods and how to\ncalculate the error percentage always be one on the important factor for\nevaluating the clustering methods, so this paper introduce one way to calculate\nthe error rate of clustering methods. Clustering algorithms can be divided into\nseveral categories including partitioning clustering algorithms, hierarchical\nalgorithms and density based algorithms. Generally speaking we should compare\nclustering algorithms by Scalability, Ability to work with different attribute,\nClusters formed by conventional, Having minimal knowledge of the computer to\nrecognize the input parameters, Classes for dealing with noise and extra\ndeposition that same error rate for clustering a new data, Thus, there is no\neffect on the input data, different dimensions of high levels, K-means is one\nof the simplest approach to clustering that clustering is an unsupervised\nproblem.",
    "text": "Comparison three methods of clustering: k-means, spectral clustering and\n  hierarchical clustering\n\nComparison of three kind of the clustering and find cost function and loss\nfunction and calculate them. Error rate of the clustering methods and how to\ncalculate the error percentage always be one on the important factor for\nevaluating the clustering methods, so this paper introduce one way to calculate\nthe error rate of clustering methods. Clustering algorithms can be divided into\nseveral categories including partitioning clustering algorithms, hierarchical\nalgorithms and density based algorithms. Generally speaking we should compare\nclustering algorithms by Scalability, Ability to work with different attribute,\nClusters formed by conventional, Having minimal knowledge of the computer to\nrecognize the input parameters, Classes for dealing with noise and extra\ndeposition that same error rate for clustering a new data, Thus, there is no\neffect on the input data, different dimensions of high levels, K-means is one\nof the simplest approach to clustering that clustering is an unsupervised\nproblem."
  },
  {
    "id": "arxiv-149",
    "title": "Submodular Mutual Information for Targeted Data Subset Selection",
    "abstract": "With the rapid growth of data, it is becoming increasingly difficult to train\nor improve deep learning models with the right subset of data. We show that\nthis problem can be effectively solved at an additional labeling cost by\ntargeted data subset selection(TSS) where a subset of unlabeled data points\nsimilar to an auxiliary set are added to the training data. We do so by using a\nrich class of Submodular Mutual Information (SMI) functions and demonstrate its\neffectiveness for image classification on CIFAR-10 and MNIST datasets. Lastly,\nwe compare the performance of SMI functions for TSS with other state-of-the-art\nmethods for closely related problems like active learning. Using SMI functions,\nwe observe ~20-30% gain over the model's performance before re-training with\nadded targeted subset; ~12% more than other methods.",
    "text": "Submodular Mutual Information for Targeted Data Subset Selection\n\nWith the rapid growth of data, it is becoming increasingly difficult to train\nor improve deep learning models with the right subset of data. We show that\nthis problem can be effectively solved at an additional labeling cost by\ntargeted data subset selection(TSS) where a subset of unlabeled data points\nsimilar to an auxiliary set are added to the training data. We do so by using a\nrich class of Submodular Mutual Information (SMI) functions and demonstrate its\neffectiveness for image classification on CIFAR-10 and MNIST datasets. Lastly,\nwe compare the performance of SMI functions for TSS with other state-of-the-art\nmethods for closely related problems like active learning. Using SMI functions,\nwe observe ~20-30% gain over the model's performance before re-training with\nadded targeted subset; ~12% more than other methods."
  },
  {
    "id": "arxiv-150",
    "title": "Multi-agent Bayesian Deep Reinforcement Learning for Microgrid Energy\n  Management under Communication Failures",
    "abstract": "Microgrids (MGs) are important players for the future transactive energy\nsystems where a number of intelligent Internet of Things (IoT) devices interact\nfor energy management in the smart grid. Although there have been many works on\nMG energy management, most studies assume a perfect communication environment,\nwhere communication failures are not considered. In this paper, we consider the\nMG as a multi-agent environment with IoT devices in which AI agents exchange\ninformation with their peers for collaboration. However, the collaboration\ninformation may be lost due to communication failures or packet loss. Such\nevents may affect the operation of the whole MG. To this end, we propose a\nmulti-agent Bayesian deep reinforcement learning (BA-DRL) method for MG energy\nmanagement under communication failures. We first define a multi-agent\npartially observable Markov decision process (MA-POMDP) to describe agents\nunder communication failures, in which each agent can update its beliefs on the\nactions of its peers. Then, we apply a double deep Q-learning (DDQN)\narchitecture for Q-value estimation in BA-DRL, and propose a belief-based\ncorrelated equilibrium for the joint-action selection of multi-agent BA-DRL.\nFinally, the simulation results show that BA-DRL is robust to both power supply\nuncertainty and communication failure uncertainty. BA-DRL has 4.1% and 10.3%\nhigher reward than Nash Deep Q-learning (Nash-DQN) and alternating direction\nmethod of multipliers (ADMM) respectively under 1% communication failure\nprobability.",
    "text": "Multi-agent Bayesian Deep Reinforcement Learning for Microgrid Energy\n  Management under Communication Failures\n\nMicrogrids (MGs) are important players for the future transactive energy\nsystems where a number of intelligent Internet of Things (IoT) devices interact\nfor energy management in the smart grid. Although there have been many works on\nMG energy management, most studies assume a perfect communication environment,\nwhere communication failures are not considered. In this paper, we consider the\nMG as a multi-agent environment with IoT devices in which AI agents exchange\ninformation with their peers for collaboration. However, the collaboration\ninformation may be lost due to communication failures or packet loss. Such\nevents may affect the operation of the whole MG. To this end, we propose a\nmulti-agent Bayesian deep reinforcement learning (BA-DRL) method for MG energy\nmanagement under communication failures. We first define a multi-agent\npartially observable Markov decision process (MA-POMDP) to describe agents\nunder communication failures, in which each agent can update its beliefs on the\nactions of its peers. Then, we apply a double deep Q-learning (DDQN)\narchitecture for Q-value estimation in BA-DRL, and propose a belief-based\ncorrelated equilibrium for the joint-action selection of multi-agent BA-DRL.\nFinally, the simulation results show that BA-DRL is robust to both power supply\nuncertainty and communication failure uncertainty. BA-DRL has 4.1% and 10.3%\nhigher reward than Nash Deep Q-learning (Nash-DQN) and alternating direction\nmethod of multipliers (ADMM) respectively under 1% communication failure\nprobability."
  },
  {
    "id": "arxiv-151",
    "title": "Optimizing Bayesian Recurrent Neural Networks on an FPGA-based\n  Accelerator",
    "abstract": "Neural networks have demonstrated their outstanding performance in a wide\nrange of tasks. Specifically recurrent architectures based on long-short term\nmemory (LSTM) cells have manifested excellent capability to model time\ndependencies in real-world data. However, standard recurrent architectures\ncannot estimate their uncertainty which is essential for safety-critical\napplications such as in medicine. In contrast, Bayesian recurrent neural\nnetworks (RNNs) are able to provide uncertainty estimation with improved\naccuracy. Nonetheless, Bayesian RNNs are computationally and memory demanding,\nwhich limits their practicality despite their advantages. To address this\nissue, we propose an FPGA-based hardware design to accelerate Bayesian\nLSTM-based RNNs. To further improve the overall algorithmic-hardware\nperformance, a co-design framework is proposed to explore the most fitting\nalgorithmic-hardware configurations for Bayesian RNNs. We conduct extensive\nexperiments on healthcare applications to demonstrate the improvement of our\ndesign and the effectiveness of our framework. Compared with GPU\nimplementation, our FPGA-based design can achieve up to 10 times speedup with\nnearly 106 times higher energy efficiency. To the best of our knowledge, this\nis the first work targeting acceleration of Bayesian RNNs on FPGAs.",
    "text": "Optimizing Bayesian Recurrent Neural Networks on an FPGA-based\n  Accelerator\n\nNeural networks have demonstrated their outstanding performance in a wide\nrange of tasks. Specifically recurrent architectures based on long-short term\nmemory (LSTM) cells have manifested excellent capability to model time\ndependencies in real-world data. However, standard recurrent architectures\ncannot estimate their uncertainty which is essential for safety-critical\napplications such as in medicine. In contrast, Bayesian recurrent neural\nnetworks (RNNs) are able to provide uncertainty estimation with improved\naccuracy. Nonetheless, Bayesian RNNs are computationally and memory demanding,\nwhich limits their practicality despite their advantages. To address this\nissue, we propose an FPGA-based hardware design to accelerate Bayesian\nLSTM-based RNNs. To further improve the overall algorithmic-hardware\nperformance, a co-design framework is proposed to explore the most fitting\nalgorithmic-hardware configurations for Bayesian RNNs. We conduct extensive\nexperiments on healthcare applications to demonstrate the improvement of our\ndesign and the effectiveness of our framework. Compared with GPU\nimplementation, our FPGA-based design can achieve up to 10 times speedup with\nnearly 106 times higher energy efficiency. To the best of our knowledge, this\nis the first work targeting acceleration of Bayesian RNNs on FPGAs."
  },
  {
    "id": "arxiv-152",
    "title": "A Base Camp for Scaling AI",
    "abstract": "Modern statistical machine learning (SML) methods share a major limitation\nwith the early approaches to AI: there is no scalable way to adapt them to new\ndomains. Human learning solves this in part by leveraging a rich, shared,\nupdateable world model. Such scalability requires modularity: updating part of\nthe world model should not impact unrelated parts. We have argued that such\nmodularity will require both \"correctability\" (so that errors can be corrected\nwithout introducing new errors) and \"interpretability\" (so that we can\nunderstand what components need correcting).\n  To achieve this, one could attempt to adapt state of the art SML systems to\nbe interpretable and correctable; or one could see how far the simplest\npossible interpretable, correctable learning methods can take us, and try to\ncontrol the limitations of SML methods by applying them only where needed. Here\nwe focus on the latter approach and we investigate two main ideas: \"Teacher\nAssisted Learning\", which leverages crowd sourcing to learn language; and\n\"Factored Dialog Learning\", which factors the process of application\ndevelopment into roles where the language competencies needed are isolated,\nenabling non-experts to quickly create new applications.\n  We test these ideas in an \"Automated Personal Assistant\" (APA) setting, with\ntwo scenarios: that of detecting user intent from a user-APA dialog; and that\nof creating a class of event reminder applications, where a non-expert\n\"teacher\" can then create specific apps. For the intent detection task, we use\na dataset of a thousand labeled utterances from user dialogs with Cortana, and\nwe show that our approach matches state of the art SML methods, but in addition\nprovides full transparency: the whole (editable) model can be summarized on one\nhuman-readable page. For the reminder app task, we ran small user studies to\nverify the efficacy of the approach.",
    "text": "A Base Camp for Scaling AI\n\nModern statistical machine learning (SML) methods share a major limitation\nwith the early approaches to AI: there is no scalable way to adapt them to new\ndomains. Human learning solves this in part by leveraging a rich, shared,\nupdateable world model. Such scalability requires modularity: updating part of\nthe world model should not impact unrelated parts. We have argued that such\nmodularity will require both \"correctability\" (so that errors can be corrected\nwithout introducing new errors) and \"interpretability\" (so that we can\nunderstand what components need correcting).\n  To achieve this, one could attempt to adapt state of the art SML systems to\nbe interpretable and correctable; or one could see how far the simplest\npossible interpretable, correctable learning methods can take us, and try to\ncontrol the limitations of SML methods by applying them only where needed. Here\nwe focus on the latter approach and we investigate two main ideas: \"Teacher\nAssisted Learning\", which leverages crowd sourcing to learn language; and\n\"Factored Dialog Learning\", which factors the process of application\ndevelopment into roles where the language competencies needed are isolated,\nenabling non-experts to quickly create new applications.\n  We test these ideas in an \"Automated Personal Assistant\" (APA) setting, with\ntwo scenarios: that of detecting user intent from a user-APA dialog; and that\nof creating a class of event reminder applications, where a non-expert\n\"teacher\" can then create specific apps. For the intent detection task, we use\na dataset of a thousand labeled utterances from user dialogs with Cortana, and\nwe show that our approach matches state of the art SML methods, but in addition\nprovides full transparency: the whole (editable) model can be summarized on one\nhuman-readable page. For the reminder app task, we ran small user studies to\nverify the efficacy of the approach."
  },
  {
    "id": "arxiv-153",
    "title": "Multichannel Sound Event Detection Using 3D Convolutional Neural\n  Networks for Learning Inter-channel Features",
    "abstract": "In this paper, we propose a stacked convolutional and recurrent neural\nnetwork (CRNN) with a 3D convolutional neural network (CNN) in the first layer\nfor the multichannel sound event detection (SED) task. The 3D CNN enables the\nnetwork to simultaneously learn the inter- and intra-channel features from the\ninput multichannel audio. In order to evaluate the proposed method,\nmultichannel audio datasets with different number of overlapping sound sources\nare synthesized. Each of this dataset has a four-channel first-order Ambisonic,\nbinaural, and single-channel versions, on which the performance of SED using\nthe proposed method are compared to study the potential of SED using\nmultichannel audio. A similar study is also done with the binaural and\nsingle-channel versions of the real-life recording TUT-SED 2017 development\ndataset. The proposed method learns to recognize overlapping sound events from\nmultichannel features faster and performs better SED with a fewer number of\ntraining epochs. The results show that on using multichannel Ambisonic audio in\nplace of single-channel audio we improve the overall F-score by 7.5%, overall\nerror rate by 10% and recognize 15.6% more sound events in time frames with\nfour overlapping sound sources.",
    "text": "Multichannel Sound Event Detection Using 3D Convolutional Neural\n  Networks for Learning Inter-channel Features\n\nIn this paper, we propose a stacked convolutional and recurrent neural\nnetwork (CRNN) with a 3D convolutional neural network (CNN) in the first layer\nfor the multichannel sound event detection (SED) task. The 3D CNN enables the\nnetwork to simultaneously learn the inter- and intra-channel features from the\ninput multichannel audio. In order to evaluate the proposed method,\nmultichannel audio datasets with different number of overlapping sound sources\nare synthesized. Each of this dataset has a four-channel first-order Ambisonic,\nbinaural, and single-channel versions, on which the performance of SED using\nthe proposed method are compared to study the potential of SED using\nmultichannel audio. A similar study is also done with the binaural and\nsingle-channel versions of the real-life recording TUT-SED 2017 development\ndataset. The proposed method learns to recognize overlapping sound events from\nmultichannel features faster and performs better SED with a fewer number of\ntraining epochs. The results show that on using multichannel Ambisonic audio in\nplace of single-channel audio we improve the overall F-score by 7.5%, overall\nerror rate by 10% and recognize 15.6% more sound events in time frames with\nfour overlapping sound sources."
  },
  {
    "id": "arxiv-154",
    "title": "An Asymptotically Optimal Algorithm for Communicating Multiplayer\n  Multi-Armed Bandit Problems",
    "abstract": "We consider a decentralized stochastic multi-armed bandit problem with\nmultiple players. Each player aims to maximize his/her own reward by pulling an\narm. The arms give rewards based on i.i.d. stochastic Bernoulli distributions.\nPlayers are not aware about the probability distributions of the arms. At the\nend of each turn, the players inform their neighbors about the arm he/she\npulled and the reward he/she got. Neighbors of players are determined according\nto an Erd{\\H{o}}s-R{\\'e}nyi graph with connectivity $\\alpha$. This graph is\nreproduced in the beginning of every turn with the same connectivity. When more\nthan one player choose the same arm in a turn, we assume that only one of the\nplayers who is randomly chosen gets the reward where the others get nothing. We\nfirst start by assuming players are not aware of the collision model and offer\nan asymptotically optimal algorithm for $\\alpha = 1$ case. Then, we extend our\nprior work and offer an asymptotically optimal algorithm for any connectivity\nbut zero, assuming players aware of the collision model. We also study the\neffect of $\\alpha$, the degree of communication between players, empirically on\nthe cumulative regret by comparing them with traditional multi-armed bandit\nalgorithms.",
    "text": "An Asymptotically Optimal Algorithm for Communicating Multiplayer\n  Multi-Armed Bandit Problems\n\nWe consider a decentralized stochastic multi-armed bandit problem with\nmultiple players. Each player aims to maximize his/her own reward by pulling an\narm. The arms give rewards based on i.i.d. stochastic Bernoulli distributions.\nPlayers are not aware about the probability distributions of the arms. At the\nend of each turn, the players inform their neighbors about the arm he/she\npulled and the reward he/she got. Neighbors of players are determined according\nto an Erd{\\H{o}}s-R{\\'e}nyi graph with connectivity $\\alpha$. This graph is\nreproduced in the beginning of every turn with the same connectivity. When more\nthan one player choose the same arm in a turn, we assume that only one of the\nplayers who is randomly chosen gets the reward where the others get nothing. We\nfirst start by assuming players are not aware of the collision model and offer\nan asymptotically optimal algorithm for $\\alpha = 1$ case. Then, we extend our\nprior work and offer an asymptotically optimal algorithm for any connectivity\nbut zero, assuming players aware of the collision model. We also study the\neffect of $\\alpha$, the degree of communication between players, empirically on\nthe cumulative regret by comparing them with traditional multi-armed bandit\nalgorithms."
  },
  {
    "id": "arxiv-155",
    "title": "Robust Estimation of Tree Structured Ising Models",
    "abstract": "We consider the task of learning Ising models when the signs of different\nrandom variables are flipped independently with possibly unequal, unknown\nprobabilities. In this paper, we focus on the problem of robust estimation of\ntree-structured Ising models. Without any additional assumption of side\ninformation, this is an open problem. We first prove that this problem is\nunidentifiable, however, this unidentifiability is limited to a small\nequivalence class of trees formed by leaf nodes exchanging positions with their\nneighbors. Next, we propose an algorithm to solve the above problem with\nlogarithmic sample complexity in the number of nodes and polynomial run-time\ncomplexity. Lastly, we empirically demonstrate that, as expected, existing\nalgorithms are not inherently robust in the proposed setting whereas our\nalgorithm correctly recovers the underlying equivalence class.",
    "text": "Robust Estimation of Tree Structured Ising Models\n\nWe consider the task of learning Ising models when the signs of different\nrandom variables are flipped independently with possibly unequal, unknown\nprobabilities. In this paper, we focus on the problem of robust estimation of\ntree-structured Ising models. Without any additional assumption of side\ninformation, this is an open problem. We first prove that this problem is\nunidentifiable, however, this unidentifiability is limited to a small\nequivalence class of trees formed by leaf nodes exchanging positions with their\nneighbors. Next, we propose an algorithm to solve the above problem with\nlogarithmic sample complexity in the number of nodes and polynomial run-time\ncomplexity. Lastly, we empirically demonstrate that, as expected, existing\nalgorithms are not inherently robust in the proposed setting whereas our\nalgorithm correctly recovers the underlying equivalence class."
  },
  {
    "id": "arxiv-156",
    "title": "A Variational U-Net for Weather Forecasting",
    "abstract": "Not only can discovering patterns and insights from atmospheric data enable\nmore accurate weather predictions, but it may also provide valuable information\nto help tackle climate change. Weather4cast is an open competition that aims to\nevaluate machine learning algorithms' capability to predict future atmospheric\nstates. Here, we describe our third-place solution to Weather4cast. We present\na novel Variational U-Net that combines a Variational Autoencoder's ability to\nconsider the probabilistic nature of data with a U-Net's ability to recover\nfine-grained details. This solution is an evolution from our fourth-place\nsolution to Traffic4cast 2020 with many commonalities, suggesting its\napplicability to vastly different domains, such as weather and traffic.",
    "text": "A Variational U-Net for Weather Forecasting\n\nNot only can discovering patterns and insights from atmospheric data enable\nmore accurate weather predictions, but it may also provide valuable information\nto help tackle climate change. Weather4cast is an open competition that aims to\nevaluate machine learning algorithms' capability to predict future atmospheric\nstates. Here, we describe our third-place solution to Weather4cast. We present\na novel Variational U-Net that combines a Variational Autoencoder's ability to\nconsider the probabilistic nature of data with a U-Net's ability to recover\nfine-grained details. This solution is an evolution from our fourth-place\nsolution to Traffic4cast 2020 with many commonalities, suggesting its\napplicability to vastly different domains, such as weather and traffic."
  },
  {
    "id": "arxiv-157",
    "title": "A Domain-Shrinking based Bayesian Optimization Algorithm with\n  Order-Optimal Regret Performance",
    "abstract": "We consider sequential optimization of an unknown function in a reproducing\nkernel Hilbert space. We propose a Gaussian process-based algorithm and\nestablish its order-optimal regret performance (up to a poly-logarithmic\nfactor). This is the first GP-based algorithm with an order-optimal regret\nguarantee. The proposed algorithm is rooted in the methodology of domain\nshrinking realized through a sequence of tree-based region pruning and refining\nto concentrate queries in increasingly smaller high-performing regions of the\nfunction domain. The search for high-performing regions is localized and guided\nby an iterative estimation of the optimal function value to ensure both\nlearning efficiency and computational efficiency. Compared with the prevailing\nGP-UCB family of algorithms, the proposed algorithm reduces computational\ncomplexity by a factor of $O(T^{2d-1})$ (where $T$ is the time horizon and $d$\nthe dimension of the function domain).",
    "text": "A Domain-Shrinking based Bayesian Optimization Algorithm with\n  Order-Optimal Regret Performance\n\nWe consider sequential optimization of an unknown function in a reproducing\nkernel Hilbert space. We propose a Gaussian process-based algorithm and\nestablish its order-optimal regret performance (up to a poly-logarithmic\nfactor). This is the first GP-based algorithm with an order-optimal regret\nguarantee. The proposed algorithm is rooted in the methodology of domain\nshrinking realized through a sequence of tree-based region pruning and refining\nto concentrate queries in increasingly smaller high-performing regions of the\nfunction domain. The search for high-performing regions is localized and guided\nby an iterative estimation of the optimal function value to ensure both\nlearning efficiency and computational efficiency. Compared with the prevailing\nGP-UCB family of algorithms, the proposed algorithm reduces computational\ncomplexity by a factor of $O(T^{2d-1})$ (where $T$ is the time horizon and $d$\nthe dimension of the function domain)."
  },
  {
    "id": "arxiv-158",
    "title": "Bootstrapping and Multiple Imputation Ensemble Approaches for Missing\n  Data",
    "abstract": "Presence of missing values in a dataset can adversely affect the performance\nof a classifier. Single and Multiple Imputation are normally performed to fill\nin the missing values. In this paper, we present several variants of combining\nsingle and multiple imputation with bootstrapping to create ensembles that can\nmodel uncertainty and diversity in the data, and that are robust to high\nmissingness in the data. We present three ensemble strategies: bootstrapping on\nincomplete data followed by (i) single imputation and (ii) multiple imputation,\nand (iii) multiple imputation ensemble without bootstrapping. We perform an\nextensive evaluation of the performance of the these ensemble strategies on 8\ndatasets by varying the missingness ratio. Our results show that bootstrapping\nfollowed by multiple imputation using expectation maximization is the most\nrobust method even at high missingness ratio (up to 30%). For small missingness\nratio (up to 10%) most of the ensemble methods perform quivalently but better\nthan single imputation. Kappa-error plots suggest that accurate classifiers\nwith reasonable diversity is the reason for this behaviour. A consistent\nobservation in all the datasets suggests that for small missingness (up to\n10%), bootstrapping on incomplete data without any imputation produces\nequivalent results to other ensemble methods.",
    "text": "Bootstrapping and Multiple Imputation Ensemble Approaches for Missing\n  Data\n\nPresence of missing values in a dataset can adversely affect the performance\nof a classifier. Single and Multiple Imputation are normally performed to fill\nin the missing values. In this paper, we present several variants of combining\nsingle and multiple imputation with bootstrapping to create ensembles that can\nmodel uncertainty and diversity in the data, and that are robust to high\nmissingness in the data. We present three ensemble strategies: bootstrapping on\nincomplete data followed by (i) single imputation and (ii) multiple imputation,\nand (iii) multiple imputation ensemble without bootstrapping. We perform an\nextensive evaluation of the performance of the these ensemble strategies on 8\ndatasets by varying the missingness ratio. Our results show that bootstrapping\nfollowed by multiple imputation using expectation maximization is the most\nrobust method even at high missingness ratio (up to 30%). For small missingness\nratio (up to 10%) most of the ensemble methods perform quivalently but better\nthan single imputation. Kappa-error plots suggest that accurate classifiers\nwith reasonable diversity is the reason for this behaviour. A consistent\nobservation in all the datasets suggests that for small missingness (up to\n10%), bootstrapping on incomplete data without any imputation produces\nequivalent results to other ensemble methods."
  },
  {
    "id": "arxiv-159",
    "title": "A Framework for Multi-View Classification of Features",
    "abstract": "One of the most important problems in the field of pattern recognition is\ndata classification. Due to the increasing development of technologies\nintroduced in the field of data classification, some of the solutions are still\nopen and need more research. One of the challenging problems in this area is\nthe curse of dimensionality of the feature set of the data classification\nproblem. In solving the data classification problems, when the feature set is\ntoo large, typical approaches will not be able to solve the problem. In this\ncase, an approach can be used to partition the feature set into multiple\nfeature sub-sets so that the data classification problem is solved for each of\nthe feature subsets and finally using the ensemble classification, the\nclassification is applied to the entire feature set. In the above-mentioned\napproach, the partitioning of feature set into feature sub-sets is still an\ninteresting area in the literature of this field. In this research, an\ninnovative framework for multi-view ensemble classification, inspired by the\nproblem of object recognition in the multiple views theory of humans, is\nproposed. In this method, at first, the collaboration values between the\nfeatures is calculated using a criterion called the features collaboration\ncriterion. Then, the collaboration graph is formed based on the calculated\ncollaboration values. In the next step, using the community detection method,\ngraph communities are found. The communities are considered as the problem\nviews and the different base classifiers are trained for different views using\nthe views corresponding training data. The multi-view ensemble classifier is\nthen formed by a combination of base classifiers based on the AdaBoost\nalgorithm. The simulation results of the proposed method on the real and\nsynthetic datasets show that the proposed method increases the classification\naccuracy.",
    "text": "A Framework for Multi-View Classification of Features\n\nOne of the most important problems in the field of pattern recognition is\ndata classification. Due to the increasing development of technologies\nintroduced in the field of data classification, some of the solutions are still\nopen and need more research. One of the challenging problems in this area is\nthe curse of dimensionality of the feature set of the data classification\nproblem. In solving the data classification problems, when the feature set is\ntoo large, typical approaches will not be able to solve the problem. In this\ncase, an approach can be used to partition the feature set into multiple\nfeature sub-sets so that the data classification problem is solved for each of\nthe feature subsets and finally using the ensemble classification, the\nclassification is applied to the entire feature set. In the above-mentioned\napproach, the partitioning of feature set into feature sub-sets is still an\ninteresting area in the literature of this field. In this research, an\ninnovative framework for multi-view ensemble classification, inspired by the\nproblem of object recognition in the multiple views theory of humans, is\nproposed. In this method, at first, the collaboration values between the\nfeatures is calculated using a criterion called the features collaboration\ncriterion. Then, the collaboration graph is formed based on the calculated\ncollaboration values. In the next step, using the community detection method,\ngraph communities are found. The communities are considered as the problem\nviews and the different base classifiers are trained for different views using\nthe views corresponding training data. The multi-view ensemble classifier is\nthen formed by a combination of base classifiers based on the AdaBoost\nalgorithm. The simulation results of the proposed method on the real and\nsynthetic datasets show that the proposed method increases the classification\naccuracy."
  },
  {
    "id": "arxiv-160",
    "title": "Approximately Solving Mean Field Games via Entropy-Regularized Deep\n  Reinforcement Learning",
    "abstract": "The recent mean field game (MFG) formalism facilitates otherwise intractable\ncomputation of approximate Nash equilibria in many-agent settings. In this\npaper, we consider discrete-time finite MFGs subject to finite-horizon\nobjectives. We show that all discrete-time finite MFGs with non-constant fixed\npoint operators fail to be contractive as typically assumed in existing MFG\nliterature, barring convergence via fixed point iteration. Instead, we\nincorporate entropy-regularization and Boltzmann policies into the fixed point\niteration. As a result, we obtain provable convergence to approximate fixed\npoints where existing methods fail, and reach the original goal of approximate\nNash equilibria. All proposed methods are evaluated with respect to their\nexploitability, on both instructive examples with tractable exact solutions and\nhigh-dimensional problems where exact methods become intractable. In\nhigh-dimensional scenarios, we apply established deep reinforcement learning\nmethods and empirically combine fictitious play with our approximations.",
    "text": "Approximately Solving Mean Field Games via Entropy-Regularized Deep\n  Reinforcement Learning\n\nThe recent mean field game (MFG) formalism facilitates otherwise intractable\ncomputation of approximate Nash equilibria in many-agent settings. In this\npaper, we consider discrete-time finite MFGs subject to finite-horizon\nobjectives. We show that all discrete-time finite MFGs with non-constant fixed\npoint operators fail to be contractive as typically assumed in existing MFG\nliterature, barring convergence via fixed point iteration. Instead, we\nincorporate entropy-regularization and Boltzmann policies into the fixed point\niteration. As a result, we obtain provable convergence to approximate fixed\npoints where existing methods fail, and reach the original goal of approximate\nNash equilibria. All proposed methods are evaluated with respect to their\nexploitability, on both instructive examples with tractable exact solutions and\nhigh-dimensional problems where exact methods become intractable. In\nhigh-dimensional scenarios, we apply established deep reinforcement learning\nmethods and empirically combine fictitious play with our approximations."
  },
  {
    "id": "arxiv-161",
    "title": "A Log-Linear Time Sequential Optimal Calibration Algorithm for Quantized Isotonic L2 Regression",
    "abstract": "We study the sequential calibration of estimations in a quantized isotonic L2\nregression setting. We start by showing that the optimal calibrated quantized\nestimations can be acquired from the traditional isotonic L2 regression\nsolution. We modify the traditional PAVA algorithm to create calibrators for\nboth batch and sequential optimization of the quantized isotonic regression\nproblem. Our algorithm can update the optimal quantized monotone mapping for\nthe samples observed so far in linear space and logarithmic time per new\nunordered sample.",
    "text": "A Log-Linear Time Sequential Optimal Calibration Algorithm for Quantized Isotonic L2 Regression\n\nWe study the sequential calibration of estimations in a quantized isotonic L2\nregression setting. We start by showing that the optimal calibrated quantized\nestimations can be acquired from the traditional isotonic L2 regression\nsolution. We modify the traditional PAVA algorithm to create calibrators for\nboth batch and sequential optimization of the quantized isotonic regression\nproblem. Our algorithm can update the optimal quantized monotone mapping for\nthe samples observed so far in linear space and logarithmic time per new\nunordered sample."
  },
  {
    "id": "arxiv-162",
    "title": "High-Dimensional Bayesian Optimisation with Variational Autoencoders and\n  Deep Metric Learning",
    "abstract": "We introduce a method combining variational autoencoders (VAEs) and deep\nmetric learning to perform Bayesian optimisation (BO) over high-dimensional and\nstructured input spaces. By adapting ideas from deep metric learning, we use\nlabel guidance from the blackbox function to structure the VAE latent space,\nfacilitating the Gaussian process fit and yielding improved BO performance.\nImportantly for BO problem settings, our method operates in semi-supervised\nregimes where only few labelled data points are available. We run experiments\non three real-world tasks, achieving state-of-the-art results on the penalised\nlogP molecule generation benchmark using just 3% of the labelled data required\nby previous approaches. As a theoretical contribution, we present a proof of\nvanishing regret for VAE BO.",
    "text": "High-Dimensional Bayesian Optimisation with Variational Autoencoders and\n  Deep Metric Learning\n\nWe introduce a method combining variational autoencoders (VAEs) and deep\nmetric learning to perform Bayesian optimisation (BO) over high-dimensional and\nstructured input spaces. By adapting ideas from deep metric learning, we use\nlabel guidance from the blackbox function to structure the VAE latent space,\nfacilitating the Gaussian process fit and yielding improved BO performance.\nImportantly for BO problem settings, our method operates in semi-supervised\nregimes where only few labelled data points are available. We run experiments\non three real-world tasks, achieving state-of-the-art results on the penalised\nlogP molecule generation benchmark using just 3% of the labelled data required\nby previous approaches. As a theoretical contribution, we present a proof of\nvanishing regret for VAE BO."
  },
  {
    "id": "arxiv-163",
    "title": "Towards High Performance Java-based Deep Learning Frameworks",
    "abstract": "The advent of modern cloud services along with the huge volume of data\nproduced on a daily basis, have set the demand for fast and efficient data\nprocessing. This demand is common among numerous application domains, such as\ndeep learning, data mining, and computer vision. Prior research has focused on\nemploying hardware accelerators as a means to overcome this inefficiency. This\ntrend has driven software development to target heterogeneous execution, and\nseveral modern computing systems have incorporated a mixture of diverse\ncomputing components, including GPUs and FPGAs. However, the specialization of\nthe applications' code for heterogeneous execution is not a trivial task, as it\nrequires developers to have hardware expertise in order to obtain high\nperformance. The vast majority of the existing deep learning frameworks that\nsupport heterogeneous acceleration, rely on the implementation of wrapper calls\nfrom a high-level programming language to a low-level accelerator backend, such\nas OpenCL, CUDA or HLS.\n  In this paper we have employed TornadoVM, a state-of-the-art heterogeneous\nprogramming framework to transparently accelerate Deep Netts; a Java-based deep\nlearning framework. Our initial results demonstrate up to 8x performance\nspeedup when executing the back propagation process of the network's training\non AMD GPUs against the sequential execution of the original Deep Netts\nframework.",
    "text": "Towards High Performance Java-based Deep Learning Frameworks\n\nThe advent of modern cloud services along with the huge volume of data\nproduced on a daily basis, have set the demand for fast and efficient data\nprocessing. This demand is common among numerous application domains, such as\ndeep learning, data mining, and computer vision. Prior research has focused on\nemploying hardware accelerators as a means to overcome this inefficiency. This\ntrend has driven software development to target heterogeneous execution, and\nseveral modern computing systems have incorporated a mixture of diverse\ncomputing components, including GPUs and FPGAs. However, the specialization of\nthe applications' code for heterogeneous execution is not a trivial task, as it\nrequires developers to have hardware expertise in order to obtain high\nperformance. The vast majority of the existing deep learning frameworks that\nsupport heterogeneous acceleration, rely on the implementation of wrapper calls\nfrom a high-level programming language to a low-level accelerator backend, such\nas OpenCL, CUDA or HLS.\n  In this paper we have employed TornadoVM, a state-of-the-art heterogeneous\nprogramming framework to transparently accelerate Deep Netts; a Java-based deep\nlearning framework. Our initial results demonstrate up to 8x performance\nspeedup when executing the back propagation process of the network's training\non AMD GPUs against the sequential execution of the original Deep Netts\nframework."
  },
  {
    "id": "arxiv-164",
    "title": "Incomplete Multi-view Clustering via Graph Regularized Matrix\n  Factorization",
    "abstract": "Clustering with incomplete views is a challenge in multi-view clustering. In\nthis paper, we provide a novel and simple method to address this issue.\nSpecifically, the proposed method simultaneously exploits the local information\nof each view and the complementary information among views to learn the common\nlatent representation for all samples, which can greatly improve the\ncompactness and discriminability of the obtained representation. Compared with\nthe conventional graph embedding methods, the proposed method does not\nintroduce any extra regularization term and corresponding penalty parameter to\npreserve the local structure of data, and thus does not increase the burden of\nextra parameter selection. By imposing the orthogonal constraint on the basis\nmatrix of each view, the proposed method is able to handle the out-of-sample.\nMoreover, the proposed method can be viewed as a unified framework for\nmulti-view learning since it can handle both incomplete and complete multi-view\nclustering and classification tasks. Extensive experiments conducted on several\nmulti-view datasets prove that the proposed method can significantly improve\nthe clustering performance.",
    "text": "Incomplete Multi-view Clustering via Graph Regularized Matrix\n  Factorization\n\nClustering with incomplete views is a challenge in multi-view clustering. In\nthis paper, we provide a novel and simple method to address this issue.\nSpecifically, the proposed method simultaneously exploits the local information\nof each view and the complementary information among views to learn the common\nlatent representation for all samples, which can greatly improve the\ncompactness and discriminability of the obtained representation. Compared with\nthe conventional graph embedding methods, the proposed method does not\nintroduce any extra regularization term and corresponding penalty parameter to\npreserve the local structure of data, and thus does not increase the burden of\nextra parameter selection. By imposing the orthogonal constraint on the basis\nmatrix of each view, the proposed method is able to handle the out-of-sample.\nMoreover, the proposed method can be viewed as a unified framework for\nmulti-view learning since it can handle both incomplete and complete multi-view\nclustering and classification tasks. Extensive experiments conducted on several\nmulti-view datasets prove that the proposed method can significantly improve\nthe clustering performance."
  },
  {
    "id": "arxiv-165",
    "title": "RGPNet: A Real-Time General Purpose Semantic Segmentation",
    "abstract": "We propose a real-time general purpose semantic segmentation architecture,\nRGPNet, which achieves significant performance gain in complex environments.\nRGPNet consists of a light-weight asymmetric encoder-decoder and an adaptor.\nThe adaptor helps preserve and refine the abstract concepts from multiple\nlevels of distributed representations between the encoder and decoder. It also\nfacilitates the gradient flow from deeper layers to shallower layers. Our\nexperiments demonstrate that RGPNet can generate segmentation results in\nreal-time with comparable accuracy to the state-of-the-art non-real-time heavy\nmodels. Moreover, towards green AI, we show that using an optimized\nlabel-relaxation technique with progressive resizing can reduce the training\ntime by up to 60% while preserving the performance. We conclude that RGPNet\nobtains a better speed-accuracy trade-off across multiple datasets.",
    "text": "RGPNet: A Real-Time General Purpose Semantic Segmentation\n\nWe propose a real-time general purpose semantic segmentation architecture,\nRGPNet, which achieves significant performance gain in complex environments.\nRGPNet consists of a light-weight asymmetric encoder-decoder and an adaptor.\nThe adaptor helps preserve and refine the abstract concepts from multiple\nlevels of distributed representations between the encoder and decoder. It also\nfacilitates the gradient flow from deeper layers to shallower layers. Our\nexperiments demonstrate that RGPNet can generate segmentation results in\nreal-time with comparable accuracy to the state-of-the-art non-real-time heavy\nmodels. Moreover, towards green AI, we show that using an optimized\nlabel-relaxation technique with progressive resizing can reduce the training\ntime by up to 60% while preserving the performance. We conclude that RGPNet\nobtains a better speed-accuracy trade-off across multiple datasets."
  },
  {
    "id": "arxiv-166",
    "title": "Spatial Context-Aware Self-Attention Model For Multi-Organ Segmentation",
    "abstract": "Multi-organ segmentation is one of most successful applications of deep\nlearning in medical image analysis. Deep convolutional neural nets (CNNs) have\nshown great promise in achieving clinically applicable image segmentation\nperformance on CT or MRI images. State-of-the-art CNN segmentation models apply\neither 2D or 3D convolutions on input images, with pros and cons associated\nwith each method: 2D convolution is fast, less memory-intensive but inadequate\nfor extracting 3D contextual information from volumetric images, while the\nopposite is true for 3D convolution. To fit a 3D CNN model on CT or MRI images\non commodity GPUs, one usually has to either downsample input images or use\ncropped local regions as inputs, which limits the utility of 3D models for\nmulti-organ segmentation. In this work, we propose a new framework for\ncombining 3D and 2D models, in which the segmentation is realized through\nhigh-resolution 2D convolutions, but guided by spatial contextual information\nextracted from a low-resolution 3D model. We implement a self-attention\nmechanism to control which 3D features should be used to guide 2D segmentation.\nOur model is light on memory usage but fully equipped to take 3D contextual\ninformation into account. Experiments on multiple organ segmentation datasets\ndemonstrate that by taking advantage of both 2D and 3D models, our method is\nconsistently outperforms existing 2D and 3D models in organ segmentation\naccuracy, while being able to directly take raw whole-volume image data as\ninputs.",
    "text": "Spatial Context-Aware Self-Attention Model For Multi-Organ Segmentation\n\nMulti-organ segmentation is one of most successful applications of deep\nlearning in medical image analysis. Deep convolutional neural nets (CNNs) have\nshown great promise in achieving clinically applicable image segmentation\nperformance on CT or MRI images. State-of-the-art CNN segmentation models apply\neither 2D or 3D convolutions on input images, with pros and cons associated\nwith each method: 2D convolution is fast, less memory-intensive but inadequate\nfor extracting 3D contextual information from volumetric images, while the\nopposite is true for 3D convolution. To fit a 3D CNN model on CT or MRI images\non commodity GPUs, one usually has to either downsample input images or use\ncropped local regions as inputs, which limits the utility of 3D models for\nmulti-organ segmentation. In this work, we propose a new framework for\ncombining 3D and 2D models, in which the segmentation is realized through\nhigh-resolution 2D convolutions, but guided by spatial contextual information\nextracted from a low-resolution 3D model. We implement a self-attention\nmechanism to control which 3D features should be used to guide 2D segmentation.\nOur model is light on memory usage but fully equipped to take 3D contextual\ninformation into account. Experiments on multiple organ segmentation datasets\ndemonstrate that by taking advantage of both 2D and 3D models, our method is\nconsistently outperforms existing 2D and 3D models in organ segmentation\naccuracy, while being able to directly take raw whole-volume image data as\ninputs."
  },
  {
    "id": "arxiv-167",
    "title": "Interpreting Deep Learning-Based Networking Systems",
    "abstract": "While many deep learning (DL)-based networking systems have demonstrated\nsuperior performance, the underlying Deep Neural Networks (DNNs) remain\nblackboxes and stay uninterpretable for network operators. The lack of\ninterpretability makes DL-based networking systems prohibitive to deploy in\npractice. In this paper, we propose Metis, a framework that provides\ninterpretability for two general categories of networking problems spanning\nlocal and global control. Accordingly, Metis introduces two different\ninterpretation methods based on decision tree and hypergraph, where it converts\nDNN policies to interpretable rule-based controllers and highlight critical\ncomponents based on analysis over hypergraph. We evaluate Metis over several\nstate-of-the-art DL-based networking systems and show that Metis provides\nhuman-readable interpretations while preserving nearly no degradation in\nperformance. We further present four concrete use cases of Metis, showcasing\nhow Metis helps network operators to design, debug, deploy, and ad-hoc adjust\nDL-based networking systems.",
    "text": "Interpreting Deep Learning-Based Networking Systems\n\nWhile many deep learning (DL)-based networking systems have demonstrated\nsuperior performance, the underlying Deep Neural Networks (DNNs) remain\nblackboxes and stay uninterpretable for network operators. The lack of\ninterpretability makes DL-based networking systems prohibitive to deploy in\npractice. In this paper, we propose Metis, a framework that provides\ninterpretability for two general categories of networking problems spanning\nlocal and global control. Accordingly, Metis introduces two different\ninterpretation methods based on decision tree and hypergraph, where it converts\nDNN policies to interpretable rule-based controllers and highlight critical\ncomponents based on analysis over hypergraph. We evaluate Metis over several\nstate-of-the-art DL-based networking systems and show that Metis provides\nhuman-readable interpretations while preserving nearly no degradation in\nperformance. We further present four concrete use cases of Metis, showcasing\nhow Metis helps network operators to design, debug, deploy, and ad-hoc adjust\nDL-based networking systems."
  },
  {
    "id": "arxiv-168",
    "title": "How Framelets Enhance Graph Neural Networks",
    "abstract": "This paper presents a new approach for assembling graph neural networks based\non framelet transforms. The latter provides a multi-scale representation for\ngraph-structured data. We decompose an input graph into low-pass and high-pass\nfrequencies coefficients for network training, which then defines a\nframelet-based graph convolution. The framelet decomposition naturally induces\na graph pooling strategy by aggregating the graph feature into low-pass and\nhigh-pass spectra, which considers both the feature values and geometry of the\ngraph data and conserves the total information. The graph neural networks with\nthe proposed framelet convolution and pooling achieve state-of-the-art\nperformance in many node and graph prediction tasks. Moreover, we propose\nshrinkage as a new activation for the framelet convolution, which thresholds\nhigh-frequency information at different scales. Compared to ReLU, shrinkage\nactivation improves model performance on denoising and signal compression:\nnoises in both node and structure can be significantly reduced by accurately\ncutting off the high-pass coefficients from framelet decomposition, and the\nsignal can be compressed to less than half its original size with\nwell-preserved prediction performance.",
    "text": "How Framelets Enhance Graph Neural Networks\n\nThis paper presents a new approach for assembling graph neural networks based\non framelet transforms. The latter provides a multi-scale representation for\ngraph-structured data. We decompose an input graph into low-pass and high-pass\nfrequencies coefficients for network training, which then defines a\nframelet-based graph convolution. The framelet decomposition naturally induces\na graph pooling strategy by aggregating the graph feature into low-pass and\nhigh-pass spectra, which considers both the feature values and geometry of the\ngraph data and conserves the total information. The graph neural networks with\nthe proposed framelet convolution and pooling achieve state-of-the-art\nperformance in many node and graph prediction tasks. Moreover, we propose\nshrinkage as a new activation for the framelet convolution, which thresholds\nhigh-frequency information at different scales. Compared to ReLU, shrinkage\nactivation improves model performance on denoising and signal compression:\nnoises in both node and structure can be significantly reduced by accurately\ncutting off the high-pass coefficients from framelet decomposition, and the\nsignal can be compressed to less than half its original size with\nwell-preserved prediction performance."
  },
  {
    "id": "arxiv-169",
    "title": "STORM: Foundations of End-to-End Empirical Risk Minimization on the Edge",
    "abstract": "Empirical risk minimization is perhaps the most influential idea in\nstatistical learning, with applications to nearly all scientific and technical\ndomains in the form of regression and classification models. To analyze massive\nstreaming datasets in distributed computing environments, practitioners\nincreasingly prefer to deploy regression models on edge rather than in the\ncloud. By keeping data on edge devices, we minimize the energy, communication,\nand data security risk associated with the model. Although it is equally\nadvantageous to train models at the edge, a common assumption is that the model\nwas originally trained in the cloud, since training typically requires\nsubstantial computation and memory. To this end, we propose STORM, an online\nsketch for empirical risk minimization. STORM compresses a data stream into a\ntiny array of integer counters. This sketch is sufficient to estimate a variety\nof surrogate losses over the original dataset. We provide rigorous theoretical\nanalysis and show that STORM can estimate a carefully chosen surrogate loss for\nthe least-squares objective. In an exhaustive experimental comparison for\nlinear regression models on real-world datasets, we find that STORM allows\naccurate regression models to be trained.",
    "text": "STORM: Foundations of End-to-End Empirical Risk Minimization on the Edge\n\nEmpirical risk minimization is perhaps the most influential idea in\nstatistical learning, with applications to nearly all scientific and technical\ndomains in the form of regression and classification models. To analyze massive\nstreaming datasets in distributed computing environments, practitioners\nincreasingly prefer to deploy regression models on edge rather than in the\ncloud. By keeping data on edge devices, we minimize the energy, communication,\nand data security risk associated with the model. Although it is equally\nadvantageous to train models at the edge, a common assumption is that the model\nwas originally trained in the cloud, since training typically requires\nsubstantial computation and memory. To this end, we propose STORM, an online\nsketch for empirical risk minimization. STORM compresses a data stream into a\ntiny array of integer counters. This sketch is sufficient to estimate a variety\nof surrogate losses over the original dataset. We provide rigorous theoretical\nanalysis and show that STORM can estimate a carefully chosen surrogate loss for\nthe least-squares objective. In an exhaustive experimental comparison for\nlinear regression models on real-world datasets, we find that STORM allows\naccurate regression models to be trained."
  },
  {
    "id": "arxiv-170",
    "title": "NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse\n  Networks",
    "abstract": "Long training times of deep neural networks are a bottleneck in machine\nlearning research. The major impediment to fast training is the quadratic\ngrowth of both memory and compute requirements of dense and convolutional\nlayers with respect to their information bandwidth. Recently, training `a\npriori' sparse networks has been proposed as a method for allowing layers to\nretain high information bandwidth, while keeping memory and compute low.\nHowever, the choice of which sparse topology should be used in these networks\nis unclear. In this work, we provide a theoretical foundation for the choice of\nintra-layer topology. First, we derive a new sparse neural network\ninitialization scheme that allows us to explore the space of very deep sparse\nnetworks. Next, we evaluate several topologies and show that seemingly similar\ntopologies can often have a large difference in attainable accuracy. To explain\nthese differences, we develop a data-free heuristic that can evaluate a\ntopology independently from the dataset the network will be trained on. We then\nderive a set of requirements that make a good topology, and arrive at a single\ntopology that satisfies all of them.",
    "text": "NeuroFabric: Identifying Ideal Topologies for Training A Priori Sparse\n  Networks\n\nLong training times of deep neural networks are a bottleneck in machine\nlearning research. The major impediment to fast training is the quadratic\ngrowth of both memory and compute requirements of dense and convolutional\nlayers with respect to their information bandwidth. Recently, training `a\npriori' sparse networks has been proposed as a method for allowing layers to\nretain high information bandwidth, while keeping memory and compute low.\nHowever, the choice of which sparse topology should be used in these networks\nis unclear. In this work, we provide a theoretical foundation for the choice of\nintra-layer topology. First, we derive a new sparse neural network\ninitialization scheme that allows us to explore the space of very deep sparse\nnetworks. Next, we evaluate several topologies and show that seemingly similar\ntopologies can often have a large difference in attainable accuracy. To explain\nthese differences, we develop a data-free heuristic that can evaluate a\ntopology independently from the dataset the network will be trained on. We then\nderive a set of requirements that make a good topology, and arrive at a single\ntopology that satisfies all of them."
  },
  {
    "id": "arxiv-171",
    "title": "Autoregressive-Model-Based Methods for Online Time Series Prediction\n  with Missing Values: an Experimental Evaluation",
    "abstract": "Time series prediction with missing values is an important problem of time\nseries analysis since complete data is usually hard to obtain in many\nreal-world applications. To model the generation of time series, autoregressive\n(AR) model is a basic and widely used one, which assumes that each observation\nin the time series is a noisy linear combination of some previous observations\nalong with a constant shift. To tackle the problem of prediction with missing\nvalues, a number of methods were proposed based on various data models. For\nreal application scenarios, how do these methods perform over different types\nof time series with different levels of data missing remains to be\ninvestigated. In this paper, we focus on online methods for AR-model-based time\nseries prediction with missing values. We adapted five mainstream methods to\nfit in such a scenario. We make detailed discussion on each of them by\nintroducing their core ideas about how to estimate the AR coefficients and\ntheir different strategies to deal with missing values. We also present\nalgorithmic implementations for better understanding. In order to\ncomprehensively evaluate these methods and do the comparison, we conduct\nexperiments with various configurations of relative parameters over both\nsynthetic and real data. From the experimental results, we derived several\nnoteworthy conclusions and shows that imputation is a simple but reliable\nstrategy to handle missing values in online prediction tasks.",
    "text": "Autoregressive-Model-Based Methods for Online Time Series Prediction\n  with Missing Values: an Experimental Evaluation\n\nTime series prediction with missing values is an important problem of time\nseries analysis since complete data is usually hard to obtain in many\nreal-world applications. To model the generation of time series, autoregressive\n(AR) model is a basic and widely used one, which assumes that each observation\nin the time series is a noisy linear combination of some previous observations\nalong with a constant shift. To tackle the problem of prediction with missing\nvalues, a number of methods were proposed based on various data models. For\nreal application scenarios, how do these methods perform over different types\nof time series with different levels of data missing remains to be\ninvestigated. In this paper, we focus on online methods for AR-model-based time\nseries prediction with missing values. We adapted five mainstream methods to\nfit in such a scenario. We make detailed discussion on each of them by\nintroducing their core ideas about how to estimate the AR coefficients and\ntheir different strategies to deal with missing values. We also present\nalgorithmic implementations for better understanding. In order to\ncomprehensively evaluate these methods and do the comparison, we conduct\nexperiments with various configurations of relative parameters over both\nsynthetic and real data. From the experimental results, we derived several\nnoteworthy conclusions and shows that imputation is a simple but reliable\nstrategy to handle missing values in online prediction tasks."
  },
  {
    "id": "arxiv-172",
    "title": "Meta-Reinforcement Learning via Buffering Graph Signatures for Live\n  Video Streaming Events",
    "abstract": "In this study, we present a meta-learning model to adapt the predictions of\nthe network's capacity between viewers who participate in a live video\nstreaming event. We propose the MELANIE model, where an event is formulated as\na Markov Decision Process, performing meta-learning on reinforcement learning\ntasks. By considering a new event as a task, we design an actor-critic learning\nscheme to compute the optimal policy on estimating the viewers' high-bandwidth\nconnections. To ensure fast adaptation to new connections or changes among\nviewers during an event, we implement a prioritized replay memory buffer based\non the Kullback-Leibler divergence of the reward/throughput of the viewers'\nconnections. Moreover, we adopt a model-agnostic meta-learning framework to\ngenerate a global model from past events. As viewers scarcely participate in\nseveral events, the challenge resides on how to account for the low structural\nsimilarity of different events. To combat this issue, we design a graph\nsignature buffer to calculate the structural similarities of several streaming\nevents and adjust the training of the global model accordingly. We evaluate the\nproposed model on the link weight prediction task on three real-world datasets\nof live video streaming events. Our experiments demonstrate the effectiveness\nof our proposed model, with an average relative gain of 25% against\nstate-of-the-art strategies. For reproduction purposes, our evaluation datasets\nand implementation are publicly available at\nhttps://github.com/stefanosantaris/melanie",
    "text": "Meta-Reinforcement Learning via Buffering Graph Signatures for Live\n  Video Streaming Events\n\nIn this study, we present a meta-learning model to adapt the predictions of\nthe network's capacity between viewers who participate in a live video\nstreaming event. We propose the MELANIE model, where an event is formulated as\na Markov Decision Process, performing meta-learning on reinforcement learning\ntasks. By considering a new event as a task, we design an actor-critic learning\nscheme to compute the optimal policy on estimating the viewers' high-bandwidth\nconnections. To ensure fast adaptation to new connections or changes among\nviewers during an event, we implement a prioritized replay memory buffer based\non the Kullback-Leibler divergence of the reward/throughput of the viewers'\nconnections. Moreover, we adopt a model-agnostic meta-learning framework to\ngenerate a global model from past events. As viewers scarcely participate in\nseveral events, the challenge resides on how to account for the low structural\nsimilarity of different events. To combat this issue, we design a graph\nsignature buffer to calculate the structural similarities of several streaming\nevents and adjust the training of the global model accordingly. We evaluate the\nproposed model on the link weight prediction task on three real-world datasets\nof live video streaming events. Our experiments demonstrate the effectiveness\nof our proposed model, with an average relative gain of 25% against\nstate-of-the-art strategies. For reproduction purposes, our evaluation datasets\nand implementation are publicly available at\nhttps://github.com/stefanosantaris/melanie"
  },
  {
    "id": "arxiv-173",
    "title": "Infinite Factorial Finite State Machine for Blind Multiuser Channel\n  Estimation",
    "abstract": "New communication standards need to deal with machine-to-machine\ncommunications, in which users may start or stop transmitting at any time in an\nasynchronous manner. Thus, the number of users is an unknown and time-varying\nparameter that needs to be accurately estimated in order to properly recover\nthe symbols transmitted by all users in the system. In this paper, we address\nthe problem of joint channel parameter and data estimation in a multiuser\ncommunication channel in which the number of transmitters is not known. For\nthat purpose, we develop the infinite factorial finite state machine model, a\nBayesian nonparametric model based on the Markov Indian buffet that allows for\nan unbounded number of transmitters with arbitrary channel length. We propose\nan inference algorithm that makes use of slice sampling and particle Gibbs with\nancestor sampling. Our approach is fully blind as it does not require a prior\nchannel estimation step, prior knowledge of the number of transmitters, or any\nsignaling information. Our experimental results, loosely based on the LTE\nrandom access channel, show that the proposed approach can effectively recover\nthe data-generating process for a wide range of scenarios, with varying number\nof transmitters, number of receivers, constellation order, channel length, and\nsignal-to-noise ratio.",
    "text": "Infinite Factorial Finite State Machine for Blind Multiuser Channel\n  Estimation\n\nNew communication standards need to deal with machine-to-machine\ncommunications, in which users may start or stop transmitting at any time in an\nasynchronous manner. Thus, the number of users is an unknown and time-varying\nparameter that needs to be accurately estimated in order to properly recover\nthe symbols transmitted by all users in the system. In this paper, we address\nthe problem of joint channel parameter and data estimation in a multiuser\ncommunication channel in which the number of transmitters is not known. For\nthat purpose, we develop the infinite factorial finite state machine model, a\nBayesian nonparametric model based on the Markov Indian buffet that allows for\nan unbounded number of transmitters with arbitrary channel length. We propose\nan inference algorithm that makes use of slice sampling and particle Gibbs with\nancestor sampling. Our approach is fully blind as it does not require a prior\nchannel estimation step, prior knowledge of the number of transmitters, or any\nsignaling information. Our experimental results, loosely based on the LTE\nrandom access channel, show that the proposed approach can effectively recover\nthe data-generating process for a wide range of scenarios, with varying number\nof transmitters, number of receivers, constellation order, channel length, and\nsignal-to-noise ratio."
  },
  {
    "id": "arxiv-174",
    "title": "Machine Learning Simulates Agent-Based Model Towards Policy",
    "abstract": "Public Policies are not intrinsically positive or negative. Rather, policies\nprovide varying levels of effects across different recipients.\nMethodologically, computational modeling enables the application of a\ncombination of multiple influences on empirical data, thus allowing for\nheterogeneous response to policies. We use a random forest machine learning\nalgorithm to emulate an agent-based model (ABM) and evaluate competing policies\nacross 46 Metropolitan Regions (MRs) in Brazil. In doing so, we use input\nparameters and output indicators of 11,076 actual simulation runs and one\nmillion emulated runs. As a result, we obtain the optimal (and non-optimal)\nperformance of each region over the policies. Optimum is defined as a\ncombination of production and inequality indicators for the full ensemble of\nMRs. Results suggest that MRs already have embedded structures that favor\noptimal or non-optimal results, but they also illustrate which policy is more\nbeneficial to each place. In addition to providing MR-specific policies'\nresults, the use of machine learning to simulate an ABM reduces the\ncomputational burden, whereas allowing for a much larger variation among model\nparameters. The coherence of results within the context of larger uncertainty\n-- vis-\\`a-vis those of the original ABM -- suggests an additional test of\nrobustness of the model. At the same time the exercise indicates which\nparameters should policymakers intervene, in order to work towards optimum of\nMRs.",
    "text": "Machine Learning Simulates Agent-Based Model Towards Policy\n\nPublic Policies are not intrinsically positive or negative. Rather, policies\nprovide varying levels of effects across different recipients.\nMethodologically, computational modeling enables the application of a\ncombination of multiple influences on empirical data, thus allowing for\nheterogeneous response to policies. We use a random forest machine learning\nalgorithm to emulate an agent-based model (ABM) and evaluate competing policies\nacross 46 Metropolitan Regions (MRs) in Brazil. In doing so, we use input\nparameters and output indicators of 11,076 actual simulation runs and one\nmillion emulated runs. As a result, we obtain the optimal (and non-optimal)\nperformance of each region over the policies. Optimum is defined as a\ncombination of production and inequality indicators for the full ensemble of\nMRs. Results suggest that MRs already have embedded structures that favor\noptimal or non-optimal results, but they also illustrate which policy is more\nbeneficial to each place. In addition to providing MR-specific policies'\nresults, the use of machine learning to simulate an ABM reduces the\ncomputational burden, whereas allowing for a much larger variation among model\nparameters. The coherence of results within the context of larger uncertainty\n-- vis-\\`a-vis those of the original ABM -- suggests an additional test of\nrobustness of the model. At the same time the exercise indicates which\nparameters should policymakers intervene, in order to work towards optimum of\nMRs."
  },
  {
    "id": "arxiv-175",
    "title": "On Expressivity and Trainability of Quadratic Networks",
    "abstract": "Inspired by the diversity of biological neurons, quadratic artificial neurons\ncan play an important role in deep learning models. The type of quadratic\nneurons of our interest replaces the inner-product operation in the\nconventional neuron with a quadratic function. Despite promising results so far\nachieved by networks of quadratic neurons, there are important issues not well\naddressed. Theoretically, the superior expressivity of a quadratic network over\neither a conventional network or a conventional network via quadratic\nactivation is not fully elucidated, which makes the use of quadratic networks\nnot well grounded. Practically, although a quadratic network can be trained via\ngeneric backpropagation, it can be subject to a higher risk of collapse than\nthe conventional counterpart. To address these issues, we first apply the\nspline theory and a measure from algebraic geometry to give two theorems that\ndemonstrate better model expressivity of a quadratic network than the\nconventional counterpart with or without quadratic activation. Then, we propose\nan effective and efficient training strategy referred to as ReLinear to\nstabilize the training process of a quadratic network, thereby unleashing the\nfull potential in its associated machine learning tasks. Comprehensive\nexperiments on popular datasets are performed to support our findings and\nevaluate the performance of quadratic deep learning.",
    "text": "On Expressivity and Trainability of Quadratic Networks\n\nInspired by the diversity of biological neurons, quadratic artificial neurons\ncan play an important role in deep learning models. The type of quadratic\nneurons of our interest replaces the inner-product operation in the\nconventional neuron with a quadratic function. Despite promising results so far\nachieved by networks of quadratic neurons, there are important issues not well\naddressed. Theoretically, the superior expressivity of a quadratic network over\neither a conventional network or a conventional network via quadratic\nactivation is not fully elucidated, which makes the use of quadratic networks\nnot well grounded. Practically, although a quadratic network can be trained via\ngeneric backpropagation, it can be subject to a higher risk of collapse than\nthe conventional counterpart. To address these issues, we first apply the\nspline theory and a measure from algebraic geometry to give two theorems that\ndemonstrate better model expressivity of a quadratic network than the\nconventional counterpart with or without quadratic activation. Then, we propose\nan effective and efficient training strategy referred to as ReLinear to\nstabilize the training process of a quadratic network, thereby unleashing the\nfull potential in its associated machine learning tasks. Comprehensive\nexperiments on popular datasets are performed to support our findings and\nevaluate the performance of quadratic deep learning."
  },
  {
    "id": "arxiv-176",
    "title": "Physical Side-Channel Attacks on Embedded Neural Networks: A Survey",
    "abstract": "During the last decade, Deep Neural Networks (DNN) have progressively been\nintegrated on all types of platforms, from data centers to embedded systems\nincluding low-power processors and, recently, FPGAs. Neural Networks (NN) are\nexpected to become ubiquitous in IoT systems by transforming all sorts of\nreal-world applications, including applications in the safety-critical and\nsecurity-sensitive domains. However, the underlying hardware security\nvulnerabilities of embedded NN implementations remain unaddressed. In\nparticular, embedded DNN implementations are vulnerable to Side-Channel\nAnalysis (SCA) attacks, which are especially important in the IoT and edge\ncomputing contexts where an attacker can usually gain physical access to the\ntargeted device. A research field has therefore emerged and is rapidly growing\nin terms of the use of SCA including timing, electromagnetic attacks and power\nattacks to target NN embedded implementations. Since 2018, research papers have\nshown that SCA enables an attacker to recover inference models architectures\nand parameters, to expose industrial IP and endangers data confidentiality and\nprivacy. Without a complete review of this emerging field in the literature so\nfar, this paper surveys state-of-the-art physical SCA attacks relative to the\nimplementation of embedded DNNs on micro-controllers and FPGAs in order to\nprovide a thorough analysis on the current landscape. It provides a taxonomy\nand a detailed classification of current attacks. It first discusses mitigation\ntechniques and then provides insights for future research leads.",
    "text": "Physical Side-Channel Attacks on Embedded Neural Networks: A Survey\n\nDuring the last decade, Deep Neural Networks (DNN) have progressively been\nintegrated on all types of platforms, from data centers to embedded systems\nincluding low-power processors and, recently, FPGAs. Neural Networks (NN) are\nexpected to become ubiquitous in IoT systems by transforming all sorts of\nreal-world applications, including applications in the safety-critical and\nsecurity-sensitive domains. However, the underlying hardware security\nvulnerabilities of embedded NN implementations remain unaddressed. In\nparticular, embedded DNN implementations are vulnerable to Side-Channel\nAnalysis (SCA) attacks, which are especially important in the IoT and edge\ncomputing contexts where an attacker can usually gain physical access to the\ntargeted device. A research field has therefore emerged and is rapidly growing\nin terms of the use of SCA including timing, electromagnetic attacks and power\nattacks to target NN embedded implementations. Since 2018, research papers have\nshown that SCA enables an attacker to recover inference models architectures\nand parameters, to expose industrial IP and endangers data confidentiality and\nprivacy. Without a complete review of this emerging field in the literature so\nfar, this paper surveys state-of-the-art physical SCA attacks relative to the\nimplementation of embedded DNNs on micro-controllers and FPGAs in order to\nprovide a thorough analysis on the current landscape. It provides a taxonomy\nand a detailed classification of current attacks. It first discusses mitigation\ntechniques and then provides insights for future research leads."
  },
  {
    "id": "arxiv-177",
    "title": "Interrupted and cascaded permutation invariant training for speech\n  separation",
    "abstract": "Permutation Invariant Training (PIT) has long been a stepping stone method\nfor training speech separation model in handling the label ambiguity problem.\nWith PIT selecting the minimum cost label assignments dynamically, very few\nstudies considered the separation problem to be optimizing both the model\nparameters and the label assignments, but focused on searching for good model\narchitecture and parameters. In this paper, we investigate instead for a given\nmodel architecture the various flexible label assignment strategies for\ntraining the model, rather than directly using PIT. Surprisingly, we discover a\nsignificant performance boost compared to PIT is possible if the model is\ntrained with fixed label assignments and a good set of labels is chosen. With\nfixed label training cascaded between two sections of PIT, we achieved the\nstate-of-the-art performance on WSJ0-2mix without changing the model\narchitecture at all.",
    "text": "Interrupted and cascaded permutation invariant training for speech\n  separation\n\nPermutation Invariant Training (PIT) has long been a stepping stone method\nfor training speech separation model in handling the label ambiguity problem.\nWith PIT selecting the minimum cost label assignments dynamically, very few\nstudies considered the separation problem to be optimizing both the model\nparameters and the label assignments, but focused on searching for good model\narchitecture and parameters. In this paper, we investigate instead for a given\nmodel architecture the various flexible label assignment strategies for\ntraining the model, rather than directly using PIT. Surprisingly, we discover a\nsignificant performance boost compared to PIT is possible if the model is\ntrained with fixed label assignments and a good set of labels is chosen. With\nfixed label training cascaded between two sections of PIT, we achieved the\nstate-of-the-art performance on WSJ0-2mix without changing the model\narchitecture at all."
  },
  {
    "id": "arxiv-178",
    "title": "e-QRAQ: A Multi-turn Reasoning Dataset and Simulator with Explanations",
    "abstract": "In this paper we present a new dataset and user simulator e-QRAQ (explainable\nQuery, Reason, and Answer Question) which tests an Agent's ability to read an\nambiguous text; ask questions until it can answer a challenge question; and\nexplain the reasoning behind its questions and answer. The User simulator\nprovides the Agent with a short, ambiguous story and a challenge question about\nthe story. The story is ambiguous because some of the entities have been\nreplaced by variables. At each turn the Agent may ask for the value of a\nvariable or try to answer the challenge question. In response the User\nsimulator provides a natural language explanation of why the Agent's query or\nanswer was useful in narrowing down the set of possible answers, or not. To\ndemonstrate one potential application of the e-QRAQ dataset, we train a new\nneural architecture based on End-to-End Memory Networks to successfully\ngenerate both predictions and partial explanations of its current understanding\nof the problem. We observe a strong correlation between the quality of the\nprediction and explanation.",
    "text": "e-QRAQ: A Multi-turn Reasoning Dataset and Simulator with Explanations\n\nIn this paper we present a new dataset and user simulator e-QRAQ (explainable\nQuery, Reason, and Answer Question) which tests an Agent's ability to read an\nambiguous text; ask questions until it can answer a challenge question; and\nexplain the reasoning behind its questions and answer. The User simulator\nprovides the Agent with a short, ambiguous story and a challenge question about\nthe story. The story is ambiguous because some of the entities have been\nreplaced by variables. At each turn the Agent may ask for the value of a\nvariable or try to answer the challenge question. In response the User\nsimulator provides a natural language explanation of why the Agent's query or\nanswer was useful in narrowing down the set of possible answers, or not. To\ndemonstrate one potential application of the e-QRAQ dataset, we train a new\nneural architecture based on End-to-End Memory Networks to successfully\ngenerate both predictions and partial explanations of its current understanding\nof the problem. We observe a strong correlation between the quality of the\nprediction and explanation."
  },
  {
    "id": "arxiv-179",
    "title": "Constrained Monotonic Neural Networks",
    "abstract": "Deep neural networks are becoming increasingly popular in approximating\narbitrary functions from noisy data. But wider adoption is being hindered by\nthe need to explain such models and to impose additional constraints on them.\nMonotonicity constraint is one of the most requested properties in real-world\nscenarios and is the focus of this paper. One of the oldest ways to construct a\nmonotonic fully connected neural network is to constrain its weights to be\nnon-negative while employing a monotonic activation function. Unfortunately,\nthis construction does not work with popular non-saturated activation functions\nsuch as ReLU, ELU, SELU etc, as it can only approximate convex functions. We\nshow this shortcoming can be fixed by employing the original activation\nfunction for a part of the neurons in the layer, and employing its point\nreflection for the other part. Our experiments show this approach of building\nmonotonic deep neural networks have matching or better accuracy when compared\nto other state-of-the-art methods such as deep lattice networks or monotonic\nnetworks obtained by heuristic regularization. This method is the simplest one\nin the sense of having the least number of parameters, not requiring any\nmodifications to the learning procedure or steps post-learning steps.",
    "text": "Constrained Monotonic Neural Networks\n\nDeep neural networks are becoming increasingly popular in approximating\narbitrary functions from noisy data. But wider adoption is being hindered by\nthe need to explain such models and to impose additional constraints on them.\nMonotonicity constraint is one of the most requested properties in real-world\nscenarios and is the focus of this paper. One of the oldest ways to construct a\nmonotonic fully connected neural network is to constrain its weights to be\nnon-negative while employing a monotonic activation function. Unfortunately,\nthis construction does not work with popular non-saturated activation functions\nsuch as ReLU, ELU, SELU etc, as it can only approximate convex functions. We\nshow this shortcoming can be fixed by employing the original activation\nfunction for a part of the neurons in the layer, and employing its point\nreflection for the other part. Our experiments show this approach of building\nmonotonic deep neural networks have matching or better accuracy when compared\nto other state-of-the-art methods such as deep lattice networks or monotonic\nnetworks obtained by heuristic regularization. This method is the simplest one\nin the sense of having the least number of parameters, not requiring any\nmodifications to the learning procedure or steps post-learning steps."
  },
  {
    "id": "arxiv-180",
    "title": "A Multi-scale Time-series Dataset with Benchmark for Machine Learning in\n  Decarbonized Energy Grids",
    "abstract": "The electric grid is a key enabling infrastructure for the ambitious\ntransition towards carbon neutrality as we grapple with climate change. With\ndeepening penetration of renewable energy resources and electrified\ntransportation, the reliable and secure operation of the electric grid becomes\nincreasingly challenging. In this paper, we present PSML, a first-of-its-kind\nopen-access multi-scale time-series dataset, to aid in the development of\ndata-driven machine learning (ML) based approaches towards reliable operation\nof future electric grids. The dataset is generated through a novel transmission\n+ distribution (T+D) co-simulation designed to capture the increasingly\nimportant interactions and uncertainties of the grid dynamics, containing\nelectric load, renewable generation, weather, voltage and current measurements\nover multiple spatio-temporal scales. Using PSML, we provide state-of-the-art\nML baselines on three challenging use cases of critical importance to achieve:\n(i) early detection, accurate classification and localization of dynamic\ndisturbance events; (ii) robust hierarchical forecasting of load and renewable\nenergy with the presence of uncertainties and extreme events; and (iii)\nrealistic synthetic generation of physical-law-constrained measurement time\nseries. We envision that this dataset will enable advances for ML in dynamic\nsystems, while simultaneously allowing ML researchers to contribute towards\ncarbon-neutral electricity and mobility.",
    "text": "A Multi-scale Time-series Dataset with Benchmark for Machine Learning in\n  Decarbonized Energy Grids\n\nThe electric grid is a key enabling infrastructure for the ambitious\ntransition towards carbon neutrality as we grapple with climate change. With\ndeepening penetration of renewable energy resources and electrified\ntransportation, the reliable and secure operation of the electric grid becomes\nincreasingly challenging. In this paper, we present PSML, a first-of-its-kind\nopen-access multi-scale time-series dataset, to aid in the development of\ndata-driven machine learning (ML) based approaches towards reliable operation\nof future electric grids. The dataset is generated through a novel transmission\n+ distribution (T+D) co-simulation designed to capture the increasingly\nimportant interactions and uncertainties of the grid dynamics, containing\nelectric load, renewable generation, weather, voltage and current measurements\nover multiple spatio-temporal scales. Using PSML, we provide state-of-the-art\nML baselines on three challenging use cases of critical importance to achieve:\n(i) early detection, accurate classification and localization of dynamic\ndisturbance events; (ii) robust hierarchical forecasting of load and renewable\nenergy with the presence of uncertainties and extreme events; and (iii)\nrealistic synthetic generation of physical-law-constrained measurement time\nseries. We envision that this dataset will enable advances for ML in dynamic\nsystems, while simultaneously allowing ML researchers to contribute towards\ncarbon-neutral electricity and mobility."
  },
  {
    "id": "arxiv-181",
    "title": "Catch Me If You GAN: Using Artificial Intelligence for Fake Log\n  Generation",
    "abstract": "With artificial intelligence (AI) becoming relevant in various parts of\neveryday life, other technologies are already widely influenced by the new way\nof handling large amounts of data. Although widespread already, AI has had only\npunctual influences on the cybersecurity field specifically. Many techniques\nand technologies used by cybersecurity experts function through manual labor\nand barely draw on automation, e.g., logs are often reviewed manually by system\nadmins for potentially malicious keywords. This work evaluates the use of a\nspecial type of AI called generative adversarial networks (GANs) for log\ngeneration. More precisely, three different generative adversarial networks,\nSeqGAN, MaliGAN, and CoT, are reviewed in this research regarding their\nperformance, focusing on generating new logs as a means of deceiving system\nadmins for red teams. Although static generators for fake logs have been around\nfor a while, their produces are usually easy to reveal as such. Using AI as an\napproach to this problem has not been widely researched. Identified challenges\nconsist of formatting, dates and times, and overall consistency. Summing up the\nresults, GANs seem not to be a good fit for generating fake logs. Their\ncapability to detect fake logs, however, might be of use in practical\nscenarios.",
    "text": "Catch Me If You GAN: Using Artificial Intelligence for Fake Log\n  Generation\n\nWith artificial intelligence (AI) becoming relevant in various parts of\neveryday life, other technologies are already widely influenced by the new way\nof handling large amounts of data. Although widespread already, AI has had only\npunctual influences on the cybersecurity field specifically. Many techniques\nand technologies used by cybersecurity experts function through manual labor\nand barely draw on automation, e.g., logs are often reviewed manually by system\nadmins for potentially malicious keywords. This work evaluates the use of a\nspecial type of AI called generative adversarial networks (GANs) for log\ngeneration. More precisely, three different generative adversarial networks,\nSeqGAN, MaliGAN, and CoT, are reviewed in this research regarding their\nperformance, focusing on generating new logs as a means of deceiving system\nadmins for red teams. Although static generators for fake logs have been around\nfor a while, their produces are usually easy to reveal as such. Using AI as an\napproach to this problem has not been widely researched. Identified challenges\nconsist of formatting, dates and times, and overall consistency. Summing up the\nresults, GANs seem not to be a good fit for generating fake logs. Their\ncapability to detect fake logs, however, might be of use in practical\nscenarios."
  },
  {
    "id": "arxiv-182",
    "title": "Truncated Marginal Neural Ratio Estimation",
    "abstract": "Parametric stochastic simulators are ubiquitous in science, often featuring\nhigh-dimensional input parameters and/or an intractable likelihood. Performing\nBayesian parameter inference in this context can be challenging. We present a\nneural simulation-based inference algorithm which simultaneously offers\nsimulation efficiency and fast empirical posterior testability, which is unique\namong modern algorithms. Our approach is simulation efficient by simultaneously\nestimating low-dimensional marginal posteriors instead of the joint posterior\nand by proposing simulations targeted to an observation of interest via a prior\nsuitably truncated by an indicator function. Furthermore, by estimating a\nlocally amortized posterior our algorithm enables efficient empirical tests of\nthe robustness of the inference results. Since scientists cannot access the\nground truth, these tests are necessary for trusting inference in real-world\napplications. We perform experiments on a marginalized version of the\nsimulation-based inference benchmark and two complex and narrow posteriors,\nhighlighting the simulator efficiency of our algorithm as well as the quality\nof the estimated marginal posteriors.",
    "text": "Truncated Marginal Neural Ratio Estimation\n\nParametric stochastic simulators are ubiquitous in science, often featuring\nhigh-dimensional input parameters and/or an intractable likelihood. Performing\nBayesian parameter inference in this context can be challenging. We present a\nneural simulation-based inference algorithm which simultaneously offers\nsimulation efficiency and fast empirical posterior testability, which is unique\namong modern algorithms. Our approach is simulation efficient by simultaneously\nestimating low-dimensional marginal posteriors instead of the joint posterior\nand by proposing simulations targeted to an observation of interest via a prior\nsuitably truncated by an indicator function. Furthermore, by estimating a\nlocally amortized posterior our algorithm enables efficient empirical tests of\nthe robustness of the inference results. Since scientists cannot access the\nground truth, these tests are necessary for trusting inference in real-world\napplications. We perform experiments on a marginalized version of the\nsimulation-based inference benchmark and two complex and narrow posteriors,\nhighlighting the simulator efficiency of our algorithm as well as the quality\nof the estimated marginal posteriors."
  },
  {
    "id": "arxiv-183",
    "title": "Individual health-disease phase diagrams for disease prevention based on\n  machine learning",
    "abstract": "Early disease detection and prevention methods based on effective\ninterventions are gaining attention. Machine learning technology has enabled\nprecise disease prediction by capturing individual differences in multivariate\ndata. Progress in precision medicine has revealed that substantial\nheterogeneity exists in health data at the individual level and that complex\nhealth factors are involved in the development of chronic diseases. However, it\nremains a challenge to identify individual physiological state changes in\ncross-disease onset processes because of the complex relationships among\nmultiple biomarkers. Here, we present the health-disease phase diagram (HDPD),\nwhich represents a personal health state by visualizing the boundary values of\nmultiple biomarkers that fluctuate early in the disease progression process. In\nHDPDs, future onset predictions are represented by perturbing multiple\nbiomarker values while accounting for dependencies among variables. We\nconstructed HDPDs for 11 non-communicable diseases (NCDs) from a longitudinal\nhealth checkup cohort of 3,238 individuals, comprising 3,215 measurement items\nand genetic data. Improvement of biomarker values to the non-onset region in\nHDPD significantly prevented future disease onset in 7 out of 11 NCDs. Our\nresults demonstrate that HDPDs can represent individual physiological states in\nthe onset process and be used as intervention goals for disease prevention.",
    "text": "Individual health-disease phase diagrams for disease prevention based on\n  machine learning\n\nEarly disease detection and prevention methods based on effective\ninterventions are gaining attention. Machine learning technology has enabled\nprecise disease prediction by capturing individual differences in multivariate\ndata. Progress in precision medicine has revealed that substantial\nheterogeneity exists in health data at the individual level and that complex\nhealth factors are involved in the development of chronic diseases. However, it\nremains a challenge to identify individual physiological state changes in\ncross-disease onset processes because of the complex relationships among\nmultiple biomarkers. Here, we present the health-disease phase diagram (HDPD),\nwhich represents a personal health state by visualizing the boundary values of\nmultiple biomarkers that fluctuate early in the disease progression process. In\nHDPDs, future onset predictions are represented by perturbing multiple\nbiomarker values while accounting for dependencies among variables. We\nconstructed HDPDs for 11 non-communicable diseases (NCDs) from a longitudinal\nhealth checkup cohort of 3,238 individuals, comprising 3,215 measurement items\nand genetic data. Improvement of biomarker values to the non-onset region in\nHDPD significantly prevented future disease onset in 7 out of 11 NCDs. Our\nresults demonstrate that HDPDs can represent individual physiological states in\nthe onset process and be used as intervention goals for disease prevention."
  },
  {
    "id": "arxiv-184",
    "title": "Classification of Stochastic Processes with Topological Data Analysis",
    "abstract": "In this study, we examine if engineered topological features can distinguish\ntime series sampled from different stochastic processes with different noise\ncharacteristics, in both balanced and unbalanced sampling schemes. We compare\nour classification results against the results of the same classification tasks\nbuilt on statistical and raw features. We conclude that in classification tasks\nof time series, different machine learning models built on engineered\ntopological features perform consistently better than those built on standard\nstatistical and raw features.",
    "text": "Classification of Stochastic Processes with Topological Data Analysis\n\nIn this study, we examine if engineered topological features can distinguish\ntime series sampled from different stochastic processes with different noise\ncharacteristics, in both balanced and unbalanced sampling schemes. We compare\nour classification results against the results of the same classification tasks\nbuilt on statistical and raw features. We conclude that in classification tasks\nof time series, different machine learning models built on engineered\ntopological features perform consistently better than those built on standard\nstatistical and raw features."
  },
  {
    "id": "arxiv-185",
    "title": "Dynamic Network Adaptation at Inference",
    "abstract": "Machine learning (ML) inference is a real-time workload that must comply with\nstrict Service Level Objectives (SLOs), including latency and accuracy targets.\nUnfortunately, ensuring that SLOs are not violated in inference-serving systems\nis challenging due to inherent model accuracy-latency tradeoffs, SLO diversity\nacross and within application domains, evolution of SLOs over time,\nunpredictable query patterns, and co-location interference. In this paper, we\nobserve that neural networks exhibit high degrees of per-input activation\nsparsity during inference. . Thus, we propose SLO-Aware Neural Networks which\ndynamically drop out nodes per-inference query, thereby tuning the amount of\ncomputation performed, according to specified SLO optimization targets and\nmachine utilization. SLO-Aware Neural Networks achieve average speedups of\n$1.3-56.7\\times$ with little to no accuracy loss (less than 0.3%). When\naccuracy constrained, SLO-Aware Neural Networks are able to serve a range of\naccuracy targets at low latency with the same trained model. When latency\nconstrained, SLO-Aware Neural Networks can proactively alleviate latency\ndegradation from co-location interference while maintaining high accuracy to\nmeet latency constraints.",
    "text": "Dynamic Network Adaptation at Inference\n\nMachine learning (ML) inference is a real-time workload that must comply with\nstrict Service Level Objectives (SLOs), including latency and accuracy targets.\nUnfortunately, ensuring that SLOs are not violated in inference-serving systems\nis challenging due to inherent model accuracy-latency tradeoffs, SLO diversity\nacross and within application domains, evolution of SLOs over time,\nunpredictable query patterns, and co-location interference. In this paper, we\nobserve that neural networks exhibit high degrees of per-input activation\nsparsity during inference. . Thus, we propose SLO-Aware Neural Networks which\ndynamically drop out nodes per-inference query, thereby tuning the amount of\ncomputation performed, according to specified SLO optimization targets and\nmachine utilization. SLO-Aware Neural Networks achieve average speedups of\n$1.3-56.7\\times$ with little to no accuracy loss (less than 0.3%). When\naccuracy constrained, SLO-Aware Neural Networks are able to serve a range of\naccuracy targets at low latency with the same trained model. When latency\nconstrained, SLO-Aware Neural Networks can proactively alleviate latency\ndegradation from co-location interference while maintaining high accuracy to\nmeet latency constraints."
  },
  {
    "id": "arxiv-186",
    "title": "Search-Based Testing Approach for Deep Reinforcement Learning Agents",
    "abstract": "Deep Reinforcement Learning (DRL) algorithms have been increasingly employed\nduring the last decade to solve various decision-making problems such as\nautonomous driving and robotics. However, these algorithms have faced great\nchallenges when deployed in safety-critical environments since they often\nexhibit erroneous behaviors that can lead to potentially critical errors. One\nway to assess the safety of DRL agents is to test them to detect possible\nfaults leading to critical failures during their execution. This raises the\nquestion of how we can efficiently test DRL policies to ensure their\ncorrectness and adherence to safety requirements. Most existing works on\ntesting DRL agents use adversarial attacks that perturb states or actions of\nthe agent. However, such attacks often lead to unrealistic states of the\nenvironment. Their main goal is to test the robustness of DRL agents rather\nthan testing the compliance of agents' policies with respect to requirements.\nDue to the huge state space of DRL environments, the high cost of test\nexecution, and the black-box nature of DRL algorithms, the exhaustive testing\nof DRL agents is impossible. In this paper, we propose a Search-based Testing\nApproach of Reinforcement Learning Agents (STARLA) to test the policy of a DRL\nagent by effectively searching for failing executions of the agent within a\nlimited testing budget. We use machine learning models and a dedicated genetic\nalgorithm to narrow the search towards faulty episodes. We apply STARLA on a\nDeep-Q-Learning agent which is widely used as a benchmark and show that it\nsignificantly outperforms Random Testing by detecting more faults related to\nthe agent's policy. We also investigate how to extract rules that characterize\nfaulty episodes of the DRL agent using our search results. Such rules can be\nused to understand the conditions under which the agent fails and thus assess\nits deployment risks.",
    "text": "Search-Based Testing Approach for Deep Reinforcement Learning Agents\n\nDeep Reinforcement Learning (DRL) algorithms have been increasingly employed\nduring the last decade to solve various decision-making problems such as\nautonomous driving and robotics. However, these algorithms have faced great\nchallenges when deployed in safety-critical environments since they often\nexhibit erroneous behaviors that can lead to potentially critical errors. One\nway to assess the safety of DRL agents is to test them to detect possible\nfaults leading to critical failures during their execution. This raises the\nquestion of how we can efficiently test DRL policies to ensure their\ncorrectness and adherence to safety requirements. Most existing works on\ntesting DRL agents use adversarial attacks that perturb states or actions of\nthe agent. However, such attacks often lead to unrealistic states of the\nenvironment. Their main goal is to test the robustness of DRL agents rather\nthan testing the compliance of agents' policies with respect to requirements.\nDue to the huge state space of DRL environments, the high cost of test\nexecution, and the black-box nature of DRL algorithms, the exhaustive testing\nof DRL agents is impossible. In this paper, we propose a Search-based Testing\nApproach of Reinforcement Learning Agents (STARLA) to test the policy of a DRL\nagent by effectively searching for failing executions of the agent within a\nlimited testing budget. We use machine learning models and a dedicated genetic\nalgorithm to narrow the search towards faulty episodes. We apply STARLA on a\nDeep-Q-Learning agent which is widely used as a benchmark and show that it\nsignificantly outperforms Random Testing by detecting more faults related to\nthe agent's policy. We also investigate how to extract rules that characterize\nfaulty episodes of the DRL agent using our search results. Such rules can be\nused to understand the conditions under which the agent fails and thus assess\nits deployment risks."
  },
  {
    "id": "arxiv-187",
    "title": "Mining Robust Default Configurations for Resource-constrained AutoML",
    "abstract": "Automatic machine learning (AutoML) is a key enabler of the mass deployment\nof the next generation of machine learning systems. A key desideratum for\nfuture ML systems is the automatic selection of models and hyperparameters. We\npresent a novel method of selecting performant configurations for a given task\nby performing offline autoML and mining over a diverse set of tasks. By mining\nthe training tasks, we can select a compact portfolio of configurations that\nperform well over a wide variety of tasks, as well as learn a strategy to\nselect portfolio configurations for yet-unseen tasks. The algorithm runs in a\nzero-shot manner, that is without training any models online except the chosen\none. In a compute- or time-constrained setting, this virtually instant\nselection is highly performant. Further, we show that our approach is effective\nfor warm-starting existing autoML platforms. In both settings, we demonstrate\nan improvement on the state-of-the-art by testing over 62 classification and\nregression datasets. We also demonstrate the utility of recommending\ndata-dependent default configurations that outperform widely used hand-crafted\ndefaults.",
    "text": "Mining Robust Default Configurations for Resource-constrained AutoML\n\nAutomatic machine learning (AutoML) is a key enabler of the mass deployment\nof the next generation of machine learning systems. A key desideratum for\nfuture ML systems is the automatic selection of models and hyperparameters. We\npresent a novel method of selecting performant configurations for a given task\nby performing offline autoML and mining over a diverse set of tasks. By mining\nthe training tasks, we can select a compact portfolio of configurations that\nperform well over a wide variety of tasks, as well as learn a strategy to\nselect portfolio configurations for yet-unseen tasks. The algorithm runs in a\nzero-shot manner, that is without training any models online except the chosen\none. In a compute- or time-constrained setting, this virtually instant\nselection is highly performant. Further, we show that our approach is effective\nfor warm-starting existing autoML platforms. In both settings, we demonstrate\nan improvement on the state-of-the-art by testing over 62 classification and\nregression datasets. We also demonstrate the utility of recommending\ndata-dependent default configurations that outperform widely used hand-crafted\ndefaults."
  },
  {
    "id": "arxiv-188",
    "title": "Asynchronous Stochastic Proximal Optimization Algorithms with Variance\n  Reduction",
    "abstract": "Regularized empirical risk minimization (R-ERM) is an important branch of\nmachine learning, since it constrains the capacity of the hypothesis space and\nguarantees the generalization ability of the learning algorithm. Two classic\nproximal optimization algorithms, i.e., proximal stochastic gradient descent\n(ProxSGD) and proximal stochastic coordinate descent (ProxSCD) have been widely\nused to solve the R-ERM problem. Recently, variance reduction technique was\nproposed to improve ProxSGD and ProxSCD, and the corresponding ProxSVRG and\nProxSVRCD have better convergence rate. These proximal algorithms with variance\nreduction technique have also achieved great success in applications at small\nand moderate scales. However, in order to solve large-scale R-ERM problems and\nmake more practical impacts, the parallel version of these algorithms are\nsorely needed. In this paper, we propose asynchronous ProxSVRG (Async-ProxSVRG)\nand asynchronous ProxSVRCD (Async-ProxSVRCD) algorithms, and prove that\nAsync-ProxSVRG can achieve near linear speedup when the training data is\nsparse, while Async-ProxSVRCD can achieve near linear speedup regardless of the\nsparse condition, as long as the number of block partitions are appropriately\nset. We have conducted experiments on a regularized logistic regression task.\nThe results verified our theoretical findings and demonstrated the practical\nefficiency of the asynchronous stochastic proximal algorithms with variance\nreduction.",
    "text": "Asynchronous Stochastic Proximal Optimization Algorithms with Variance\n  Reduction\n\nRegularized empirical risk minimization (R-ERM) is an important branch of\nmachine learning, since it constrains the capacity of the hypothesis space and\nguarantees the generalization ability of the learning algorithm. Two classic\nproximal optimization algorithms, i.e., proximal stochastic gradient descent\n(ProxSGD) and proximal stochastic coordinate descent (ProxSCD) have been widely\nused to solve the R-ERM problem. Recently, variance reduction technique was\nproposed to improve ProxSGD and ProxSCD, and the corresponding ProxSVRG and\nProxSVRCD have better convergence rate. These proximal algorithms with variance\nreduction technique have also achieved great success in applications at small\nand moderate scales. However, in order to solve large-scale R-ERM problems and\nmake more practical impacts, the parallel version of these algorithms are\nsorely needed. In this paper, we propose asynchronous ProxSVRG (Async-ProxSVRG)\nand asynchronous ProxSVRCD (Async-ProxSVRCD) algorithms, and prove that\nAsync-ProxSVRG can achieve near linear speedup when the training data is\nsparse, while Async-ProxSVRCD can achieve near linear speedup regardless of the\nsparse condition, as long as the number of block partitions are appropriately\nset. We have conducted experiments on a regularized logistic regression task.\nThe results verified our theoretical findings and demonstrated the practical\nefficiency of the asynchronous stochastic proximal algorithms with variance\nreduction."
  },
  {
    "id": "arxiv-189",
    "title": "Differential Privacy-enabled Federated Learning for Sensitive Health\n  Data",
    "abstract": "Leveraging real-world health data for machine learning tasks requires\naddressing many practical challenges, such as distributed data silos, privacy\nconcerns with creating a centralized database from person-specific sensitive\ndata, resource constraints for transferring and integrating data from multiple\nsites, and risk of a single point of failure. In this paper, we introduce a\nfederated learning framework that can learn a global model from distributed\nhealth data held locally at different sites. The framework offers two levels of\nprivacy protection. First, it does not move or share raw data across sites or\nwith a centralized server during the model training process. Second, it uses a\ndifferential privacy mechanism to further protect the model from potential\nprivacy attacks. We perform a comprehensive evaluation of our approach on two\nhealthcare applications, using real-world electronic health data of 1 million\npatients. We demonstrate the feasibility and effectiveness of the federated\nlearning framework in offering an elevated level of privacy and maintaining\nutility of the global model.",
    "text": "Differential Privacy-enabled Federated Learning for Sensitive Health\n  Data\n\nLeveraging real-world health data for machine learning tasks requires\naddressing many practical challenges, such as distributed data silos, privacy\nconcerns with creating a centralized database from person-specific sensitive\ndata, resource constraints for transferring and integrating data from multiple\nsites, and risk of a single point of failure. In this paper, we introduce a\nfederated learning framework that can learn a global model from distributed\nhealth data held locally at different sites. The framework offers two levels of\nprivacy protection. First, it does not move or share raw data across sites or\nwith a centralized server during the model training process. Second, it uses a\ndifferential privacy mechanism to further protect the model from potential\nprivacy attacks. We perform a comprehensive evaluation of our approach on two\nhealthcare applications, using real-world electronic health data of 1 million\npatients. We demonstrate the feasibility and effectiveness of the federated\nlearning framework in offering an elevated level of privacy and maintaining\nutility of the global model."
  },
  {
    "id": "arxiv-190",
    "title": "Image Restoration Using Deep Regulated Convolutional Networks",
    "abstract": "While the depth of convolutional neural networks has attracted substantial\nattention in the deep learning research, the width of these networks has\nrecently received greater interest. The width of networks, defined as the size\nof the receptive fields and the density of the channels, has demonstrated\ncrucial importance in low-level vision tasks such as image denoising and\nrestoration. However, the limited generalization ability, due to the increased\nwidth of networks, creates a bottleneck in designing wider networks. In this\npaper, we propose the Deep Regulated Convolutional Network (RC-Net), a deep\nnetwork composed of regulated sub-network blocks cascaded by skip-connections,\nto overcome this bottleneck. Specifically, the Regulated Convolution block\n(RC-block), featured by a combination of large and small convolution filters,\nbalances the effectiveness of prominent feature extraction and the\ngeneralization ability of the network. RC-Nets have several compelling\nadvantages: they embrace diversified features through large-small filter\ncombinations, alleviate the hazy boundary and blurred details in image\ndenoising and super-resolution problems, and stabilize the learning process.\nOur proposed RC-Nets outperform state-of-the-art approaches with significant\nperformance gains in various image restoration tasks while demonstrating\npromising generalization ability. The code is available at\nhttps://github.com/cswin/RC-Nets.",
    "text": "Image Restoration Using Deep Regulated Convolutional Networks\n\nWhile the depth of convolutional neural networks has attracted substantial\nattention in the deep learning research, the width of these networks has\nrecently received greater interest. The width of networks, defined as the size\nof the receptive fields and the density of the channels, has demonstrated\ncrucial importance in low-level vision tasks such as image denoising and\nrestoration. However, the limited generalization ability, due to the increased\nwidth of networks, creates a bottleneck in designing wider networks. In this\npaper, we propose the Deep Regulated Convolutional Network (RC-Net), a deep\nnetwork composed of regulated sub-network blocks cascaded by skip-connections,\nto overcome this bottleneck. Specifically, the Regulated Convolution block\n(RC-block), featured by a combination of large and small convolution filters,\nbalances the effectiveness of prominent feature extraction and the\ngeneralization ability of the network. RC-Nets have several compelling\nadvantages: they embrace diversified features through large-small filter\ncombinations, alleviate the hazy boundary and blurred details in image\ndenoising and super-resolution problems, and stabilize the learning process.\nOur proposed RC-Nets outperform state-of-the-art approaches with significant\nperformance gains in various image restoration tasks while demonstrating\npromising generalization ability. The code is available at\nhttps://github.com/cswin/RC-Nets."
  },
  {
    "id": "arxiv-191",
    "title": "Moment Matching Deep Contrastive Latent Variable Models",
    "abstract": "In the contrastive analysis (CA) setting, machine learning practitioners are\nspecifically interested in discovering patterns that are enriched in a target\ndataset as compared to a background dataset generated from sources of variation\nirrelevant to the task at hand. For example, a biomedical data analyst may seek\nto understand variations in genomic data only present among patients with a\ngiven disease as opposed to those also present in healthy control subjects.\nSuch scenarios have motivated the development of contrastive latent variable\nmodels to isolate variations unique to these target datasets from those shared\nacross the target and background datasets, with current state of the art models\nbased on the variational autoencoder (VAE) framework. However, previously\nproposed models do not explicitly enforce the constraints on latent variables\nunderlying CA, potentially leading to the undesirable leakage of information\nbetween the two sets of latent variables. Here we propose the moment matching\ncontrastive VAE (MM-cVAE), a reformulation of the VAE for CA that uses the\nmaximum mean discrepancy to explicitly enforce two crucial latent variable\nconstraints underlying CA. On three challenging CA tasks we find that our\nmethod outperforms the previous state-of-the-art both qualitatively and on a\nset of quantitative metrics.",
    "text": "Moment Matching Deep Contrastive Latent Variable Models\n\nIn the contrastive analysis (CA) setting, machine learning practitioners are\nspecifically interested in discovering patterns that are enriched in a target\ndataset as compared to a background dataset generated from sources of variation\nirrelevant to the task at hand. For example, a biomedical data analyst may seek\nto understand variations in genomic data only present among patients with a\ngiven disease as opposed to those also present in healthy control subjects.\nSuch scenarios have motivated the development of contrastive latent variable\nmodels to isolate variations unique to these target datasets from those shared\nacross the target and background datasets, with current state of the art models\nbased on the variational autoencoder (VAE) framework. However, previously\nproposed models do not explicitly enforce the constraints on latent variables\nunderlying CA, potentially leading to the undesirable leakage of information\nbetween the two sets of latent variables. Here we propose the moment matching\ncontrastive VAE (MM-cVAE), a reformulation of the VAE for CA that uses the\nmaximum mean discrepancy to explicitly enforce two crucial latent variable\nconstraints underlying CA. On three challenging CA tasks we find that our\nmethod outperforms the previous state-of-the-art both qualitatively and on a\nset of quantitative metrics."
  },
  {
    "id": "arxiv-192",
    "title": "Decoupled coordinates for machine learning-based molecular fragment\n  linking",
    "abstract": "Recent developments in machine-learning based molecular fragment linking have\ndemonstrated the importance of informing the generation process with structural\ninformation specifying the relative orientation of the fragments to be linked.\nHowever, such structural information has not yet been provided in the form of a\ncomplete relative coordinate system. Mathematical details for a decoupled set\nof bond lengths, bond angles and torsion angles are elaborated and the\ncoordinate system is demonstrated to be complete. Significant impact on the\nquality of the generated linkers is demonstrated numerically. The amount of\nreliable information within the different types of degrees of freedom is\ninvestigated. Ablation studies and an information-theoretical analysis are\nperformed. The presented benefits suggest the application of a complete and\ndecoupled relative coordinate system as a standard good practice in linker\ndesign.",
    "text": "Decoupled coordinates for machine learning-based molecular fragment\n  linking\n\nRecent developments in machine-learning based molecular fragment linking have\ndemonstrated the importance of informing the generation process with structural\ninformation specifying the relative orientation of the fragments to be linked.\nHowever, such structural information has not yet been provided in the form of a\ncomplete relative coordinate system. Mathematical details for a decoupled set\nof bond lengths, bond angles and torsion angles are elaborated and the\ncoordinate system is demonstrated to be complete. Significant impact on the\nquality of the generated linkers is demonstrated numerically. The amount of\nreliable information within the different types of degrees of freedom is\ninvestigated. Ablation studies and an information-theoretical analysis are\nperformed. The presented benefits suggest the application of a complete and\ndecoupled relative coordinate system as a standard good practice in linker\ndesign."
  },
  {
    "id": "arxiv-193",
    "title": "Unsupervised Deep Feature Transfer for Low Resolution Image\n  Classification",
    "abstract": "In this paper, we propose a simple while effective unsupervised deep feature\ntransfer algorithm for low resolution image classification. No fine-tuning on\nconvenet filters is required in our method. We use pre-trained convenet to\nextract features for both high- and low-resolution images, and then feed them\ninto a two-layer feature transfer network for knowledge transfer. A SVM\nclassifier is learned directly using these transferred low resolution features.\nOur network can be embedded into the state-of-the-art deep neural networks as a\nplug-in feature enhancement module. It preserves data structures in feature\nspace for high resolution images, and transfers the distinguishing features\nfrom a well-structured source domain (high resolution features space) to a not\nwell-organized target domain (low resolution features space). Extensive\nexperiments on VOC2007 test set show that the proposed method achieves\nsignificant improvements over the baseline of using feature extraction.",
    "text": "Unsupervised Deep Feature Transfer for Low Resolution Image\n  Classification\n\nIn this paper, we propose a simple while effective unsupervised deep feature\ntransfer algorithm for low resolution image classification. No fine-tuning on\nconvenet filters is required in our method. We use pre-trained convenet to\nextract features for both high- and low-resolution images, and then feed them\ninto a two-layer feature transfer network for knowledge transfer. A SVM\nclassifier is learned directly using these transferred low resolution features.\nOur network can be embedded into the state-of-the-art deep neural networks as a\nplug-in feature enhancement module. It preserves data structures in feature\nspace for high resolution images, and transfers the distinguishing features\nfrom a well-structured source domain (high resolution features space) to a not\nwell-organized target domain (low resolution features space). Extensive\nexperiments on VOC2007 test set show that the proposed method achieves\nsignificant improvements over the baseline of using feature extraction."
  },
  {
    "id": "arxiv-194",
    "title": "Convolutional Neural Networks with Intermediate Loss for 3D\n  Super-Resolution of CT and MRI Scans",
    "abstract": "CT scanners that are commonly-used in hospitals nowadays produce\nlow-resolution images, up to 512 pixels in size. One pixel in the image\ncorresponds to a one millimeter piece of tissue. In order to accurately segment\ntumors and make treatment plans, doctors need CT scans of higher resolution.\nThe same problem appears in MRI. In this paper, we propose an approach for the\nsingle-image super-resolution of 3D CT or MRI scans. Our method is based on\ndeep convolutional neural networks (CNNs) composed of 10 convolutional layers\nand an intermediate upscaling layer that is placed after the first 6\nconvolutional layers. Our first CNN, which increases the resolution on two axes\n(width and height), is followed by a second CNN, which increases the resolution\non the third axis (depth). Different from other methods, we compute the loss\nwith respect to the ground-truth high-resolution output right after the\nupscaling layer, in addition to computing the loss after the last convolutional\nlayer. The intermediate loss forces our network to produce a better output,\ncloser to the ground-truth. A widely-used approach to obtain sharp results is\nto add Gaussian blur using a fixed standard deviation. In order to avoid\noverfitting to a fixed standard deviation, we apply Gaussian smoothing with\nvarious standard deviations, unlike other approaches. We evaluate our method in\nthe context of 2D and 3D super-resolution of CT and MRI scans from two\ndatabases, comparing it to relevant related works from the literature and\nbaselines based on various interpolation schemes, using 2x and 4x scaling\nfactors. The empirical results show that our approach attains superior results\nto all other methods. Moreover, our human annotation study reveals that both\ndoctors and regular annotators chose our method in favor of Lanczos\ninterpolation in 97.55% cases for 2x upscaling factor and in 96.69% cases for\n4x upscaling factor.",
    "text": "Convolutional Neural Networks with Intermediate Loss for 3D\n  Super-Resolution of CT and MRI Scans\n\nCT scanners that are commonly-used in hospitals nowadays produce\nlow-resolution images, up to 512 pixels in size. One pixel in the image\ncorresponds to a one millimeter piece of tissue. In order to accurately segment\ntumors and make treatment plans, doctors need CT scans of higher resolution.\nThe same problem appears in MRI. In this paper, we propose an approach for the\nsingle-image super-resolution of 3D CT or MRI scans. Our method is based on\ndeep convolutional neural networks (CNNs) composed of 10 convolutional layers\nand an intermediate upscaling layer that is placed after the first 6\nconvolutional layers. Our first CNN, which increases the resolution on two axes\n(width and height), is followed by a second CNN, which increases the resolution\non the third axis (depth). Different from other methods, we compute the loss\nwith respect to the ground-truth high-resolution output right after the\nupscaling layer, in addition to computing the loss after the last convolutional\nlayer. The intermediate loss forces our network to produce a better output,\ncloser to the ground-truth. A widely-used approach to obtain sharp results is\nto add Gaussian blur using a fixed standard deviation. In order to avoid\noverfitting to a fixed standard deviation, we apply Gaussian smoothing with\nvarious standard deviations, unlike other approaches. We evaluate our method in\nthe context of 2D and 3D super-resolution of CT and MRI scans from two\ndatabases, comparing it to relevant related works from the literature and\nbaselines based on various interpolation schemes, using 2x and 4x scaling\nfactors. The empirical results show that our approach attains superior results\nto all other methods. Moreover, our human annotation study reveals that both\ndoctors and regular annotators chose our method in favor of Lanczos\ninterpolation in 97.55% cases for 2x upscaling factor and in 96.69% cases for\n4x upscaling factor."
  },
  {
    "id": "arxiv-195",
    "title": "k2-means for fast and accurate large scale clustering",
    "abstract": "We propose k^2-means, a new clustering method which efficiently copes with\nlarge numbers of clusters and achieves low energy solutions. k^2-means builds\nupon the standard k-means (Lloyd's algorithm) and combines a new strategy to\naccelerate the convergence with a new low time complexity divisive\ninitialization. The accelerated convergence is achieved through only looking at\nk_n nearest clusters and using triangle inequality bounds in the assignment\nstep while the divisive initialization employs an optimal 2-clustering along a\ndirection. The worst-case time complexity per iteration of our k^2-means is\nO(nk_nd+k^2d), where d is the dimension of the n data points and k is the\nnumber of clusters and usually n << k << k_n. Compared to k-means' O(nkd)\ncomplexity, our k^2-means complexity is significantly lower, at the expense of\nslightly increasing the memory complexity by O(nk_n+k^2). In our extensive\nexperiments k^2-means is order(s) of magnitude faster than standard methods in\ncomputing accurate clusterings on several standard datasets and settings with\nhundreds of clusters and high dimensional data. Moreover, the proposed divisive\ninitialization generally leads to clustering energies comparable to those\nachieved with the standard k-means++ initialization, while being significantly\nfaster.",
    "text": "k2-means for fast and accurate large scale clustering\n\nWe propose k^2-means, a new clustering method which efficiently copes with\nlarge numbers of clusters and achieves low energy solutions. k^2-means builds\nupon the standard k-means (Lloyd's algorithm) and combines a new strategy to\naccelerate the convergence with a new low time complexity divisive\ninitialization. The accelerated convergence is achieved through only looking at\nk_n nearest clusters and using triangle inequality bounds in the assignment\nstep while the divisive initialization employs an optimal 2-clustering along a\ndirection. The worst-case time complexity per iteration of our k^2-means is\nO(nk_nd+k^2d), where d is the dimension of the n data points and k is the\nnumber of clusters and usually n << k << k_n. Compared to k-means' O(nkd)\ncomplexity, our k^2-means complexity is significantly lower, at the expense of\nslightly increasing the memory complexity by O(nk_n+k^2). In our extensive\nexperiments k^2-means is order(s) of magnitude faster than standard methods in\ncomputing accurate clusterings on several standard datasets and settings with\nhundreds of clusters and high dimensional data. Moreover, the proposed divisive\ninitialization generally leads to clustering energies comparable to those\nachieved with the standard k-means++ initialization, while being significantly\nfaster."
  },
  {
    "id": "arxiv-196",
    "title": "Importance sampling strategy for non-convex randomized block-coordinate\n  descent",
    "abstract": "As the number of samples and dimensionality of optimization problems related\nto statistics an machine learning explode, block coordinate descent algorithms\nhave gained popularity since they reduce the original problem to several\nsmaller ones. Coordinates to be optimized are usually selected randomly\naccording to a given probability distribution. We introduce an importance\nsampling strategy that helps randomized coordinate descent algorithms to focus\non blocks that are still far from convergence. The framework applies to\nproblems composed of the sum of two possibly non-convex terms, one being\nseparable and non-smooth. We have compared our algorithm to a full gradient\nproximal approach as well as to a randomized block coordinate algorithm that\nconsiders uniform sampling and cyclic block coordinate descent. Experimental\nevidences show the clear benefit of using an importance sampling strategy.",
    "text": "Importance sampling strategy for non-convex randomized block-coordinate\n  descent\n\nAs the number of samples and dimensionality of optimization problems related\nto statistics an machine learning explode, block coordinate descent algorithms\nhave gained popularity since they reduce the original problem to several\nsmaller ones. Coordinates to be optimized are usually selected randomly\naccording to a given probability distribution. We introduce an importance\nsampling strategy that helps randomized coordinate descent algorithms to focus\non blocks that are still far from convergence. The framework applies to\nproblems composed of the sum of two possibly non-convex terms, one being\nseparable and non-smooth. We have compared our algorithm to a full gradient\nproximal approach as well as to a randomized block coordinate algorithm that\nconsiders uniform sampling and cyclic block coordinate descent. Experimental\nevidences show the clear benefit of using an importance sampling strategy."
  },
  {
    "id": "arxiv-197",
    "title": "ST3D: Self-training for Unsupervised Domain Adaptation on 3D Object\n  Detection",
    "abstract": "We present a new domain adaptive self-training pipeline, named ST3D, for\nunsupervised domain adaptation on 3D object detection from point clouds. First,\nwe pre-train the 3D detector on the source domain with our proposed random\nobject scaling strategy for mitigating the negative effects of source domain\nbias. Then, the detector is iteratively improved on the target domain by\nalternatively conducting two steps, which are the pseudo label updating with\nthe developed quality-aware triplet memory bank and the model training with\ncurriculum data augmentation. These specific designs for 3D object detection\nenable the detector to be trained with consistent and high-quality pseudo\nlabels and to avoid overfitting to the large number of easy examples in pseudo\nlabeled data. Our ST3D achieves state-of-the-art performance on all evaluated\ndatasets and even surpasses fully supervised results on KITTI 3D object\ndetection benchmark. Code will be available at\nhttps://github.com/CVMI-Lab/ST3D.",
    "text": "ST3D: Self-training for Unsupervised Domain Adaptation on 3D Object\n  Detection\n\nWe present a new domain adaptive self-training pipeline, named ST3D, for\nunsupervised domain adaptation on 3D object detection from point clouds. First,\nwe pre-train the 3D detector on the source domain with our proposed random\nobject scaling strategy for mitigating the negative effects of source domain\nbias. Then, the detector is iteratively improved on the target domain by\nalternatively conducting two steps, which are the pseudo label updating with\nthe developed quality-aware triplet memory bank and the model training with\ncurriculum data augmentation. These specific designs for 3D object detection\nenable the detector to be trained with consistent and high-quality pseudo\nlabels and to avoid overfitting to the large number of easy examples in pseudo\nlabeled data. Our ST3D achieves state-of-the-art performance on all evaluated\ndatasets and even surpasses fully supervised results on KITTI 3D object\ndetection benchmark. Code will be available at\nhttps://github.com/CVMI-Lab/ST3D."
  },
  {
    "id": "arxiv-198",
    "title": "Stacking With Auxiliary Features",
    "abstract": "Ensembling methods are well known for improving prediction accuracy. However,\nthey are limited in the sense that they cannot discriminate among component\nmodels effectively. In this paper, we propose stacking with auxiliary features\nthat learns to fuse relevant information from multiple systems to improve\nperformance. Auxiliary features enable the stacker to rely on systems that not\njust agree on an output but also the provenance of the output. We demonstrate\nour approach on three very different and difficult problems -- the Cold Start\nSlot Filling, the Tri-lingual Entity Discovery and Linking and the ImageNet\nobject detection tasks. We obtain new state-of-the-art results on the first two\ntasks and substantial improvements on the detection task, thus verifying the\npower and generality of our approach.",
    "text": "Stacking With Auxiliary Features\n\nEnsembling methods are well known for improving prediction accuracy. However,\nthey are limited in the sense that they cannot discriminate among component\nmodels effectively. In this paper, we propose stacking with auxiliary features\nthat learns to fuse relevant information from multiple systems to improve\nperformance. Auxiliary features enable the stacker to rely on systems that not\njust agree on an output but also the provenance of the output. We demonstrate\nour approach on three very different and difficult problems -- the Cold Start\nSlot Filling, the Tri-lingual Entity Discovery and Linking and the ImageNet\nobject detection tasks. We obtain new state-of-the-art results on the first two\ntasks and substantial improvements on the detection task, thus verifying the\npower and generality of our approach."
  },
  {
    "id": "arxiv-199",
    "title": "Convergence Rates of Active Learning for Maximum Likelihood Estimation",
    "abstract": "An active learner is given a class of models, a large set of unlabeled\nexamples, and the ability to interactively query labels of a subset of these\nexamples; the goal of the learner is to learn a model in the class that fits\nthe data well.\n  Previous theoretical work has rigorously characterized label complexity of\nactive learning, but most of this work has focused on the PAC or the agnostic\nPAC model. In this paper, we shift our attention to a more general setting --\nmaximum likelihood estimation. Provided certain conditions hold on the model\nclass, we provide a two-stage active learning algorithm for this problem. The\nconditions we require are fairly general, and cover the widely popular class of\nGeneralized Linear Models, which in turn, include models for binary and\nmulti-class classification, regression, and conditional random fields.\n  We provide an upper bound on the label requirement of our algorithm, and a\nlower bound that matches it up to lower order terms. Our analysis shows that\nunlike binary classification in the realizable case, just a single extra round\nof interaction is sufficient to achieve near-optimal performance in maximum\nlikelihood estimation. On the empirical side, the recent work in\n~\\cite{Zhang12} and~\\cite{Zhang14} (on active linear and logistic regression)\nshows the promise of this approach.",
    "text": "Convergence Rates of Active Learning for Maximum Likelihood Estimation\n\nAn active learner is given a class of models, a large set of unlabeled\nexamples, and the ability to interactively query labels of a subset of these\nexamples; the goal of the learner is to learn a model in the class that fits\nthe data well.\n  Previous theoretical work has rigorously characterized label complexity of\nactive learning, but most of this work has focused on the PAC or the agnostic\nPAC model. In this paper, we shift our attention to a more general setting --\nmaximum likelihood estimation. Provided certain conditions hold on the model\nclass, we provide a two-stage active learning algorithm for this problem. The\nconditions we require are fairly general, and cover the widely popular class of\nGeneralized Linear Models, which in turn, include models for binary and\nmulti-class classification, regression, and conditional random fields.\n  We provide an upper bound on the label requirement of our algorithm, and a\nlower bound that matches it up to lower order terms. Our analysis shows that\nunlike binary classification in the realizable case, just a single extra round\nof interaction is sufficient to achieve near-optimal performance in maximum\nlikelihood estimation. On the empirical side, the recent work in\n~\\cite{Zhang12} and~\\cite{Zhang14} (on active linear and logistic regression)\nshows the promise of this approach."
  },
  {
    "id": "arxiv-200",
    "title": "Off-Policy Actor-Critic with Emphatic Weightings",
    "abstract": "A variety of theoretically-sound policy gradient algorithms exist for the\non-policy setting due to the policy gradient theorem, which provides a\nsimplified form for the gradient. The off-policy setting, however, has been\nless clear due to the existence of multiple objectives and the lack of an\nexplicit off-policy policy gradient theorem. In this work, we unify these\nobjectives into one off-policy objective, and provide a policy gradient theorem\nfor this unified objective. The derivation involves emphatic weightings and\ninterest functions. We show multiple strategies to approximate the gradients,\nin an algorithm called Actor Critic with Emphatic weightings (ACE). We prove in\na counterexample that previous (semi-gradient) off-policy actor-critic\nmethods--particularly OffPAC and DPG--converge to the wrong solution whereas\nACE finds the optimal solution. We also highlight why these semi-gradient\napproaches can still perform well in practice, suggesting strategies for\nvariance reduction in ACE. We empirically study several variants of ACE on two\nclassic control environments and an image-based environment designed to\nillustrate the tradeoffs made by each gradient approximation. We find that by\napproximating the emphatic weightings directly, ACE performs as well as or\nbetter than OffPAC in all settings tested.",
    "text": "Off-Policy Actor-Critic with Emphatic Weightings\n\nA variety of theoretically-sound policy gradient algorithms exist for the\non-policy setting due to the policy gradient theorem, which provides a\nsimplified form for the gradient. The off-policy setting, however, has been\nless clear due to the existence of multiple objectives and the lack of an\nexplicit off-policy policy gradient theorem. In this work, we unify these\nobjectives into one off-policy objective, and provide a policy gradient theorem\nfor this unified objective. The derivation involves emphatic weightings and\ninterest functions. We show multiple strategies to approximate the gradients,\nin an algorithm called Actor Critic with Emphatic weightings (ACE). We prove in\na counterexample that previous (semi-gradient) off-policy actor-critic\nmethods--particularly OffPAC and DPG--converge to the wrong solution whereas\nACE finds the optimal solution. We also highlight why these semi-gradient\napproaches can still perform well in practice, suggesting strategies for\nvariance reduction in ACE. We empirically study several variants of ACE on two\nclassic control environments and an image-based environment designed to\nillustrate the tradeoffs made by each gradient approximation. We find that by\napproximating the emphatic weightings directly, ACE performs as well as or\nbetter than OffPAC in all settings tested."
  },
  {
    "id": "arxiv-201",
    "title": "Top-N: Equivariant set and graph generation without exchangeability",
    "abstract": "This work addresses one-shot set and graph generation, and, more\nspecifically, the parametrization of probabilistic decoders that map a\nvector-shaped prior to a distribution over sets or graphs. Sets and graphs are\nmost commonly generated by first sampling points i.i.d. from a normal\ndistribution, and then processing these points along with the prior vector\nusing Transformer layers or Graph Neural Networks. This architecture is\ndesigned to generate exchangeable distributions, i.e., all permutations of the\ngenerated outputs are equally likely. We however show that it only optimizes a\nproxy to the evidence lower bound, which makes it hard to train. We then study\nequivariance in generative settings and show that non-exchangeable methods can\nstill achieve permutation equivariance. Using this result, we introduce Top-n\ncreation, a differentiable generation mechanism that uses the latent vector to\nselect the most relevant points from a trainable reference set. Top-n can\nreplace i.i.d. generation in any Variational Autoencoder or Generative\nAdversarial Network. Experimentally, our method outperforms i.i.d. generation\nby 15% at SetMNIST reconstruction, by 33% at object detection on CLEVR,\ngenerates sets that are 74% closer to the true distribution on a synthetic\nmolecule-like dataset, and generates more valid molecules on QM9.",
    "text": "Top-N: Equivariant set and graph generation without exchangeability\n\nThis work addresses one-shot set and graph generation, and, more\nspecifically, the parametrization of probabilistic decoders that map a\nvector-shaped prior to a distribution over sets or graphs. Sets and graphs are\nmost commonly generated by first sampling points i.i.d. from a normal\ndistribution, and then processing these points along with the prior vector\nusing Transformer layers or Graph Neural Networks. This architecture is\ndesigned to generate exchangeable distributions, i.e., all permutations of the\ngenerated outputs are equally likely. We however show that it only optimizes a\nproxy to the evidence lower bound, which makes it hard to train. We then study\nequivariance in generative settings and show that non-exchangeable methods can\nstill achieve permutation equivariance. Using this result, we introduce Top-n\ncreation, a differentiable generation mechanism that uses the latent vector to\nselect the most relevant points from a trainable reference set. Top-n can\nreplace i.i.d. generation in any Variational Autoencoder or Generative\nAdversarial Network. Experimentally, our method outperforms i.i.d. generation\nby 15% at SetMNIST reconstruction, by 33% at object detection on CLEVR,\ngenerates sets that are 74% closer to the true distribution on a synthetic\nmolecule-like dataset, and generates more valid molecules on QM9."
  },
  {
    "id": "arxiv-202",
    "title": "PoNet: Pooling Network for Efficient Token Mixing in Long Sequences",
    "abstract": "Transformer-based models have achieved great success in various NLP, vision,\nand speech tasks. However, the core of Transformer, the self-attention\nmechanism, has a quadratic time and memory complexity with respect to the\nsequence length, which hinders applications of Transformer-based models to long\nsequences. Many approaches have been proposed to mitigate this problem, such as\nsparse attention mechanisms, low-rank matrix approximations and scalable\nkernels, and token mixing alternatives to self-attention. We propose a novel\nPooling Network (PoNet) for token mixing in long sequences with linear\ncomplexity. We design multi-granularity pooling and pooling fusion to capture\ndifferent levels of contextual information and combine their interactions with\ntokens. On the Long Range Arena benchmark, PoNet significantly outperforms\nTransformer and achieves competitive accuracy, while being only slightly slower\nthan the fastest model, FNet, across all sequence lengths measured on GPUs. We\nalso conduct systematic studies on the transfer learning capability of PoNet\nand observe that PoNet achieves 95.7% of the accuracy of BERT on the GLUE\nbenchmark, outperforming FNet by 4.5% relative. Comprehensive ablation analysis\ndemonstrates effectiveness of the designed multi-granularity pooling and\npooling fusion for token mixing in long sequences and efficacy of the designed\npre-training tasks for PoNet to learn transferable contextualized language\nrepresentations.",
    "text": "PoNet: Pooling Network for Efficient Token Mixing in Long Sequences\n\nTransformer-based models have achieved great success in various NLP, vision,\nand speech tasks. However, the core of Transformer, the self-attention\nmechanism, has a quadratic time and memory complexity with respect to the\nsequence length, which hinders applications of Transformer-based models to long\nsequences. Many approaches have been proposed to mitigate this problem, such as\nsparse attention mechanisms, low-rank matrix approximations and scalable\nkernels, and token mixing alternatives to self-attention. We propose a novel\nPooling Network (PoNet) for token mixing in long sequences with linear\ncomplexity. We design multi-granularity pooling and pooling fusion to capture\ndifferent levels of contextual information and combine their interactions with\ntokens. On the Long Range Arena benchmark, PoNet significantly outperforms\nTransformer and achieves competitive accuracy, while being only slightly slower\nthan the fastest model, FNet, across all sequence lengths measured on GPUs. We\nalso conduct systematic studies on the transfer learning capability of PoNet\nand observe that PoNet achieves 95.7% of the accuracy of BERT on the GLUE\nbenchmark, outperforming FNet by 4.5% relative. Comprehensive ablation analysis\ndemonstrates effectiveness of the designed multi-granularity pooling and\npooling fusion for token mixing in long sequences and efficacy of the designed\npre-training tasks for PoNet to learn transferable contextualized language\nrepresentations."
  },
  {
    "id": "arxiv-203",
    "title": "Imaging Time-Series to Improve Classification and Imputation",
    "abstract": "Inspired by recent successes of deep learning in computer vision, we propose\na novel framework for encoding time series as different types of images,\nnamely, Gramian Angular Summation/Difference Fields (GASF/GADF) and Markov\nTransition Fields (MTF). This enables the use of techniques from computer\nvision for time series classification and imputation. We used Tiled\nConvolutional Neural Networks (tiled CNNs) on 20 standard datasets to learn\nhigh-level features from the individual and compound GASF-GADF-MTF images. Our\napproaches achieve highly competitive results when compared to nine of the\ncurrent best time series classification approaches. Inspired by the bijection\nproperty of GASF on 0/1 rescaled data, we train Denoised Auto-encoders (DA) on\nthe GASF images of four standard and one synthesized compound dataset. The\nimputation MSE on test data is reduced by 12.18%-48.02% when compared to using\nthe raw data. An analysis of the features and weights learned via tiled CNNs\nand DAs explains why the approaches work.",
    "text": "Imaging Time-Series to Improve Classification and Imputation\n\nInspired by recent successes of deep learning in computer vision, we propose\na novel framework for encoding time series as different types of images,\nnamely, Gramian Angular Summation/Difference Fields (GASF/GADF) and Markov\nTransition Fields (MTF). This enables the use of techniques from computer\nvision for time series classification and imputation. We used Tiled\nConvolutional Neural Networks (tiled CNNs) on 20 standard datasets to learn\nhigh-level features from the individual and compound GASF-GADF-MTF images. Our\napproaches achieve highly competitive results when compared to nine of the\ncurrent best time series classification approaches. Inspired by the bijection\nproperty of GASF on 0/1 rescaled data, we train Denoised Auto-encoders (DA) on\nthe GASF images of four standard and one synthesized compound dataset. The\nimputation MSE on test data is reduced by 12.18%-48.02% when compared to using\nthe raw data. An analysis of the features and weights learned via tiled CNNs\nand DAs explains why the approaches work."
  },
  {
    "id": "arxiv-204",
    "title": "On Training Implicit Models",
    "abstract": "This paper focuses on training implicit models of infinite layers.\nSpecifically, previous works employ implicit differentiation and solve the\nexact gradient for the backward propagation. However, is it necessary to\ncompute such an exact but expensive gradient for training? In this work, we\npropose a novel gradient estimate for implicit models, named phantom gradient,\nthat 1) forgoes the costly computation of the exact gradient; and 2) provides\nan update direction empirically preferable to the implicit model training. We\ntheoretically analyze the condition under which an ascent direction of the loss\nlandscape could be found, and provide two specific instantiations of the\nphantom gradient based on the damped unrolling and Neumann series. Experiments\non large-scale tasks demonstrate that these lightweight phantom gradients\nsignificantly accelerate the backward passes in training implicit models by\nroughly 1.7 times, and even boost the performance over approaches based on the\nexact gradient on ImageNet.",
    "text": "On Training Implicit Models\n\nThis paper focuses on training implicit models of infinite layers.\nSpecifically, previous works employ implicit differentiation and solve the\nexact gradient for the backward propagation. However, is it necessary to\ncompute such an exact but expensive gradient for training? In this work, we\npropose a novel gradient estimate for implicit models, named phantom gradient,\nthat 1) forgoes the costly computation of the exact gradient; and 2) provides\nan update direction empirically preferable to the implicit model training. We\ntheoretically analyze the condition under which an ascent direction of the loss\nlandscape could be found, and provide two specific instantiations of the\nphantom gradient based on the damped unrolling and Neumann series. Experiments\non large-scale tasks demonstrate that these lightweight phantom gradients\nsignificantly accelerate the backward passes in training implicit models by\nroughly 1.7 times, and even boost the performance over approaches based on the\nexact gradient on ImageNet."
  },
  {
    "id": "arxiv-205",
    "title": "End-To-End Graph-based Deep Semi-Supervised Learning",
    "abstract": "The quality of a graph is determined jointly by three key factors of the\ngraph: nodes, edges and similarity measure (or edge weights), and is very\ncrucial to the success of graph-based semi-supervised learning (SSL)\napproaches. Recently, dynamic graph, which means part/all its factors are\ndynamically updated during the training process, has demonstrated to be\npromising for graph-based semi-supervised learning. However, existing\napproaches only update part of the three factors and keep the rest manually\nspecified during the learning stage. In this paper, we propose a novel\ngraph-based semi-supervised learning approach to optimize all three factors\nsimultaneously in an end-to-end learning fashion. To this end, we concatenate\ntwo neural networks (feature network and similarity network) together to learn\nthe categorical label and semantic similarity, respectively, and train the\nnetworks to minimize a unified SSL objective function. We also introduce an\nextended graph Laplacian regularization term to increase training efficiency.\nExtensive experiments on several benchmark datasets demonstrate the\neffectiveness of our approach.",
    "text": "End-To-End Graph-based Deep Semi-Supervised Learning\n\nThe quality of a graph is determined jointly by three key factors of the\ngraph: nodes, edges and similarity measure (or edge weights), and is very\ncrucial to the success of graph-based semi-supervised learning (SSL)\napproaches. Recently, dynamic graph, which means part/all its factors are\ndynamically updated during the training process, has demonstrated to be\npromising for graph-based semi-supervised learning. However, existing\napproaches only update part of the three factors and keep the rest manually\nspecified during the learning stage. In this paper, we propose a novel\ngraph-based semi-supervised learning approach to optimize all three factors\nsimultaneously in an end-to-end learning fashion. To this end, we concatenate\ntwo neural networks (feature network and similarity network) together to learn\nthe categorical label and semantic similarity, respectively, and train the\nnetworks to minimize a unified SSL objective function. We also introduce an\nextended graph Laplacian regularization term to increase training efficiency.\nExtensive experiments on several benchmark datasets demonstrate the\neffectiveness of our approach."
  },
  {
    "id": "arxiv-206",
    "title": "Transformer-based Machine Learning for Fast SAT Solvers and Logic\n  Synthesis",
    "abstract": "CNF-based SAT and MaxSAT solvers are central to logic synthesis and\nverification systems. The increasing popularity of these constraint problems in\nelectronic design automation encourages studies on different SAT problems and\ntheir properties for further computational efficiency. There has been both\ntheoretical and practical success of modern Conflict-driven clause learning SAT\nsolvers, which allows solving very large industrial instances in a relatively\nshort amount of time. Recently, machine learning approaches provide a new\ndimension to solving this challenging problem. Neural symbolic models could\nserve as generic solvers that can be specialized for specific domains based on\ndata without any changes to the structure of the model. In this work, we\npropose a one-shot model derived from the Transformer architecture to solve the\nMaxSAT problem, which is the optimization version of SAT where the goal is to\nsatisfy the maximum number of clauses. Our model has a scale-free structure\nwhich could process varying size of instances. We use meta-path and\nself-attention mechanism to capture interactions among homogeneous nodes. We\nadopt cross-attention mechanisms on the bipartite graph to capture interactions\namong heterogeneous nodes. We further apply an iterative algorithm to our model\nto satisfy additional clauses, enabling a solution approaching that of an\nexact-SAT problem. The attention mechanisms leverage the parallelism for\nspeedup. Our evaluation indicates improved speedup compared to heuristic\napproaches and improved completion rate compared to machine learning\napproaches.",
    "text": "Transformer-based Machine Learning for Fast SAT Solvers and Logic\n  Synthesis\n\nCNF-based SAT and MaxSAT solvers are central to logic synthesis and\nverification systems. The increasing popularity of these constraint problems in\nelectronic design automation encourages studies on different SAT problems and\ntheir properties for further computational efficiency. There has been both\ntheoretical and practical success of modern Conflict-driven clause learning SAT\nsolvers, which allows solving very large industrial instances in a relatively\nshort amount of time. Recently, machine learning approaches provide a new\ndimension to solving this challenging problem. Neural symbolic models could\nserve as generic solvers that can be specialized for specific domains based on\ndata without any changes to the structure of the model. In this work, we\npropose a one-shot model derived from the Transformer architecture to solve the\nMaxSAT problem, which is the optimization version of SAT where the goal is to\nsatisfy the maximum number of clauses. Our model has a scale-free structure\nwhich could process varying size of instances. We use meta-path and\nself-attention mechanism to capture interactions among homogeneous nodes. We\nadopt cross-attention mechanisms on the bipartite graph to capture interactions\namong heterogeneous nodes. We further apply an iterative algorithm to our model\nto satisfy additional clauses, enabling a solution approaching that of an\nexact-SAT problem. The attention mechanisms leverage the parallelism for\nspeedup. Our evaluation indicates improved speedup compared to heuristic\napproaches and improved completion rate compared to machine learning\napproaches."
  },
  {
    "id": "arxiv-207",
    "title": "INSIGHT-1 at SemEval-2016 Task 5: Deep Learning for Multilingual\n  Aspect-based Sentiment Analysis",
    "abstract": "This paper describes our deep learning-based approach to multilingual\naspect-based sentiment analysis as part of SemEval 2016 Task 5. We use a\nconvolutional neural network (CNN) for both aspect extraction and aspect-based\nsentiment analysis. We cast aspect extraction as a multi-label classification\nproblem, outputting probabilities over aspects parameterized by a threshold. To\ndetermine the sentiment towards an aspect, we concatenate an aspect vector with\nevery word embedding and apply a convolution over it. Our constrained system\n(unconstrained for English) achieves competitive results across all languages\nand domains, placing first or second in 5 and 7 out of 11 language-domain pairs\nfor aspect category detection (slot 1) and sentiment polarity (slot 3)\nrespectively, thereby demonstrating the viability of a deep learning-based\napproach for multilingual aspect-based sentiment analysis.",
    "text": "INSIGHT-1 at SemEval-2016 Task 5: Deep Learning for Multilingual\n  Aspect-based Sentiment Analysis\n\nThis paper describes our deep learning-based approach to multilingual\naspect-based sentiment analysis as part of SemEval 2016 Task 5. We use a\nconvolutional neural network (CNN) for both aspect extraction and aspect-based\nsentiment analysis. We cast aspect extraction as a multi-label classification\nproblem, outputting probabilities over aspects parameterized by a threshold. To\ndetermine the sentiment towards an aspect, we concatenate an aspect vector with\nevery word embedding and apply a convolution over it. Our constrained system\n(unconstrained for English) achieves competitive results across all languages\nand domains, placing first or second in 5 and 7 out of 11 language-domain pairs\nfor aspect category detection (slot 1) and sentiment polarity (slot 3)\nrespectively, thereby demonstrating the viability of a deep learning-based\napproach for multilingual aspect-based sentiment analysis."
  },
  {
    "id": "arxiv-208",
    "title": "Robustness of Quantum-Enhanced Adaptive Phase Estimation",
    "abstract": "As all physical adaptive quantum-enhanced metrology schemes operate under\nnoisy conditions with only partially understood noise characteristics, so a\npractical control policy must be robust even for unknown noise. We aim to\ndevise a test to evaluate the robustness of AQEM policies and assess the\nresource used by the policies. The robustness test is performed on QEAPE by\nsimulating the scheme under four phase-noise models corresponding to\nnormal-distribution noise, random-telegraph noise, skew-normal-distribution\nnoise, and log-normal-distribution noise. Control policies are devised either\nby an evolutionary algorithm under the same noisy conditions, albeit ignorant\nof its properties, or a Bayesian-based feedback method that assumes no noise.\nOur robustness test and resource comparison method can be used to determining\nthe efficacy and selecting a suitable policy.",
    "text": "Robustness of Quantum-Enhanced Adaptive Phase Estimation\n\nAs all physical adaptive quantum-enhanced metrology schemes operate under\nnoisy conditions with only partially understood noise characteristics, so a\npractical control policy must be robust even for unknown noise. We aim to\ndevise a test to evaluate the robustness of AQEM policies and assess the\nresource used by the policies. The robustness test is performed on QEAPE by\nsimulating the scheme under four phase-noise models corresponding to\nnormal-distribution noise, random-telegraph noise, skew-normal-distribution\nnoise, and log-normal-distribution noise. Control policies are devised either\nby an evolutionary algorithm under the same noisy conditions, albeit ignorant\nof its properties, or a Bayesian-based feedback method that assumes no noise.\nOur robustness test and resource comparison method can be used to determining\nthe efficacy and selecting a suitable policy."
  },
  {
    "id": "arxiv-209",
    "title": "Universal Graph Transformer Self-Attention Networks",
    "abstract": "We introduce a transformer-based GNN model, named UGformer, to learn graph\nrepresentations. In particular, we present two UGformer variants, wherein the\nfirst variant (publicized in September 2019) is to leverage the transformer on\na set of sampled neighbors for each input node, while the second (publicized in\nMay 2021) is to leverage the transformer on all input nodes. Experimental\nresults demonstrate that the first UGformer variant achieves state-of-the-art\naccuracies on benchmark datasets for graph classification in both inductive\nsetting and unsupervised transductive setting; and the second UGformer variant\nobtains state-of-the-art accuracies for inductive text classification. The code\nis available at: \\url{https://github.com/daiquocnguyen/Graph-Transformer}.",
    "text": "Universal Graph Transformer Self-Attention Networks\n\nWe introduce a transformer-based GNN model, named UGformer, to learn graph\nrepresentations. In particular, we present two UGformer variants, wherein the\nfirst variant (publicized in September 2019) is to leverage the transformer on\na set of sampled neighbors for each input node, while the second (publicized in\nMay 2021) is to leverage the transformer on all input nodes. Experimental\nresults demonstrate that the first UGformer variant achieves state-of-the-art\naccuracies on benchmark datasets for graph classification in both inductive\nsetting and unsupervised transductive setting; and the second UGformer variant\nobtains state-of-the-art accuracies for inductive text classification. The code\nis available at: \\url{https://github.com/daiquocnguyen/Graph-Transformer}."
  },
  {
    "id": "arxiv-210",
    "title": "Deep Reinforcement Q-Learning for Intelligent Traffic Signal Control\n  with Partial Detection",
    "abstract": "Intelligent traffic signal controllers, applying DQN algorithms to traffic\nlight policy optimization, efficiently reduce traffic congestion by adjusting\ntraffic signals to real-time traffic. Most propositions in the literature\nhowever consider that all vehicles at the intersection are detected, an\nunrealistic scenario. Recently, new wireless communication technologies have\nenabled cost-efficient detection of connected vehicles by infrastructures. With\nonly a small fraction of the total fleet currently equipped, methods able to\nperform under low detection rates are desirable. In this paper, we propose a\ndeep reinforcement Q-learning model to optimize traffic signal control at an\nisolated intersection, in a partially observable environment with connected\nvehicles. First, we present the novel DQN model within the RL framework. We\nintroduce a new state representation for partially observable environments and\na new reward function for traffic signal control, and provide a network\narchitecture and tuned hyper-parameters. Second, we evaluate the performances\nof the model in numerical simulations on multiple scenarios, in two steps. At\nfirst in full detection against existing actuated controllers, then in partial\ndetection with loss estimates for proportions of connected vehicles. Finally,\nfrom the obtained results, we define thresholds for detection rates with\nacceptable and optimal performance levels.",
    "text": "Deep Reinforcement Q-Learning for Intelligent Traffic Signal Control\n  with Partial Detection\n\nIntelligent traffic signal controllers, applying DQN algorithms to traffic\nlight policy optimization, efficiently reduce traffic congestion by adjusting\ntraffic signals to real-time traffic. Most propositions in the literature\nhowever consider that all vehicles at the intersection are detected, an\nunrealistic scenario. Recently, new wireless communication technologies have\nenabled cost-efficient detection of connected vehicles by infrastructures. With\nonly a small fraction of the total fleet currently equipped, methods able to\nperform under low detection rates are desirable. In this paper, we propose a\ndeep reinforcement Q-learning model to optimize traffic signal control at an\nisolated intersection, in a partially observable environment with connected\nvehicles. First, we present the novel DQN model within the RL framework. We\nintroduce a new state representation for partially observable environments and\na new reward function for traffic signal control, and provide a network\narchitecture and tuned hyper-parameters. Second, we evaluate the performances\nof the model in numerical simulations on multiple scenarios, in two steps. At\nfirst in full detection against existing actuated controllers, then in partial\ndetection with loss estimates for proportions of connected vehicles. Finally,\nfrom the obtained results, we define thresholds for detection rates with\nacceptable and optimal performance levels."
  },
  {
    "id": "arxiv-211",
    "title": "Object landmark discovery through unsupervised adaptation",
    "abstract": "This paper proposes a method to ease the unsupervised learning of object\nlandmark detectors. Similarly to previous methods, our approach is fully\nunsupervised in a sense that it does not require or make any use of annotated\nlandmarks for the target object category. Contrary to previous works, we do\nhowever assume that a landmark detector, which has already learned a structured\nrepresentation for a given object category in a fully supervised manner, is\navailable. Under this setting, our main idea boils down to adapting the given\npre-trained network to the target object categories in a fully unsupervised\nmanner. To this end, our method uses the pre-trained network as a core which\nremains frozen and does not get updated during training, and learns, in an\nunsupervised manner, only a projection matrix to perform the adaptation to the\ntarget categories. By building upon an existing structured representation\nlearned in a supervised manner, the optimization problem solved by our method\nis much more constrained with significantly less parameters to learn which\nseems to be important for the case of unsupervised learning. We show that our\nmethod surpasses fully unsupervised techniques trained from scratch as well as\na strong baseline based on fine-tuning, and produces state-of-the-art results\non several datasets. Code can be found at\nhttps://github.com/ESanchezLozano/SAIC-Unsupervised-landmark-detection-NeurIPS2019 .",
    "text": "Object landmark discovery through unsupervised adaptation\n\nThis paper proposes a method to ease the unsupervised learning of object\nlandmark detectors. Similarly to previous methods, our approach is fully\nunsupervised in a sense that it does not require or make any use of annotated\nlandmarks for the target object category. Contrary to previous works, we do\nhowever assume that a landmark detector, which has already learned a structured\nrepresentation for a given object category in a fully supervised manner, is\navailable. Under this setting, our main idea boils down to adapting the given\npre-trained network to the target object categories in a fully unsupervised\nmanner. To this end, our method uses the pre-trained network as a core which\nremains frozen and does not get updated during training, and learns, in an\nunsupervised manner, only a projection matrix to perform the adaptation to the\ntarget categories. By building upon an existing structured representation\nlearned in a supervised manner, the optimization problem solved by our method\nis much more constrained with significantly less parameters to learn which\nseems to be important for the case of unsupervised learning. We show that our\nmethod surpasses fully unsupervised techniques trained from scratch as well as\na strong baseline based on fine-tuning, and produces state-of-the-art results\non several datasets. Code can be found at\nhttps://github.com/ESanchezLozano/SAIC-Unsupervised-landmark-detection-NeurIPS2019 ."
  },
  {
    "id": "arxiv-212",
    "title": "Inner Cell Mass and Trophectoderm Segmentation in Human Blastocyst\n  Images using Deep Neural Network",
    "abstract": "Embryo quality assessment based on morphological attributes is important for\nachieving higher pregnancy rates from in vitro fertilization (IVF). The\naccurate segmentation of the embryo's inner cell mass (ICM) and trophectoderm\nepithelium (TE) is important, as these parameters can help to predict the\nembryo viability and live birth potential. However, segmentation of the ICM and\nTE is difficult due to variations in their shape and similarities in their\ntextures, both with each other and with their surroundings. To tackle this\nproblem, a deep neural network (DNN) based segmentation approach was\nimplemented. The DNN can identify the ICM region with 99.1% accuracy, 94.9%\nprecision, 93.8% recall, a 94.3% Dice Coefficient, and a 89.3% Jaccard Index.\nIt can extract the TE region with 98.3% accuracy, 91.8% precision, 93.2%\nrecall, a 92.5% Dice Coefficient, and a 85.3% Jaccard Index.",
    "text": "Inner Cell Mass and Trophectoderm Segmentation in Human Blastocyst\n  Images using Deep Neural Network\n\nEmbryo quality assessment based on morphological attributes is important for\nachieving higher pregnancy rates from in vitro fertilization (IVF). The\naccurate segmentation of the embryo's inner cell mass (ICM) and trophectoderm\nepithelium (TE) is important, as these parameters can help to predict the\nembryo viability and live birth potential. However, segmentation of the ICM and\nTE is difficult due to variations in their shape and similarities in their\ntextures, both with each other and with their surroundings. To tackle this\nproblem, a deep neural network (DNN) based segmentation approach was\nimplemented. The DNN can identify the ICM region with 99.1% accuracy, 94.9%\nprecision, 93.8% recall, a 94.3% Dice Coefficient, and a 89.3% Jaccard Index.\nIt can extract the TE region with 98.3% accuracy, 91.8% precision, 93.2%\nrecall, a 92.5% Dice Coefficient, and a 85.3% Jaccard Index."
  },
  {
    "id": "arxiv-213",
    "title": "Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval",
    "abstract": "Retrieval-based language models (R-LM) model the probability of natural\nlanguage text by combining a standard language model (LM) with examples\nretrieved from an external datastore at test time. While effective, a major\nbottleneck of using these models in practice is the computationally costly\ndatastore search, which can be performed as frequently as every time step. In\nthis paper, we present RetoMaton - retrieval automaton - which approximates the\ndatastore search, based on (1) saving pointers between consecutive datastore\nentries, and (2) clustering of entries into \"states\". This effectively results\nin a weighted finite automaton built on top of the datastore, instead of\nrepresenting the datastore as a flat list. The creation of the automaton is\nunsupervised, and a RetoMaton can be constructed from any text collection:\neither the original training corpus or from another domain. Traversing this\nautomaton at inference time, in parallel to the LM inference, reduces its\nperplexity by up to 1.85, or alternatively saves up to 83% of the nearest\nneighbor searches over $k$NN-LM (Khandelwal et al., 2020) without hurting\nperplexity. Our code and trained models are available at\nhttps://github.com/neulab/retomaton .",
    "text": "Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval\n\nRetrieval-based language models (R-LM) model the probability of natural\nlanguage text by combining a standard language model (LM) with examples\nretrieved from an external datastore at test time. While effective, a major\nbottleneck of using these models in practice is the computationally costly\ndatastore search, which can be performed as frequently as every time step. In\nthis paper, we present RetoMaton - retrieval automaton - which approximates the\ndatastore search, based on (1) saving pointers between consecutive datastore\nentries, and (2) clustering of entries into \"states\". This effectively results\nin a weighted finite automaton built on top of the datastore, instead of\nrepresenting the datastore as a flat list. The creation of the automaton is\nunsupervised, and a RetoMaton can be constructed from any text collection:\neither the original training corpus or from another domain. Traversing this\nautomaton at inference time, in parallel to the LM inference, reduces its\nperplexity by up to 1.85, or alternatively saves up to 83% of the nearest\nneighbor searches over $k$NN-LM (Khandelwal et al., 2020) without hurting\nperplexity. Our code and trained models are available at\nhttps://github.com/neulab/retomaton ."
  },
  {
    "id": "arxiv-214",
    "title": "Variational Bayesian Optimal Experimental Design",
    "abstract": "Bayesian optimal experimental design (BOED) is a principled framework for\nmaking efficient use of limited experimental resources. Unfortunately, its\napplicability is hampered by the difficulty of obtaining accurate estimates of\nthe expected information gain (EIG) of an experiment. To address this, we\nintroduce several classes of fast EIG estimators by building on ideas from\namortized variational inference. We show theoretically and empirically that\nthese estimators can provide significant gains in speed and accuracy over\nprevious approaches. We further demonstrate the practicality of our approach on\na number of end-to-end experiments.",
    "text": "Variational Bayesian Optimal Experimental Design\n\nBayesian optimal experimental design (BOED) is a principled framework for\nmaking efficient use of limited experimental resources. Unfortunately, its\napplicability is hampered by the difficulty of obtaining accurate estimates of\nthe expected information gain (EIG) of an experiment. To address this, we\nintroduce several classes of fast EIG estimators by building on ideas from\namortized variational inference. We show theoretically and empirically that\nthese estimators can provide significant gains in speed and accuracy over\nprevious approaches. We further demonstrate the practicality of our approach on\na number of end-to-end experiments."
  },
  {
    "id": "arxiv-215",
    "title": "Learning Using 1-Local Membership Queries",
    "abstract": "Classic machine learning algorithms learn from labelled examples. For\nexample, to design a machine translation system, a typical training set will\nconsist of English sentences and their translation. There is a stronger model,\nin which the algorithm can also query for labels of new examples it creates.\nE.g, in the translation task, the algorithm can create a new English sentence,\nand request its translation from the user during training. This combination of\nexamples and queries has been widely studied. Yet, despite many theoretical\nresults, query algorithms are almost never used. One of the main causes for\nthis is a report (Baum and Lang, 1992) on very disappointing empirical\nperformance of a query algorithm. These poor results were mainly attributed to\nthe fact that the algorithm queried for labels of examples that are artificial,\nand impossible to interpret by humans.\n  In this work we study a new model of local membership queries (Awasthi et\nal., 2012), which tries to resolve the problem of artificial queries. In this\nmodel, the algorithm is only allowed to query the labels of examples which are\nclose to examples from the training set. E.g., in translation, the algorithm\ncan change individual words in a sentence it has already seen, and then ask for\nthe translation. In this model, the examples queried by the algorithm will be\nclose to natural examples and hence, hopefully, will not appear as artificial\nor random. We focus on 1-local queries (i.e., queries of distance 1 from an\nexample in the training sample). We show that 1-local membership queries are\nalready stronger than the standard learning model. We also present an\nexperiment on a well known NLP task of sentiment analysis. In this experiment,\nthe users were asked to provide more information than merely indicating the\nlabel. We present results that illustrate that this extra information is\nbeneficial in practice.",
    "text": "Learning Using 1-Local Membership Queries\n\nClassic machine learning algorithms learn from labelled examples. For\nexample, to design a machine translation system, a typical training set will\nconsist of English sentences and their translation. There is a stronger model,\nin which the algorithm can also query for labels of new examples it creates.\nE.g, in the translation task, the algorithm can create a new English sentence,\nand request its translation from the user during training. This combination of\nexamples and queries has been widely studied. Yet, despite many theoretical\nresults, query algorithms are almost never used. One of the main causes for\nthis is a report (Baum and Lang, 1992) on very disappointing empirical\nperformance of a query algorithm. These poor results were mainly attributed to\nthe fact that the algorithm queried for labels of examples that are artificial,\nand impossible to interpret by humans.\n  In this work we study a new model of local membership queries (Awasthi et\nal., 2012), which tries to resolve the problem of artificial queries. In this\nmodel, the algorithm is only allowed to query the labels of examples which are\nclose to examples from the training set. E.g., in translation, the algorithm\ncan change individual words in a sentence it has already seen, and then ask for\nthe translation. In this model, the examples queried by the algorithm will be\nclose to natural examples and hence, hopefully, will not appear as artificial\nor random. We focus on 1-local queries (i.e., queries of distance 1 from an\nexample in the training sample). We show that 1-local membership queries are\nalready stronger than the standard learning model. We also present an\nexperiment on a well known NLP task of sentiment analysis. In this experiment,\nthe users were asked to provide more information than merely indicating the\nlabel. We present results that illustrate that this extra information is\nbeneficial in practice."
  },
  {
    "id": "arxiv-216",
    "title": "Sequential Short-Text Classification with Recurrent and Convolutional\n  Neural Networks",
    "abstract": "Recent approaches based on artificial neural networks (ANNs) have shown\npromising results for short-text classification. However, many short texts\noccur in sequences (e.g., sentences in a document or utterances in a dialog),\nand most existing ANN-based systems do not leverage the preceding short texts\nwhen classifying a subsequent one. In this work, we present a model based on\nrecurrent neural networks and convolutional neural networks that incorporates\nthe preceding short texts. Our model achieves state-of-the-art results on three\ndifferent datasets for dialog act prediction.",
    "text": "Sequential Short-Text Classification with Recurrent and Convolutional\n  Neural Networks\n\nRecent approaches based on artificial neural networks (ANNs) have shown\npromising results for short-text classification. However, many short texts\noccur in sequences (e.g., sentences in a document or utterances in a dialog),\nand most existing ANN-based systems do not leverage the preceding short texts\nwhen classifying a subsequent one. In this work, we present a model based on\nrecurrent neural networks and convolutional neural networks that incorporates\nthe preceding short texts. Our model achieves state-of-the-art results on three\ndifferent datasets for dialog act prediction."
  },
  {
    "id": "arxiv-217",
    "title": "OLALA: Object-Level Active Learning for Efficient Document Layout\n  Annotation",
    "abstract": "Document images often have intricate layout structures, with numerous content\nregions (e.g. texts, figures, tables) densely arranged on each page. This makes\nthe manual annotation of layout datasets expensive and inefficient. These\ncharacteristics also challenge existing active learning methods, as image-level\nscoring and selection suffer from the overexposure of common objects.Inspired\nby recent progresses in semi-supervised learning and self-training, we propose\nan Object-Level Active Learning framework for efficient document layout\nAnnotation, OLALA. In this framework, only regions with the most ambiguous\nobject predictions within an image are selected for annotators to label,\noptimizing the use of the annotation budget. For unselected predictions, the\nsemi-automatic correction algorithm is proposed to identify certain errors\nbased on prior knowledge of layout structures and rectifies them with minor\nsupervision. Additionally, we carefully design a perturbation-based object\nscoring function for document images. It governs the object selection process\nvia evaluating prediction ambiguities, and considers both the positions and\ncategories of predicted layout objects. Extensive experiments show that OLALA\ncan significantly boost model performance and improve annotation efficiency,\ngiven the same labeling budget. Code for this paper can be accessed via\nhttps://github.com/lolipopshock/detectron2_al.",
    "text": "OLALA: Object-Level Active Learning for Efficient Document Layout\n  Annotation\n\nDocument images often have intricate layout structures, with numerous content\nregions (e.g. texts, figures, tables) densely arranged on each page. This makes\nthe manual annotation of layout datasets expensive and inefficient. These\ncharacteristics also challenge existing active learning methods, as image-level\nscoring and selection suffer from the overexposure of common objects.Inspired\nby recent progresses in semi-supervised learning and self-training, we propose\nan Object-Level Active Learning framework for efficient document layout\nAnnotation, OLALA. In this framework, only regions with the most ambiguous\nobject predictions within an image are selected for annotators to label,\noptimizing the use of the annotation budget. For unselected predictions, the\nsemi-automatic correction algorithm is proposed to identify certain errors\nbased on prior knowledge of layout structures and rectifies them with minor\nsupervision. Additionally, we carefully design a perturbation-based object\nscoring function for document images. It governs the object selection process\nvia evaluating prediction ambiguities, and considers both the positions and\ncategories of predicted layout objects. Extensive experiments show that OLALA\ncan significantly boost model performance and improve annotation efficiency,\ngiven the same labeling budget. Code for this paper can be accessed via\nhttps://github.com/lolipopshock/detectron2_al."
  },
  {
    "id": "arxiv-218",
    "title": "Exposing Outlier Exposure: What Can Be Learned From Few, One, and Zero\n  Outlier Images",
    "abstract": "Traditionally anomaly detection (AD) is treated as an unsupervised problem\nutilizing only normal samples due to the intractability of characterizing\neverything that looks unlike the normal data. However, it has recently been\nfound that unsupervised image anomaly detection can be drastically improved\nthrough the utilization of huge corpora of random images to represent\nanomalousness; a technique which is known as Outlier Exposure. In this paper we\nshow that specialized AD learning methods seem actually superfluous and huge\ncorpora of data expendable. For a common AD benchmark on ImageNet, standard\nclassifiers and semi-supervised one-class methods trained to discern between\nnormal samples and just a few random natural images are able to outperform the\ncurrent state of the art in deep AD, and only one useful outlier sample is\nsufficient to perform competitively. We investigate this phenomenon and reveal\nthat one-class methods are more robust towards the particular choice of\ntraining outliers. Furthermore, we find that a simple classifier based on\nrepresentations from CLIP, a recent foundation model, achieves state-of-the-art\nresults on CIFAR-10 and also outperforms all previous AD methods on ImageNet\nwithout any training samples (i.e., in a zero-shot setting).",
    "text": "Exposing Outlier Exposure: What Can Be Learned From Few, One, and Zero\n  Outlier Images\n\nTraditionally anomaly detection (AD) is treated as an unsupervised problem\nutilizing only normal samples due to the intractability of characterizing\neverything that looks unlike the normal data. However, it has recently been\nfound that unsupervised image anomaly detection can be drastically improved\nthrough the utilization of huge corpora of random images to represent\nanomalousness; a technique which is known as Outlier Exposure. In this paper we\nshow that specialized AD learning methods seem actually superfluous and huge\ncorpora of data expendable. For a common AD benchmark on ImageNet, standard\nclassifiers and semi-supervised one-class methods trained to discern between\nnormal samples and just a few random natural images are able to outperform the\ncurrent state of the art in deep AD, and only one useful outlier sample is\nsufficient to perform competitively. We investigate this phenomenon and reveal\nthat one-class methods are more robust towards the particular choice of\ntraining outliers. Furthermore, we find that a simple classifier based on\nrepresentations from CLIP, a recent foundation model, achieves state-of-the-art\nresults on CIFAR-10 and also outperforms all previous AD methods on ImageNet\nwithout any training samples (i.e., in a zero-shot setting)."
  },
  {
    "id": "arxiv-219",
    "title": "Binary Random Projections with Controllable Sparsity Patterns",
    "abstract": "Random projection is often used to project higher-dimensional vectors onto a\nlower-dimensional space, while approximately preserving their pairwise\ndistances. It has emerged as a powerful tool in various data processing tasks\nand has attracted considerable research interest. Partly motivated by the\nrecent discoveries in neuroscience, in this paper we study the problem of\nrandom projection using binary matrices with controllable sparsity patterns.\nSpecifically, we proposed two sparse binary projection models that work on\ngeneral data vectors. Compared with the conventional random projection models\nwith dense projection matrices, our proposed models enjoy significant\ncomputational advantages due to their sparsity structure, as well as improved\naccuracies in empirical evaluations.",
    "text": "Binary Random Projections with Controllable Sparsity Patterns\n\nRandom projection is often used to project higher-dimensional vectors onto a\nlower-dimensional space, while approximately preserving their pairwise\ndistances. It has emerged as a powerful tool in various data processing tasks\nand has attracted considerable research interest. Partly motivated by the\nrecent discoveries in neuroscience, in this paper we study the problem of\nrandom projection using binary matrices with controllable sparsity patterns.\nSpecifically, we proposed two sparse binary projection models that work on\ngeneral data vectors. Compared with the conventional random projection models\nwith dense projection matrices, our proposed models enjoy significant\ncomputational advantages due to their sparsity structure, as well as improved\naccuracies in empirical evaluations."
  },
  {
    "id": "arxiv-220",
    "title": "RamBoAttack: A Robust Query Efficient Deep Neural Network Decision\n  Exploit",
    "abstract": "Machine learning models are critically susceptible to evasion attacks from\nadversarial examples. Generally, adversarial examples, modified inputs\ndeceptively similar to the original input, are constructed under whitebox\nsettings by adversaries with full access to the model. However, recent attacks\nhave shown a remarkable reduction in query numbers to craft adversarial\nexamples using blackbox attacks. Particularly, alarming is the ability to\nexploit the classification decision from the access interface of a trained\nmodel provided by a growing number of Machine Learning as a Service providers\nincluding Google, Microsoft, IBM and used by a plethora of applications\nincorporating these models. The ability of an adversary to exploit only the\npredicted label from a model to craft adversarial examples is distinguished as\na decision-based attack. In our study, we first deep dive into recent\nstate-of-the-art decision-based attacks in ICLR and SP to highlight the costly\nnature of discovering low distortion adversarial employing gradient estimation\nmethods. We develop a robust query efficient attack capable of avoiding\nentrapment in a local minimum and misdirection from noisy gradients seen in\ngradient estimation methods. The attack method we propose, RamBoAttack,\nexploits the notion of Randomized Block Coordinate Descent to explore the\nhidden classifier manifold, targeting perturbations to manipulate only\nlocalized input features to address the issues of gradient estimation methods.\nImportantly, the RamBoAttack is more robust to the different sample inputs\navailable to an adversary and the targeted class. Overall, for a given target\nclass, RamBoAttack is demonstrated to be more robust at achieving a lower\ndistortion within a given query budget. We curate our extensive results using\nthe large-scale high-resolution ImageNet dataset and open-source our attack,\ntest samples and artifacts on GitHub.",
    "text": "RamBoAttack: A Robust Query Efficient Deep Neural Network Decision\n  Exploit\n\nMachine learning models are critically susceptible to evasion attacks from\nadversarial examples. Generally, adversarial examples, modified inputs\ndeceptively similar to the original input, are constructed under whitebox\nsettings by adversaries with full access to the model. However, recent attacks\nhave shown a remarkable reduction in query numbers to craft adversarial\nexamples using blackbox attacks. Particularly, alarming is the ability to\nexploit the classification decision from the access interface of a trained\nmodel provided by a growing number of Machine Learning as a Service providers\nincluding Google, Microsoft, IBM and used by a plethora of applications\nincorporating these models. The ability of an adversary to exploit only the\npredicted label from a model to craft adversarial examples is distinguished as\na decision-based attack. In our study, we first deep dive into recent\nstate-of-the-art decision-based attacks in ICLR and SP to highlight the costly\nnature of discovering low distortion adversarial employing gradient estimation\nmethods. We develop a robust query efficient attack capable of avoiding\nentrapment in a local minimum and misdirection from noisy gradients seen in\ngradient estimation methods. The attack method we propose, RamBoAttack,\nexploits the notion of Randomized Block Coordinate Descent to explore the\nhidden classifier manifold, targeting perturbations to manipulate only\nlocalized input features to address the issues of gradient estimation methods.\nImportantly, the RamBoAttack is more robust to the different sample inputs\navailable to an adversary and the targeted class. Overall, for a given target\nclass, RamBoAttack is demonstrated to be more robust at achieving a lower\ndistortion within a given query budget. We curate our extensive results using\nthe large-scale high-resolution ImageNet dataset and open-source our attack,\ntest samples and artifacts on GitHub."
  },
  {
    "id": "arxiv-221",
    "title": "Optimal Quantum Sample Complexity of Learning Algorithms",
    "abstract": "$ \\newcommand{\\eps}{\\varepsilon} $In learning theory, the VC dimension of a\nconcept class $C$ is the most common way to measure its \"richness.\" In the PAC\nmodel $$ \\Theta\\Big(\\frac{d}{\\eps} + \\frac{\\log(1/\\delta)}{\\eps}\\Big) $$\nexamples are necessary and sufficient for a learner to output, with probability\n$1-\\delta$, a hypothesis $h$ that is $\\eps$-close to the target concept $c$. In\nthe related agnostic model, where the samples need not come from a $c\\in C$, we\nknow that $$ \\Theta\\Big(\\frac{d}{\\eps^2} + \\frac{\\log(1/\\delta)}{\\eps^2}\\Big)\n$$ examples are necessary and sufficient to output an hypothesis $h\\in C$ whose\nerror is at most $\\eps$ worse than the best concept in $C$.\n  Here we analyze quantum sample complexity, where each example is a coherent\nquantum state. This model was introduced by Bshouty and Jackson, who showed\nthat quantum examples are more powerful than classical examples in some\nfixed-distribution settings. However, Atici and Servedio, improved by Zhang,\nshowed that in the PAC setting, quantum examples cannot be much more powerful:\nthe required number of quantum examples is $$\n\\Omega\\Big(\\frac{d^{1-\\eta}}{\\eps} + d + \\frac{\\log(1/\\delta)}{\\eps}\\Big)\\mbox{\nfor all }\\eta> 0. $$ Our main result is that quantum and classical sample\ncomplexity are in fact equal up to constant factors in both the PAC and\nagnostic models. We give two approaches. The first is a fairly simple\ninformation-theoretic argument that yields the above two classical bounds and\nyields the same bounds for quantum sample complexity up to a $\\log(d/\\eps)$\nfactor. We then give a second approach that avoids the log-factor loss, based\non analyzing the behavior of the \"Pretty Good Measurement\" on the quantum state\nidentification problems that correspond to learning. This shows classical and\nquantum sample complexity are equal up to constant factors.",
    "text": "Optimal Quantum Sample Complexity of Learning Algorithms\n\n$ \\newcommand{\\eps}{\\varepsilon} $In learning theory, the VC dimension of a\nconcept class $C$ is the most common way to measure its \"richness.\" In the PAC\nmodel $$ \\Theta\\Big(\\frac{d}{\\eps} + \\frac{\\log(1/\\delta)}{\\eps}\\Big) $$\nexamples are necessary and sufficient for a learner to output, with probability\n$1-\\delta$, a hypothesis $h$ that is $\\eps$-close to the target concept $c$. In\nthe related agnostic model, where the samples need not come from a $c\\in C$, we\nknow that $$ \\Theta\\Big(\\frac{d}{\\eps^2} + \\frac{\\log(1/\\delta)}{\\eps^2}\\Big)\n$$ examples are necessary and sufficient to output an hypothesis $h\\in C$ whose\nerror is at most $\\eps$ worse than the best concept in $C$.\n  Here we analyze quantum sample complexity, where each example is a coherent\nquantum state. This model was introduced by Bshouty and Jackson, who showed\nthat quantum examples are more powerful than classical examples in some\nfixed-distribution settings. However, Atici and Servedio, improved by Zhang,\nshowed that in the PAC setting, quantum examples cannot be much more powerful:\nthe required number of quantum examples is $$\n\\Omega\\Big(\\frac{d^{1-\\eta}}{\\eps} + d + \\frac{\\log(1/\\delta)}{\\eps}\\Big)\\mbox{\nfor all }\\eta> 0. $$ Our main result is that quantum and classical sample\ncomplexity are in fact equal up to constant factors in both the PAC and\nagnostic models. We give two approaches. The first is a fairly simple\ninformation-theoretic argument that yields the above two classical bounds and\nyields the same bounds for quantum sample complexity up to a $\\log(d/\\eps)$\nfactor. We then give a second approach that avoids the log-factor loss, based\non analyzing the behavior of the \"Pretty Good Measurement\" on the quantum state\nidentification problems that correspond to learning. This shows classical and\nquantum sample complexity are equal up to constant factors."
  },
  {
    "id": "arxiv-222",
    "title": "Greedy Actor-Critic: A New Conditional Cross-Entropy Method for Policy\n  Improvement",
    "abstract": "Many policy gradient methods are variants of Actor-Critic (AC), where a value\nfunction (critic) is learned to facilitate updating the parameterized policy\n(actor). The update to the actor involves a log-likelihood update weighted by\nthe action-values, with the addition of entropy regularization for soft\nvariants. In this work, we explore an alternative update for the actor, based\non an extension of the cross entropy method (CEM) to condition on inputs\n(states). The idea is to start with a broader policy and slowly concentrate\naround maximal actions, using a maximum likelihood update towards actions in\nthe top percentile per state. The speed of this concentration is controlled by\na proposal policy, that concentrates at a slower rate than the actor. We first\nprovide a policy improvement result in an idealized setting, and then prove\nthat our conditional CEM (CCEM) strategy tracks a CEM update per state, even\nwith changing action-values. We empirically show that our Greedy AC algorithm,\nthat uses CCEM for the actor update, performs better than Soft AC and is much\nless sensitive to entropy-regularization.",
    "text": "Greedy Actor-Critic: A New Conditional Cross-Entropy Method for Policy\n  Improvement\n\nMany policy gradient methods are variants of Actor-Critic (AC), where a value\nfunction (critic) is learned to facilitate updating the parameterized policy\n(actor). The update to the actor involves a log-likelihood update weighted by\nthe action-values, with the addition of entropy regularization for soft\nvariants. In this work, we explore an alternative update for the actor, based\non an extension of the cross entropy method (CEM) to condition on inputs\n(states). The idea is to start with a broader policy and slowly concentrate\naround maximal actions, using a maximum likelihood update towards actions in\nthe top percentile per state. The speed of this concentration is controlled by\na proposal policy, that concentrates at a slower rate than the actor. We first\nprovide a policy improvement result in an idealized setting, and then prove\nthat our conditional CEM (CCEM) strategy tracks a CEM update per state, even\nwith changing action-values. We empirically show that our Greedy AC algorithm,\nthat uses CCEM for the actor update, performs better than Soft AC and is much\nless sensitive to entropy-regularization."
  },
  {
    "id": "arxiv-223",
    "title": "MIONet: Learning multiple-input operators via tensor product",
    "abstract": "As an emerging paradigm in scientific machine learning, neural operators aim\nto learn operators, via neural networks, that map between infinite-dimensional\nfunction spaces. Several neural operators have been recently developed.\nHowever, all the existing neural operators are only designed to learn operators\ndefined on a single Banach space, i.e., the input of the operator is a single\nfunction. Here, for the first time, we study the operator regression via neural\nnetworks for multiple-input operators defined on the product of Banach spaces.\nWe first prove a universal approximation theorem of continuous multiple-input\noperators. We also provide detailed theoretical analysis including the\napproximation error, which provides a guidance of the design of the network\narchitecture. Based on our theory and a low-rank approximation, we propose a\nnovel neural operator, MIONet, to learn multiple-input operators. MIONet\nconsists of several branch nets for encoding the input functions and a trunk\nnet for encoding the domain of the output function. We demonstrate that MIONet\ncan learn solution operators involving systems governed by ordinary and partial\ndifferential equations. In our computational examples, we also show that we can\nendow MIONet with prior knowledge of the underlying system, such as linearity\nand periodicity, to further improve the accuracy.",
    "text": "MIONet: Learning multiple-input operators via tensor product\n\nAs an emerging paradigm in scientific machine learning, neural operators aim\nto learn operators, via neural networks, that map between infinite-dimensional\nfunction spaces. Several neural operators have been recently developed.\nHowever, all the existing neural operators are only designed to learn operators\ndefined on a single Banach space, i.e., the input of the operator is a single\nfunction. Here, for the first time, we study the operator regression via neural\nnetworks for multiple-input operators defined on the product of Banach spaces.\nWe first prove a universal approximation theorem of continuous multiple-input\noperators. We also provide detailed theoretical analysis including the\napproximation error, which provides a guidance of the design of the network\narchitecture. Based on our theory and a low-rank approximation, we propose a\nnovel neural operator, MIONet, to learn multiple-input operators. MIONet\nconsists of several branch nets for encoding the input functions and a trunk\nnet for encoding the domain of the output function. We demonstrate that MIONet\ncan learn solution operators involving systems governed by ordinary and partial\ndifferential equations. In our computational examples, we also show that we can\nendow MIONet with prior knowledge of the underlying system, such as linearity\nand periodicity, to further improve the accuracy."
  },
  {
    "id": "arxiv-224",
    "title": "Machine Learning of Space-Fractional Differential Equations",
    "abstract": "Data-driven discovery of \"hidden physics\" -- i.e., machine learning of\ndifferential equation models underlying observed data -- has recently been\napproached by embedding the discovery problem into a Gaussian Process\nregression of spatial data, treating and discovering unknown equation\nparameters as hyperparameters of a modified \"physics informed\" Gaussian Process\nkernel. This kernel includes the parametrized differential operators applied to\na prior covariance kernel. We extend this framework to linear space-fractional\ndifferential equations. The methodology is compatible with a wide variety of\nfractional operators in $\\mathbb{R}^d$ and stationary covariance kernels,\nincluding the Matern class, and can optimize the Matern parameter during\ntraining. We provide a user-friendly and feasible way to perform fractional\nderivatives of kernels, via a unified set of d-dimensional Fourier integral\nformulas amenable to generalized Gauss-Laguerre quadrature.\n  The implementation of fractional derivatives has several benefits. First, it\nallows for discovering fractional-order PDEs for systems characterized by heavy\ntails or anomalous diffusion, bypassing the analytical difficulty of fractional\ncalculus. Data sets exhibiting such features are of increasing prevalence in\nphysical and financial domains. Second, a single fractional-order archetype\nallows for a derivative of arbitrary order to be learned, with the order itself\nbeing a parameter in the regression. This is advantageous even when used for\ndiscovering integer-order equations; the user is not required to assume a\n\"dictionary\" of derivatives of various orders, and directly controls the\nparsimony of the models being discovered. We illustrate on several examples,\nincluding fractional-order interpolation of advection-diffusion and modeling\nrelative stock performance in the S&P 500 with alpha-stable motion via a\nfractional diffusion equation.",
    "text": "Machine Learning of Space-Fractional Differential Equations\n\nData-driven discovery of \"hidden physics\" -- i.e., machine learning of\ndifferential equation models underlying observed data -- has recently been\napproached by embedding the discovery problem into a Gaussian Process\nregression of spatial data, treating and discovering unknown equation\nparameters as hyperparameters of a modified \"physics informed\" Gaussian Process\nkernel. This kernel includes the parametrized differential operators applied to\na prior covariance kernel. We extend this framework to linear space-fractional\ndifferential equations. The methodology is compatible with a wide variety of\nfractional operators in $\\mathbb{R}^d$ and stationary covariance kernels,\nincluding the Matern class, and can optimize the Matern parameter during\ntraining. We provide a user-friendly and feasible way to perform fractional\nderivatives of kernels, via a unified set of d-dimensional Fourier integral\nformulas amenable to generalized Gauss-Laguerre quadrature.\n  The implementation of fractional derivatives has several benefits. First, it\nallows for discovering fractional-order PDEs for systems characterized by heavy\ntails or anomalous diffusion, bypassing the analytical difficulty of fractional\ncalculus. Data sets exhibiting such features are of increasing prevalence in\nphysical and financial domains. Second, a single fractional-order archetype\nallows for a derivative of arbitrary order to be learned, with the order itself\nbeing a parameter in the regression. This is advantageous even when used for\ndiscovering integer-order equations; the user is not required to assume a\n\"dictionary\" of derivatives of various orders, and directly controls the\nparsimony of the models being discovered. We illustrate on several examples,\nincluding fractional-order interpolation of advection-diffusion and modeling\nrelative stock performance in the S&P 500 with alpha-stable motion via a\nfractional diffusion equation."
  },
  {
    "id": "arxiv-225",
    "title": "Group-Invariant Quantum Machine Learning",
    "abstract": "Quantum Machine Learning (QML) models are aimed at learning from data encoded\nin quantum states. Recently, it has been shown that models with little to no\ninductive biases (i.e., with no assumptions about the problem embedded in the\nmodel) are likely to have trainability and generalization issues, especially\nfor large problem sizes. As such, it is fundamental to develop schemes that\nencode as much information as available about the problem at hand. In this work\nwe present a simple, yet powerful, framework where the underlying invariances\nin the data are used to build QML models that, by construction, respect those\nsymmetries. These so-called group-invariant models produce outputs that remain\ninvariant under the action of any element of the symmetry group $\\mathfrak{G}$\nassociated to the dataset. We present theoretical results underpinning the\ndesign of $\\mathfrak{G}$-invariant models, and exemplify their application\nthrough several paradigmatic QML classification tasks including cases when\n$\\mathfrak{G}$ is a continuous Lie group and also when it is a discrete\nsymmetry group. Notably, our framework allows us to recover, in an elegant way,\nseveral well known algorithms for the literature, as well as to discover new\nones. Taken together, we expect that our results will help pave the way towards\na more geometric and group-theoretic approach to QML model design.",
    "text": "Group-Invariant Quantum Machine Learning\n\nQuantum Machine Learning (QML) models are aimed at learning from data encoded\nin quantum states. Recently, it has been shown that models with little to no\ninductive biases (i.e., with no assumptions about the problem embedded in the\nmodel) are likely to have trainability and generalization issues, especially\nfor large problem sizes. As such, it is fundamental to develop schemes that\nencode as much information as available about the problem at hand. In this work\nwe present a simple, yet powerful, framework where the underlying invariances\nin the data are used to build QML models that, by construction, respect those\nsymmetries. These so-called group-invariant models produce outputs that remain\ninvariant under the action of any element of the symmetry group $\\mathfrak{G}$\nassociated to the dataset. We present theoretical results underpinning the\ndesign of $\\mathfrak{G}$-invariant models, and exemplify their application\nthrough several paradigmatic QML classification tasks including cases when\n$\\mathfrak{G}$ is a continuous Lie group and also when it is a discrete\nsymmetry group. Notably, our framework allows us to recover, in an elegant way,\nseveral well known algorithms for the literature, as well as to discover new\nones. Taken together, we expect that our results will help pave the way towards\na more geometric and group-theoretic approach to QML model design."
  },
  {
    "id": "arxiv-226",
    "title": "Tree-based local explanations of machine learning model predictions,\n  AraucanaXAI",
    "abstract": "Increasingly complex learning methods such as boosting, bagging and deep\nlearning have made ML models more accurate, but harder to understand and\ninterpret. A tradeoff between performance and intelligibility is often to be\nfaced, especially in high-stakes applications like medicine. In the present\narticle we propose a novel methodological approach for generating explanations\nof the predictions of a generic ML model, given a specific instance for which\nthe prediction has been made, that can tackle both classification and\nregression tasks. Advantages of the proposed XAI approach include improved\nfidelity to the original model, the ability to deal with non-linear decision\nboundaries, and native support to both classification and regression problems",
    "text": "Tree-based local explanations of machine learning model predictions,\n  AraucanaXAI\n\nIncreasingly complex learning methods such as boosting, bagging and deep\nlearning have made ML models more accurate, but harder to understand and\ninterpret. A tradeoff between performance and intelligibility is often to be\nfaced, especially in high-stakes applications like medicine. In the present\narticle we propose a novel methodological approach for generating explanations\nof the predictions of a generic ML model, given a specific instance for which\nthe prediction has been made, that can tackle both classification and\nregression tasks. Advantages of the proposed XAI approach include improved\nfidelity to the original model, the ability to deal with non-linear decision\nboundaries, and native support to both classification and regression problems"
  },
  {
    "id": "arxiv-227",
    "title": "Learning partial correlation graphs and graphical models by covariance\n  queries",
    "abstract": "We study the problem of recovering the structure underlying large Gaussian\ngraphical models or, more generally, partial correlation graphs. In\nhigh-dimensional problems it is often too costly to store the entire sample\ncovariance matrix. We propose a new input model in which one can query single\nentries of the covariance matrix. We prove that it is possible to recover the\nsupport of the inverse covariance matrix with low query and computational\ncomplexity. Our algorithms work in a regime when this support is represented by\ntree-like graphs and, more generally, for graphs of small treewidth. Our\nresults demonstrate that for large classes of graphs, the structure of the\ncorresponding partial correlation graphs can be determined much faster than\neven computing the empirical covariance matrix.",
    "text": "Learning partial correlation graphs and graphical models by covariance\n  queries\n\nWe study the problem of recovering the structure underlying large Gaussian\ngraphical models or, more generally, partial correlation graphs. In\nhigh-dimensional problems it is often too costly to store the entire sample\ncovariance matrix. We propose a new input model in which one can query single\nentries of the covariance matrix. We prove that it is possible to recover the\nsupport of the inverse covariance matrix with low query and computational\ncomplexity. Our algorithms work in a regime when this support is represented by\ntree-like graphs and, more generally, for graphs of small treewidth. Our\nresults demonstrate that for large classes of graphs, the structure of the\ncorresponding partial correlation graphs can be determined much faster than\neven computing the empirical covariance matrix."
  },
  {
    "id": "arxiv-228",
    "title": "Active Clustering: Robust and Efficient Hierarchical Clustering using\n  Adaptively Selected Similarities",
    "abstract": "Hierarchical clustering based on pairwise similarities is a common tool used\nin a broad range of scientific applications. However, in many problems it may\nbe expensive to obtain or compute similarities between the items to be\nclustered. This paper investigates the hierarchical clustering of N items based\non a small subset of pairwise similarities, significantly less than the\ncomplete set of N(N-1)/2 similarities. First, we show that if the intracluster\nsimilarities exceed intercluster similarities, then it is possible to correctly\ndetermine the hierarchical clustering from as few as 3N log N similarities. We\ndemonstrate this order of magnitude savings in the number of pairwise\nsimilarities necessitates sequentially selecting which similarities to obtain\nin an adaptive fashion, rather than picking them at random. We then propose an\nactive clustering method that is robust to a limited fraction of anomalous\nsimilarities, and show how even in the presence of these noisy similarity\nvalues we can resolve the hierarchical clustering using only O(N log^2 N)\npairwise similarities.",
    "text": "Active Clustering: Robust and Efficient Hierarchical Clustering using\n  Adaptively Selected Similarities\n\nHierarchical clustering based on pairwise similarities is a common tool used\nin a broad range of scientific applications. However, in many problems it may\nbe expensive to obtain or compute similarities between the items to be\nclustered. This paper investigates the hierarchical clustering of N items based\non a small subset of pairwise similarities, significantly less than the\ncomplete set of N(N-1)/2 similarities. First, we show that if the intracluster\nsimilarities exceed intercluster similarities, then it is possible to correctly\ndetermine the hierarchical clustering from as few as 3N log N similarities. We\ndemonstrate this order of magnitude savings in the number of pairwise\nsimilarities necessitates sequentially selecting which similarities to obtain\nin an adaptive fashion, rather than picking them at random. We then propose an\nactive clustering method that is robust to a limited fraction of anomalous\nsimilarities, and show how even in the presence of these noisy similarity\nvalues we can resolve the hierarchical clustering using only O(N log^2 N)\npairwise similarities."
  },
  {
    "id": "arxiv-229",
    "title": "DGST: a Dual-Generator Network for Text Style Transfer",
    "abstract": "We propose DGST, a novel and simple Dual-Generator network architecture for\ntext Style Transfer. Our model employs two generators only, and does not rely\non any discriminators or parallel corpus for training. Both quantitative and\nqualitative experiments on the Yelp and IMDb datasets show that our model gives\ncompetitive performance compared to several strong baselines with more\ncomplicated architecture designs.",
    "text": "DGST: a Dual-Generator Network for Text Style Transfer\n\nWe propose DGST, a novel and simple Dual-Generator network architecture for\ntext Style Transfer. Our model employs two generators only, and does not rely\non any discriminators or parallel corpus for training. Both quantitative and\nqualitative experiments on the Yelp and IMDb datasets show that our model gives\ncompetitive performance compared to several strong baselines with more\ncomplicated architecture designs."
  },
  {
    "id": "arxiv-230",
    "title": "Predicting Tau Accumulation in Cerebral Cortex with Multivariate MRI\n  Morphometry Measurements, Sparse Coding, and Correntropy",
    "abstract": "Biomarker-assisted diagnosis and intervention in Alzheimer's disease (AD) may\nbe the key to prevention breakthroughs. One of the hallmarks of AD is the\naccumulation of tau plaques in the human brain. However, current methods to\ndetect tau pathology are either invasive (lumbar puncture) or quite costly and\nnot widely available (Tau PET). In our previous work, structural MRI-based\nhippocampal multivariate morphometry statistics (MMS) showed superior\nperformance as an effective neurodegenerative biomarker for preclinical AD and\nPatch Analysis-based Surface Correntropy-induced Sparse coding and max-pooling\n(PASCS-MP) has excellent ability to generate low-dimensional representations\nwith strong statistical power for brain amyloid prediction. In this work, we\napply this framework together with ridge regression models to predict Tau\ndeposition in Braak12 and Braak34 brain regions separately. We evaluate our\nframework on 925 subjects from the Alzheimer's Disease Neuroimaging Initiative\n(ADNI). Each subject has one pair consisting of a PET image and MRI scan which\nwere collected at about the same times. Experimental results suggest that the\nrepresentations from our MMS and PASCS-MP have stronger predictive power and\ntheir predicted Braak12 and Braak34 are closer to the real values compared to\nthe measures derived from other approaches such as hippocampal surface area and\nvolume, and shape morphometry features based on spherical harmonics (SPHARM).",
    "text": "Predicting Tau Accumulation in Cerebral Cortex with Multivariate MRI\n  Morphometry Measurements, Sparse Coding, and Correntropy\n\nBiomarker-assisted diagnosis and intervention in Alzheimer's disease (AD) may\nbe the key to prevention breakthroughs. One of the hallmarks of AD is the\naccumulation of tau plaques in the human brain. However, current methods to\ndetect tau pathology are either invasive (lumbar puncture) or quite costly and\nnot widely available (Tau PET). In our previous work, structural MRI-based\nhippocampal multivariate morphometry statistics (MMS) showed superior\nperformance as an effective neurodegenerative biomarker for preclinical AD and\nPatch Analysis-based Surface Correntropy-induced Sparse coding and max-pooling\n(PASCS-MP) has excellent ability to generate low-dimensional representations\nwith strong statistical power for brain amyloid prediction. In this work, we\napply this framework together with ridge regression models to predict Tau\ndeposition in Braak12 and Braak34 brain regions separately. We evaluate our\nframework on 925 subjects from the Alzheimer's Disease Neuroimaging Initiative\n(ADNI). Each subject has one pair consisting of a PET image and MRI scan which\nwere collected at about the same times. Experimental results suggest that the\nrepresentations from our MMS and PASCS-MP have stronger predictive power and\ntheir predicted Braak12 and Braak34 are closer to the real values compared to\nthe measures derived from other approaches such as hippocampal surface area and\nvolume, and shape morphometry features based on spherical harmonics (SPHARM)."
  },
  {
    "id": "arxiv-231",
    "title": "Geometric feature performance under downsampling for EEG classification\n  tasks",
    "abstract": "We experimentally investigate a collection of feature engineering pipelines\nfor use with a CNN for classifying eyes-open or eyes-closed from\nelectroencephalogram (EEG) time-series from the Bonn dataset. Using the Takens'\nembedding--a geometric representation of time-series--we construct simplicial\ncomplexes from EEG data. We then compare $\\epsilon$-series of Betti-numbers and\n$\\epsilon$-series of graph spectra (a novel construction)--two topological\ninvariants of the latent geometry from these complexes--to raw time series of\nthe EEG to fill in a gap in the literature for benchmarking. These methods,\ninspired by Topological Data Analysis, are used for feature engineering to\ncapture local geometry of the time-series. Additionally, we test these feature\npipelines' robustness to downsampling and data reduction. This paper seeks to\nestablish clearer expectations for both time-series classification via\ngeometric features, and how CNNs for time-series respond to data of degraded\nresolution.",
    "text": "Geometric feature performance under downsampling for EEG classification\n  tasks\n\nWe experimentally investigate a collection of feature engineering pipelines\nfor use with a CNN for classifying eyes-open or eyes-closed from\nelectroencephalogram (EEG) time-series from the Bonn dataset. Using the Takens'\nembedding--a geometric representation of time-series--we construct simplicial\ncomplexes from EEG data. We then compare $\\epsilon$-series of Betti-numbers and\n$\\epsilon$-series of graph spectra (a novel construction)--two topological\ninvariants of the latent geometry from these complexes--to raw time series of\nthe EEG to fill in a gap in the literature for benchmarking. These methods,\ninspired by Topological Data Analysis, are used for feature engineering to\ncapture local geometry of the time-series. Additionally, we test these feature\npipelines' robustness to downsampling and data reduction. This paper seeks to\nestablish clearer expectations for both time-series classification via\ngeometric features, and how CNNs for time-series respond to data of degraded\nresolution."
  },
  {
    "id": "arxiv-232",
    "title": "A Feasible Level Proximal Point Method for Nonconvex Sparse Constrained\n  Optimization",
    "abstract": "Nonconvex sparse models have received significant attention in\nhigh-dimensional machine learning. In this paper, we study a new model\nconsisting of a general convex or nonconvex objectives and a variety of\ncontinuous nonconvex sparsity-inducing constraints. For this constrained model,\nwe propose a novel proximal point algorithm that solves a sequence of convex\nsubproblems with gradually relaxed constraint levels. Each subproblem, having a\nproximal point objective and a convex surrogate constraint, can be efficiently\nsolved based on a fast routine for projection onto the surrogate constraint. We\nestablish the asymptotic convergence of the proposed algorithm to the\nKarush-Kuhn-Tucker (KKT) solutions. We also establish new convergence\ncomplexities to achieve an approximate KKT solution when the objective can be\nsmooth/nonsmooth, deterministic/stochastic and convex/nonconvex with complexity\nthat is on a par with gradient descent for unconstrained optimization problems\nin respective cases. To the best of our knowledge, this is the first study of\nthe first-order methods with complexity guarantee for nonconvex\nsparse-constrained problems. We perform numerical experiments to demonstrate\nthe effectiveness of our new model and efficiency of the proposed algorithm for\nlarge scale problems.",
    "text": "A Feasible Level Proximal Point Method for Nonconvex Sparse Constrained\n  Optimization\n\nNonconvex sparse models have received significant attention in\nhigh-dimensional machine learning. In this paper, we study a new model\nconsisting of a general convex or nonconvex objectives and a variety of\ncontinuous nonconvex sparsity-inducing constraints. For this constrained model,\nwe propose a novel proximal point algorithm that solves a sequence of convex\nsubproblems with gradually relaxed constraint levels. Each subproblem, having a\nproximal point objective and a convex surrogate constraint, can be efficiently\nsolved based on a fast routine for projection onto the surrogate constraint. We\nestablish the asymptotic convergence of the proposed algorithm to the\nKarush-Kuhn-Tucker (KKT) solutions. We also establish new convergence\ncomplexities to achieve an approximate KKT solution when the objective can be\nsmooth/nonsmooth, deterministic/stochastic and convex/nonconvex with complexity\nthat is on a par with gradient descent for unconstrained optimization problems\nin respective cases. To the best of our knowledge, this is the first study of\nthe first-order methods with complexity guarantee for nonconvex\nsparse-constrained problems. We perform numerical experiments to demonstrate\nthe effectiveness of our new model and efficiency of the proposed algorithm for\nlarge scale problems."
  },
  {
    "id": "arxiv-233",
    "title": "Wholesale Electricity Price Forecasting using Integrated Long-term\n  Recurrent Convolutional Network Model",
    "abstract": "Electricity price is a key factor affecting the decision-making for all\nmarket participants. Accurate forecasting of electricity prices is very\nimportant and is also very challenging since electricity price is highly\nvolatile due to various factors. This paper proposes an integrated long-term\nrecurrent convolutional network (ILRCN) model to predict electricity prices\nconsidering the majority contributing attributes to the market price as input.\nThe proposed ILRCN model combines the functionalities of convolutional neural\nnetwork and long short-term memory (LSTM) algorithm along with the proposed\nnovel conditional error correction term. The combined ILRCN model can identify\nthe linear and non-linear behavior within the input data. We have used ERCOT\nwholesale market price data along with load profile, temperature, and other\nfactors for the Houston region to illustrate the proposed model. The\nperformance of the proposed ILRCN electricity price forecasting model is\nverified using performance/evaluation metrics like mean absolute error and\naccuracy. Case studies reveal that the proposed ILRCN model is accurate and\nefficient in electricity price forecasting as compared to the support vector\nmachine (SVM) model, fully-connected neural network model, LSTM model and the\nLRCN model without the conditional error correction stage.",
    "text": "Wholesale Electricity Price Forecasting using Integrated Long-term\n  Recurrent Convolutional Network Model\n\nElectricity price is a key factor affecting the decision-making for all\nmarket participants. Accurate forecasting of electricity prices is very\nimportant and is also very challenging since electricity price is highly\nvolatile due to various factors. This paper proposes an integrated long-term\nrecurrent convolutional network (ILRCN) model to predict electricity prices\nconsidering the majority contributing attributes to the market price as input.\nThe proposed ILRCN model combines the functionalities of convolutional neural\nnetwork and long short-term memory (LSTM) algorithm along with the proposed\nnovel conditional error correction term. The combined ILRCN model can identify\nthe linear and non-linear behavior within the input data. We have used ERCOT\nwholesale market price data along with load profile, temperature, and other\nfactors for the Houston region to illustrate the proposed model. The\nperformance of the proposed ILRCN electricity price forecasting model is\nverified using performance/evaluation metrics like mean absolute error and\naccuracy. Case studies reveal that the proposed ILRCN model is accurate and\nefficient in electricity price forecasting as compared to the support vector\nmachine (SVM) model, fully-connected neural network model, LSTM model and the\nLRCN model without the conditional error correction stage."
  },
  {
    "id": "arxiv-234",
    "title": "Enhancing Food Intake Tracking in Long-Term Care with Automated Food\n  Imaging and Nutrient Intake Tracking (AFINI-T) Technology",
    "abstract": "Half of long-term care (LTC) residents are malnourished increasing\nhospitalization, mortality, morbidity, with lower quality of life. Current\ntracking methods are subjective and time consuming. This paper presents the\nautomated food imaging and nutrient intake tracking (AFINI-T) technology\ndesigned for LTC. We propose a novel convolutional autoencoder for food\nclassification, trained on an augmented UNIMIB2016 dataset and tested on our\nsimulated LTC food intake dataset (12 meal scenarios; up to 15 classes each;\ntop-1 classification accuracy: 88.9%; mean intake error: -0.4 mL$\\pm$36.7 mL).\nNutrient intake estimation by volume was strongly linearly correlated with\nnutrient estimates from mass ($r^2$ 0.92 to 0.99) with good agreement between\nmethods ($\\sigma$= -2.7 to -0.01; zero within each of the limits of agreement).\nThe AFINI-T approach is a deep-learning powered computational nutrient sensing\nsystem that may provide a novel means for more accurately and objectively\ntracking LTC resident food intake to support and prevent malnutrition tracking\nstrategies.",
    "text": "Enhancing Food Intake Tracking in Long-Term Care with Automated Food\n  Imaging and Nutrient Intake Tracking (AFINI-T) Technology\n\nHalf of long-term care (LTC) residents are malnourished increasing\nhospitalization, mortality, morbidity, with lower quality of life. Current\ntracking methods are subjective and time consuming. This paper presents the\nautomated food imaging and nutrient intake tracking (AFINI-T) technology\ndesigned for LTC. We propose a novel convolutional autoencoder for food\nclassification, trained on an augmented UNIMIB2016 dataset and tested on our\nsimulated LTC food intake dataset (12 meal scenarios; up to 15 classes each;\ntop-1 classification accuracy: 88.9%; mean intake error: -0.4 mL$\\pm$36.7 mL).\nNutrient intake estimation by volume was strongly linearly correlated with\nnutrient estimates from mass ($r^2$ 0.92 to 0.99) with good agreement between\nmethods ($\\sigma$= -2.7 to -0.01; zero within each of the limits of agreement).\nThe AFINI-T approach is a deep-learning powered computational nutrient sensing\nsystem that may provide a novel means for more accurately and objectively\ntracking LTC resident food intake to support and prevent malnutrition tracking\nstrategies."
  },
  {
    "id": "arxiv-235",
    "title": "A Dual-Perception Graph Neural Network with Multi-hop Graph Generator",
    "abstract": "Graph neural networks (GNNs) have drawn increasing attention in recent years\nand achieved remarkable performance in many graph-based tasks, especially in\nsemi-supervised learning on graphs. However, most existing GNNs excessively\nrely on topological structures and aggregate multi-hop neighborhood information\nby simply stacking network layers, which may introduce superfluous noise\ninformation, limit the expressive power of GNNs and lead to the over-smoothing\nproblem ultimately. In light of this, we propose a novel Dual-Perception Graph\nNeural Network (DPGNN) to address these issues. In DPGNN, we utilize node\nfeatures to construct a feature graph, and perform node representations\nlearning based on the original topology graph and the constructed feature graph\nsimultaneously, which conduce to capture the structural neighborhood\ninformation and the feature-related information. Furthermore, we design a\nMulti-Hop Graph Generator (MHGG), which applies a node-to-hop attention\nmechanism to aggregate node-specific multi-hop neighborhood information\nadaptively. Finally, we apply self-ensembling to form a consistent prediction\nfor unlabeled node representations. Experimental results on five datasets with\ndifferent topological structures demonstrate that our proposed DPGNN\noutperforms all the latest state-of-the-art models on all datasets, which\nproves the superiority and versatility of our model. The source code of our\nmodel is available at https://github.com.",
    "text": "A Dual-Perception Graph Neural Network with Multi-hop Graph Generator\n\nGraph neural networks (GNNs) have drawn increasing attention in recent years\nand achieved remarkable performance in many graph-based tasks, especially in\nsemi-supervised learning on graphs. However, most existing GNNs excessively\nrely on topological structures and aggregate multi-hop neighborhood information\nby simply stacking network layers, which may introduce superfluous noise\ninformation, limit the expressive power of GNNs and lead to the over-smoothing\nproblem ultimately. In light of this, we propose a novel Dual-Perception Graph\nNeural Network (DPGNN) to address these issues. In DPGNN, we utilize node\nfeatures to construct a feature graph, and perform node representations\nlearning based on the original topology graph and the constructed feature graph\nsimultaneously, which conduce to capture the structural neighborhood\ninformation and the feature-related information. Furthermore, we design a\nMulti-Hop Graph Generator (MHGG), which applies a node-to-hop attention\nmechanism to aggregate node-specific multi-hop neighborhood information\nadaptively. Finally, we apply self-ensembling to form a consistent prediction\nfor unlabeled node representations. Experimental results on five datasets with\ndifferent topological structures demonstrate that our proposed DPGNN\noutperforms all the latest state-of-the-art models on all datasets, which\nproves the superiority and versatility of our model. The source code of our\nmodel is available at https://github.com."
  },
  {
    "id": "arxiv-236",
    "title": "An Interpretable Deep Learning System for Automatically Scoring Request\n  for Proposals",
    "abstract": "The Managed Care system within Medicaid (US Healthcare) uses Request For\nProposals (RFP) to award contracts for various healthcare and related services.\nRFP responses are very detailed documents (hundreds of pages) submitted by\ncompeting organisations to win contracts. Subject matter expertise and domain\nknowledge play an important role in preparing RFP responses along with analysis\nof historical submissions. Automated analysis of these responses through\nNatural Language Processing (NLP) systems can reduce time and effort needed to\nexplore historical responses, and assisting in writing better responses. Our\nwork draws parallels between scoring RFPs and essay scoring models, while\nhighlighting new challenges and the need for interpretability. Typical scoring\nmodels focus on word level impacts to grade essays and other short write-ups.\nWe propose a novel Bi-LSTM based regression model, and provide deeper insight\ninto phrases which latently impact scoring of responses. We contend the merits\nof our proposed methodology using extensive quantitative experiments. We also\nqualitatively asses the impact of important phrases using human evaluators.\nFinally, we introduce a novel problem statement that can be used to further\nimprove the state of the art in NLP based automatic scoring systems.",
    "text": "An Interpretable Deep Learning System for Automatically Scoring Request\n  for Proposals\n\nThe Managed Care system within Medicaid (US Healthcare) uses Request For\nProposals (RFP) to award contracts for various healthcare and related services.\nRFP responses are very detailed documents (hundreds of pages) submitted by\ncompeting organisations to win contracts. Subject matter expertise and domain\nknowledge play an important role in preparing RFP responses along with analysis\nof historical submissions. Automated analysis of these responses through\nNatural Language Processing (NLP) systems can reduce time and effort needed to\nexplore historical responses, and assisting in writing better responses. Our\nwork draws parallels between scoring RFPs and essay scoring models, while\nhighlighting new challenges and the need for interpretability. Typical scoring\nmodels focus on word level impacts to grade essays and other short write-ups.\nWe propose a novel Bi-LSTM based regression model, and provide deeper insight\ninto phrases which latently impact scoring of responses. We contend the merits\nof our proposed methodology using extensive quantitative experiments. We also\nqualitatively asses the impact of important phrases using human evaluators.\nFinally, we introduce a novel problem statement that can be used to further\nimprove the state of the art in NLP based automatic scoring systems."
  },
  {
    "id": "arxiv-237",
    "title": "Variational Recurrent Auto-Encoders",
    "abstract": "In this paper we propose a model that combines the strengths of RNNs and\nSGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used\nfor efficient, large scale unsupervised learning on time series data, mapping\nthe time series data to a latent vector representation. The model is\ngenerative, such that data can be generated from samples of the latent space.\nAn important contribution of this work is that the model can make use of\nunlabeled data in order to facilitate supervised training of RNNs by\ninitialising the weights and network state.",
    "text": "Variational Recurrent Auto-Encoders\n\nIn this paper we propose a model that combines the strengths of RNNs and\nSGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used\nfor efficient, large scale unsupervised learning on time series data, mapping\nthe time series data to a latent vector representation. The model is\ngenerative, such that data can be generated from samples of the latent space.\nAn important contribution of this work is that the model can make use of\nunlabeled data in order to facilitate supervised training of RNNs by\ninitialising the weights and network state."
  },
  {
    "id": "arxiv-238",
    "title": "Finite-Time Performance Bounds and Adaptive Learning Rate Selection for\n  Two Time-Scale Reinforcement Learning",
    "abstract": "We study two time-scale linear stochastic approximation algorithms, which can\nbe used to model well-known reinforcement learning algorithms such as GTD,\nGTD2, and TDC. We present finite-time performance bounds for the case where the\nlearning rate is fixed. The key idea in obtaining these bounds is to use a\nLyapunov function motivated by singular perturbation theory for linear\ndifferential equations. We use the bound to design an adaptive learning rate\nscheme which significantly improves the convergence rate over the known optimal\npolynomial decay rule in our experiments, and can be used to potentially\nimprove the performance of any other schedule where the learning rate is\nchanged at pre-determined time instants.",
    "text": "Finite-Time Performance Bounds and Adaptive Learning Rate Selection for\n  Two Time-Scale Reinforcement Learning\n\nWe study two time-scale linear stochastic approximation algorithms, which can\nbe used to model well-known reinforcement learning algorithms such as GTD,\nGTD2, and TDC. We present finite-time performance bounds for the case where the\nlearning rate is fixed. The key idea in obtaining these bounds is to use a\nLyapunov function motivated by singular perturbation theory for linear\ndifferential equations. We use the bound to design an adaptive learning rate\nscheme which significantly improves the convergence rate over the known optimal\npolynomial decay rule in our experiments, and can be used to potentially\nimprove the performance of any other schedule where the learning rate is\nchanged at pre-determined time instants."
  },
  {
    "id": "arxiv-239",
    "title": "The Local Elasticity of Neural Networks",
    "abstract": "This paper presents a phenomenon in neural networks that we refer to as\n\\textit{local elasticity}. Roughly speaking, a classifier is said to be locally\nelastic if its prediction at a feature vector $\\bx'$ is \\textit{not}\nsignificantly perturbed, after the classifier is updated via stochastic\ngradient descent at a (labeled) feature vector $\\bx$ that is\n\\textit{dissimilar} to $\\bx'$ in a certain sense. This phenomenon is shown to\npersist for neural networks with nonlinear activation functions through\nextensive simulations on real-life and synthetic datasets, whereas this is not\nobserved in linear classifiers. In addition, we offer a geometric\ninterpretation of local elasticity using the neural tangent kernel\n\\citep{jacot2018neural}. Building on top of local elasticity, we obtain\npairwise similarity measures between feature vectors, which can be used for\nclustering in conjunction with $K$-means. The effectiveness of the clustering\nalgorithm on the MNIST and CIFAR-10 datasets in turn corroborates the\nhypothesis of local elasticity of neural networks on real-life data. Finally,\nwe discuss some implications of local elasticity to shed light on several\nintriguing aspects of deep neural networks.",
    "text": "The Local Elasticity of Neural Networks\n\nThis paper presents a phenomenon in neural networks that we refer to as\n\\textit{local elasticity}. Roughly speaking, a classifier is said to be locally\nelastic if its prediction at a feature vector $\\bx'$ is \\textit{not}\nsignificantly perturbed, after the classifier is updated via stochastic\ngradient descent at a (labeled) feature vector $\\bx$ that is\n\\textit{dissimilar} to $\\bx'$ in a certain sense. This phenomenon is shown to\npersist for neural networks with nonlinear activation functions through\nextensive simulations on real-life and synthetic datasets, whereas this is not\nobserved in linear classifiers. In addition, we offer a geometric\ninterpretation of local elasticity using the neural tangent kernel\n\\citep{jacot2018neural}. Building on top of local elasticity, we obtain\npairwise similarity measures between feature vectors, which can be used for\nclustering in conjunction with $K$-means. The effectiveness of the clustering\nalgorithm on the MNIST and CIFAR-10 datasets in turn corroborates the\nhypothesis of local elasticity of neural networks on real-life data. Finally,\nwe discuss some implications of local elasticity to shed light on several\nintriguing aspects of deep neural networks."
  },
  {
    "id": "arxiv-240",
    "title": "Measuring Alignment Bias in Neural Seq2Seq Semantic Parsers",
    "abstract": "Prior to deep learning the semantic parsing community has been interested in\nunderstanding and modeling the range of possible word alignments between\nnatural language sentences and their corresponding meaning representations.\nSequence-to-sequence models changed the research landscape suggesting that we\nno longer need to worry about alignments since they can be learned\nautomatically by means of an attention mechanism. More recently, researchers\nhave started to question such premise. In this work we investigate whether\nseq2seq models can handle both simple and complex alignments. To answer this\nquestion we augment the popular Geo semantic parsing dataset with alignment\nannotations and create Geo-Aligned. We then study the performance of standard\nseq2seq models on the examples that can be aligned monotonically versus\nexamples that require more complex alignments. Our empirical study shows that\nperformance is significantly better over monotonic alignments.",
    "text": "Measuring Alignment Bias in Neural Seq2Seq Semantic Parsers\n\nPrior to deep learning the semantic parsing community has been interested in\nunderstanding and modeling the range of possible word alignments between\nnatural language sentences and their corresponding meaning representations.\nSequence-to-sequence models changed the research landscape suggesting that we\nno longer need to worry about alignments since they can be learned\nautomatically by means of an attention mechanism. More recently, researchers\nhave started to question such premise. In this work we investigate whether\nseq2seq models can handle both simple and complex alignments. To answer this\nquestion we augment the popular Geo semantic parsing dataset with alignment\nannotations and create Geo-Aligned. We then study the performance of standard\nseq2seq models on the examples that can be aligned monotonically versus\nexamples that require more complex alignments. Our empirical study shows that\nperformance is significantly better over monotonic alignments."
  },
  {
    "id": "arxiv-241",
    "title": "FedSmart: An Auto Updating Federated Learning Optimization Mechanism",
    "abstract": "Federated learning has made an important contribution to data\nprivacy-preserving. Many previous works are based on the assumption that the\ndata are independently identically distributed (IID). As a result, the model\nperformance on non-identically independently distributed (non-IID) data is\nbeyond expectation, which is the concrete situation. Some existing methods of\nensuring the model robustness on non-IID data, like the data-sharing strategy\nor pretraining, may lead to privacy leaking. In addition, there exist some\nparticipants who try to poison the model with low-quality data. In this paper,\na performance-based parameter return method for optimization is introduced, we\nterm it FederatedSmart (FedSmart). It optimizes different model for each client\nthrough sharing global gradients, and it extracts the data from each client as\na local validation set, and the accuracy that model achieves in round t\ndetermines the weights of the next round. The experiment results show that\nFedSmart enables the participants to allocate a greater weight to the ones with\nsimilar data distribution.",
    "text": "FedSmart: An Auto Updating Federated Learning Optimization Mechanism\n\nFederated learning has made an important contribution to data\nprivacy-preserving. Many previous works are based on the assumption that the\ndata are independently identically distributed (IID). As a result, the model\nperformance on non-identically independently distributed (non-IID) data is\nbeyond expectation, which is the concrete situation. Some existing methods of\nensuring the model robustness on non-IID data, like the data-sharing strategy\nor pretraining, may lead to privacy leaking. In addition, there exist some\nparticipants who try to poison the model with low-quality data. In this paper,\na performance-based parameter return method for optimization is introduced, we\nterm it FederatedSmart (FedSmart). It optimizes different model for each client\nthrough sharing global gradients, and it extracts the data from each client as\na local validation set, and the accuracy that model achieves in round t\ndetermines the weights of the next round. The experiment results show that\nFedSmart enables the participants to allocate a greater weight to the ones with\nsimilar data distribution."
  },
  {
    "id": "arxiv-242",
    "title": "A New Framework for Machine Intelligence: Concepts and Prototype",
    "abstract": "Machine learning (ML) and artificial intelligence (AI) have become hot topics\nin many information processing areas, from chatbots to scientific data\nanalysis. At the same time, there is uncertainty about the possibility of\nextending predominant ML technologies to become general solutions with\ncontinuous learning capabilities. Here, a simple, yet comprehensive,\ntheoretical framework for intelligent systems is presented. A combination of\nMirror Compositional Representations (MCR) and a Solution-Critic Loop (SCL) is\nproposed as a generic approach for different types of problems. A prototype\nimplementation is presented for document comparison using English Wikipedia\ncorpus.",
    "text": "A New Framework for Machine Intelligence: Concepts and Prototype\n\nMachine learning (ML) and artificial intelligence (AI) have become hot topics\nin many information processing areas, from chatbots to scientific data\nanalysis. At the same time, there is uncertainty about the possibility of\nextending predominant ML technologies to become general solutions with\ncontinuous learning capabilities. Here, a simple, yet comprehensive,\ntheoretical framework for intelligent systems is presented. A combination of\nMirror Compositional Representations (MCR) and a Solution-Critic Loop (SCL) is\nproposed as a generic approach for different types of problems. A prototype\nimplementation is presented for document comparison using English Wikipedia\ncorpus."
  },
  {
    "id": "arxiv-243",
    "title": "The reparameterization trick for acquisition functions",
    "abstract": "Bayesian optimization is a sample-efficient approach to solving global\noptimization problems. Along with a surrogate model, this approach relies on\ntheoretically motivated value heuristics (acquisition functions) to guide the\nsearch process. Maximizing acquisition functions yields the best performance;\nunfortunately, this ideal is difficult to achieve since optimizing acquisition\nfunctions per se is frequently non-trivial. This statement is especially true\nin the parallel setting, where acquisition functions are routinely non-convex,\nhigh-dimensional, and intractable. Here, we demonstrate how many popular\nacquisition functions can be formulated as Gaussian integrals amenable to the\nreparameterization trick and, ensuingly, gradient-based optimization. Further,\nwe use this reparameterized representation to derive an efficient Monte Carlo\nestimator for the upper confidence bound acquisition function in the context of\nparallel selection.",
    "text": "The reparameterization trick for acquisition functions\n\nBayesian optimization is a sample-efficient approach to solving global\noptimization problems. Along with a surrogate model, this approach relies on\ntheoretically motivated value heuristics (acquisition functions) to guide the\nsearch process. Maximizing acquisition functions yields the best performance;\nunfortunately, this ideal is difficult to achieve since optimizing acquisition\nfunctions per se is frequently non-trivial. This statement is especially true\nin the parallel setting, where acquisition functions are routinely non-convex,\nhigh-dimensional, and intractable. Here, we demonstrate how many popular\nacquisition functions can be formulated as Gaussian integrals amenable to the\nreparameterization trick and, ensuingly, gradient-based optimization. Further,\nwe use this reparameterized representation to derive an efficient Monte Carlo\nestimator for the upper confidence bound acquisition function in the context of\nparallel selection."
  },
  {
    "id": "arxiv-244",
    "title": "Towards Aggregating Weighted Feature Attributions",
    "abstract": "Current approaches for explaining machine learning models fall into two\ndistinct classes: antecedent event influence and value attribution. The former\nleverages training instances to describe how much influence a training point\nexerts on a test point, while the latter attempts to attribute value to the\nfeatures most pertinent to a given prediction. In this work, we discuss an\nalgorithm, AVA: Aggregate Valuation of Antecedents, that fuses these two\nexplanation classes to form a new approach to feature attribution that not only\nretrieves local explanations but also captures global patterns learned by a\nmodel. Our experimentation convincingly favors weighting and aggregating\nfeature attributions via AVA.",
    "text": "Towards Aggregating Weighted Feature Attributions\n\nCurrent approaches for explaining machine learning models fall into two\ndistinct classes: antecedent event influence and value attribution. The former\nleverages training instances to describe how much influence a training point\nexerts on a test point, while the latter attempts to attribute value to the\nfeatures most pertinent to a given prediction. In this work, we discuss an\nalgorithm, AVA: Aggregate Valuation of Antecedents, that fuses these two\nexplanation classes to form a new approach to feature attribution that not only\nretrieves local explanations but also captures global patterns learned by a\nmodel. Our experimentation convincingly favors weighting and aggregating\nfeature attributions via AVA."
  },
  {
    "id": "arxiv-245",
    "title": "Quantization Backdoors to Deep Learning Commercial Frameworks",
    "abstract": "Currently, there is a burgeoning demand for deploying deep learning (DL)\nmodels on ubiquitous edge Internet of Things (IoT) devices attributed to their\nlow latency and high privacy preservation. However, DL models are often large\nin size and require large-scale computation, which prevents them from being\nplaced directly onto IoT devices, where resources are constrained and 32-bit\nfloating-point (float-32) operations are unavailable. Commercial framework\n(i.e., a set of toolkits) empowered model quantization is a pragmatic solution\nthat enables DL deployment on mobile devices and embedded systems by\neffortlessly post-quantizing a large high-precision model (e.g., float-32) into\na small low-precision model (e.g., int-8) while retaining the model inference\naccuracy. However, their usability might be threatened by security\nvulnerabilities.\n  This work reveals that the standard quantization toolkits can be abused to\nactivate a backdoor. We demonstrate that a full-precision backdoored model\nwhich does not have any backdoor effect in the presence of a trigger -- as the\nbackdoor is dormant -- can be activated by the default i) TensorFlow-Lite\n(TFLite) quantization, the only product-ready quantization framework to date,\nand ii) the beta released PyTorch Mobile framework. When each of the float-32\nmodels is converted into an int-8 format model through the standard TFLite or\nPytorch Mobile framework's post-training quantization, the backdoor is\nactivated in the quantized model, which shows a stable attack success rate\nclose to 100% upon inputs with the trigger, while it behaves normally upon\nnon-trigger inputs. This work highlights that a stealthy security threat occurs\nwhen an end user utilizes the on-device post-training model quantization\nframeworks, informing security researchers of cross-platform overhaul of DL\nmodels post quantization even if these models pass front-end backdoor\ninspections.",
    "text": "Quantization Backdoors to Deep Learning Commercial Frameworks\n\nCurrently, there is a burgeoning demand for deploying deep learning (DL)\nmodels on ubiquitous edge Internet of Things (IoT) devices attributed to their\nlow latency and high privacy preservation. However, DL models are often large\nin size and require large-scale computation, which prevents them from being\nplaced directly onto IoT devices, where resources are constrained and 32-bit\nfloating-point (float-32) operations are unavailable. Commercial framework\n(i.e., a set of toolkits) empowered model quantization is a pragmatic solution\nthat enables DL deployment on mobile devices and embedded systems by\neffortlessly post-quantizing a large high-precision model (e.g., float-32) into\na small low-precision model (e.g., int-8) while retaining the model inference\naccuracy. However, their usability might be threatened by security\nvulnerabilities.\n  This work reveals that the standard quantization toolkits can be abused to\nactivate a backdoor. We demonstrate that a full-precision backdoored model\nwhich does not have any backdoor effect in the presence of a trigger -- as the\nbackdoor is dormant -- can be activated by the default i) TensorFlow-Lite\n(TFLite) quantization, the only product-ready quantization framework to date,\nand ii) the beta released PyTorch Mobile framework. When each of the float-32\nmodels is converted into an int-8 format model through the standard TFLite or\nPytorch Mobile framework's post-training quantization, the backdoor is\nactivated in the quantized model, which shows a stable attack success rate\nclose to 100% upon inputs with the trigger, while it behaves normally upon\nnon-trigger inputs. This work highlights that a stealthy security threat occurs\nwhen an end user utilizes the on-device post-training model quantization\nframeworks, informing security researchers of cross-platform overhaul of DL\nmodels post quantization even if these models pass front-end backdoor\ninspections."
  },
  {
    "id": "arxiv-246",
    "title": "A Novel Sleep Stage Classification Using CNN Generated by an Efficient\n  Neural Architecture Search with a New Data Processing Trick",
    "abstract": "With the development of automatic sleep stage classification (ASSC)\ntechniques, many classical methods such as k-means, decision tree, and SVM have\nbeen used in automatic sleep stage classification. However, few methods explore\ndeep learning on ASSC. Meanwhile, most deep learning methods require extensive\nexpertise and suffer from a mass of handcrafted steps which are time-consuming\nespecially when dealing with multi-classification tasks. In this paper, we\npropose an efficient five-sleep-stage classification method using convolutional\nneural networks (CNNs) with a novel data processing trick and we design neural\narchitecture search (NAS) technique based on genetic algorithm (GA), NAS-G, to\nsearch for the best CNN architecture. Firstly, we attach each kernel with an\nadaptive coefficient to enhance the signal processing of the inputs. This can\nenhance the propagation of informative features and suppress the propagation of\nuseless features in the early stage of the network. Then, we make full use of\nGA's heuristic search and the advantage of no need for the gradient to search\nfor the best architecture of CNN. This can achieve a CNN with better\nperformance than a handcrafted one in a large search space at the minimum cost.\nWe verify the convergence of our data processing trick and compare the\nperformance of traditional CNNs before and after using our trick. Meanwhile, we\ncompare the performance between the CNN generated through NAS-G and the\ntraditional CNNs with our trick. The experiments demonstrate that the\nconvergence of CNNs with data processing trick is faster than without data\nprocessing trick and the CNN with data processing trick generated by NAS-G\noutperforms the handcrafted counterparts that use the data processing trick\ntoo.",
    "text": "A Novel Sleep Stage Classification Using CNN Generated by an Efficient\n  Neural Architecture Search with a New Data Processing Trick\n\nWith the development of automatic sleep stage classification (ASSC)\ntechniques, many classical methods such as k-means, decision tree, and SVM have\nbeen used in automatic sleep stage classification. However, few methods explore\ndeep learning on ASSC. Meanwhile, most deep learning methods require extensive\nexpertise and suffer from a mass of handcrafted steps which are time-consuming\nespecially when dealing with multi-classification tasks. In this paper, we\npropose an efficient five-sleep-stage classification method using convolutional\nneural networks (CNNs) with a novel data processing trick and we design neural\narchitecture search (NAS) technique based on genetic algorithm (GA), NAS-G, to\nsearch for the best CNN architecture. Firstly, we attach each kernel with an\nadaptive coefficient to enhance the signal processing of the inputs. This can\nenhance the propagation of informative features and suppress the propagation of\nuseless features in the early stage of the network. Then, we make full use of\nGA's heuristic search and the advantage of no need for the gradient to search\nfor the best architecture of CNN. This can achieve a CNN with better\nperformance than a handcrafted one in a large search space at the minimum cost.\nWe verify the convergence of our data processing trick and compare the\nperformance of traditional CNNs before and after using our trick. Meanwhile, we\ncompare the performance between the CNN generated through NAS-G and the\ntraditional CNNs with our trick. The experiments demonstrate that the\nconvergence of CNNs with data processing trick is faster than without data\nprocessing trick and the CNN with data processing trick generated by NAS-G\noutperforms the handcrafted counterparts that use the data processing trick\ntoo."
  },
  {
    "id": "arxiv-247",
    "title": "Quantitative CT texture-based method to predict diagnosis and prognosis of fibrosing interstitial lung disease patterns",
    "abstract": "Purpose: To utilize high-resolution quantitative CT (QCT) imaging features\nfor prediction of diagnosis and prognosis in fibrosing interstitial lung\ndiseases (ILD). Approach: 40 ILD patients (20 usual interstitial pneumonia\n(UIP), 20 non-UIP pattern ILD) were classified by expert consensus of 2\nradiologists and followed for 7 years. Clinical variables were recorded.\nFollowing segmentation of the lung field, a total of 26 texture features were\nextracted using a lattice-based approach (TM model). The TM model was compared\nwith previously histogram-based model (HM) for their abilities to classify UIP\nvs non-UIP. For prognostic assessment, survival analysis was performed\ncomparing the expert diagnostic labels versus TM metrics. Results: In the\nclassification analysis, the TM model outperformed the HM method with AUC of\n0.70. While survival curves of UIP vs non-UIP expert labels in Cox regression\nanalysis were not statistically different, TM QCT features allowed\nstatistically significant partition of the cohort. Conclusions: TM model\noutperformed HM model in distinguishing UIP from non-UIP patterns. Most\nimportantly, TM allows for partitioning of the cohort into distinct survival\ngroups, whereas expert UIP vs non-UIP labeling does not. QCT TM models may\nimprove diagnosis of ILD and offer more accurate prognostication, better\nguiding patient management.",
    "text": "Quantitative CT texture-based method to predict diagnosis and prognosis of fibrosing interstitial lung disease patterns\n\nPurpose: To utilize high-resolution quantitative CT (QCT) imaging features\nfor prediction of diagnosis and prognosis in fibrosing interstitial lung\ndiseases (ILD). Approach: 40 ILD patients (20 usual interstitial pneumonia\n(UIP), 20 non-UIP pattern ILD) were classified by expert consensus of 2\nradiologists and followed for 7 years. Clinical variables were recorded.\nFollowing segmentation of the lung field, a total of 26 texture features were\nextracted using a lattice-based approach (TM model). The TM model was compared\nwith previously histogram-based model (HM) for their abilities to classify UIP\nvs non-UIP. For prognostic assessment, survival analysis was performed\ncomparing the expert diagnostic labels versus TM metrics. Results: In the\nclassification analysis, the TM model outperformed the HM method with AUC of\n0.70. While survival curves of UIP vs non-UIP expert labels in Cox regression\nanalysis were not statistically different, TM QCT features allowed\nstatistically significant partition of the cohort. Conclusions: TM model\noutperformed HM model in distinguishing UIP from non-UIP patterns. Most\nimportantly, TM allows for partitioning of the cohort into distinct survival\ngroups, whereas expert UIP vs non-UIP labeling does not. QCT TM models may\nimprove diagnosis of ILD and offer more accurate prognostication, better\nguiding patient management."
  },
  {
    "id": "arxiv-248",
    "title": "Predictive Value Generalization Bounds",
    "abstract": "In this paper, we study a bi-criterion framework for assessing scoring\nfunctions in the context of binary classification. The positive and negative\npredictive values (ppv and npv, respectively) are conditional probabilities of\nthe true label matching a classifier's predicted label. The usual\nclassification error rate is a linear combination of these probabilities, and\ntherefore, concentration inequalities for the error rate do not yield\nconfidence intervals for the two separate predictive values. We study\ngeneralization properties of scoring functions with respect to predictive\nvalues by deriving new distribution-free large deviation and uniform\nconvergence bounds. The latter bound is stated in terms of a measure of\nfunction class complexity that we call the order coefficient; we relate this\ncombinatorial quantity to the VC-subgraph dimension.",
    "text": "Predictive Value Generalization Bounds\n\nIn this paper, we study a bi-criterion framework for assessing scoring\nfunctions in the context of binary classification. The positive and negative\npredictive values (ppv and npv, respectively) are conditional probabilities of\nthe true label matching a classifier's predicted label. The usual\nclassification error rate is a linear combination of these probabilities, and\ntherefore, concentration inequalities for the error rate do not yield\nconfidence intervals for the two separate predictive values. We study\ngeneralization properties of scoring functions with respect to predictive\nvalues by deriving new distribution-free large deviation and uniform\nconvergence bounds. The latter bound is stated in terms of a measure of\nfunction class complexity that we call the order coefficient; we relate this\ncombinatorial quantity to the VC-subgraph dimension."
  },
  {
    "id": "arxiv-249",
    "title": "Blind Speech Separation and Dereverberation using Neural Beamforming",
    "abstract": "In this paper, we present the Blind Speech Separation and Dereverberation\n(BSSD) network, which performs simultaneous speaker separation, dereverberation\nand speaker identification in a single neural network. Speaker separation is\nguided by a set of predefined spatial cues. Dereverberation is performed by\nusing neural beamforming, and speaker identification is aided by embedding\nvectors and triplet mining. We introduce a frequency-domain model which uses\ncomplex-valued neural networks, and a time-domain variant which performs\nbeamforming in latent space. Further, we propose a block-online mode to process\nlonger audio recordings, as they occur in meeting scenarios. We evaluate our\nsystem in terms of Scale Independent Signal to Distortion Ratio (SI-SDR), Word\nError Rate (WER) and Equal Error Rate (EER).",
    "text": "Blind Speech Separation and Dereverberation using Neural Beamforming\n\nIn this paper, we present the Blind Speech Separation and Dereverberation\n(BSSD) network, which performs simultaneous speaker separation, dereverberation\nand speaker identification in a single neural network. Speaker separation is\nguided by a set of predefined spatial cues. Dereverberation is performed by\nusing neural beamforming, and speaker identification is aided by embedding\nvectors and triplet mining. We introduce a frequency-domain model which uses\ncomplex-valued neural networks, and a time-domain variant which performs\nbeamforming in latent space. Further, we propose a block-online mode to process\nlonger audio recordings, as they occur in meeting scenarios. We evaluate our\nsystem in terms of Scale Independent Signal to Distortion Ratio (SI-SDR), Word\nError Rate (WER) and Equal Error Rate (EER)."
  },
  {
    "id": "arxiv-250",
    "title": "Neural Arabic Question Answering",
    "abstract": "This paper tackles the problem of open domain factual Arabic question\nanswering (QA) using Wikipedia as our knowledge source. This constrains the\nanswer of any question to be a span of text in Wikipedia. Open domain QA for\nArabic entails three challenges: annotated QA datasets in Arabic, large scale\nefficient information retrieval and machine reading comprehension. To deal with\nthe lack of Arabic QA datasets we present the Arabic Reading Comprehension\nDataset (ARCD) composed of 1,395 questions posed by crowdworkers on Wikipedia\narticles, and a machine translation of the Stanford Question Answering Dataset\n(Arabic-SQuAD). Our system for open domain question answering in Arabic (SOQAL)\nis based on two components: (1) a document retriever using a hierarchical\nTF-IDF approach and (2) a neural reading comprehension model using the\npre-trained bi-directional transformer BERT. Our experiments on ARCD indicate\nthe effectiveness of our approach with our BERT-based reader achieving a 61.3\nF1 score, and our open domain system SOQAL achieving a 27.6 F1 score.",
    "text": "Neural Arabic Question Answering\n\nThis paper tackles the problem of open domain factual Arabic question\nanswering (QA) using Wikipedia as our knowledge source. This constrains the\nanswer of any question to be a span of text in Wikipedia. Open domain QA for\nArabic entails three challenges: annotated QA datasets in Arabic, large scale\nefficient information retrieval and machine reading comprehension. To deal with\nthe lack of Arabic QA datasets we present the Arabic Reading Comprehension\nDataset (ARCD) composed of 1,395 questions posed by crowdworkers on Wikipedia\narticles, and a machine translation of the Stanford Question Answering Dataset\n(Arabic-SQuAD). Our system for open domain question answering in Arabic (SOQAL)\nis based on two components: (1) a document retriever using a hierarchical\nTF-IDF approach and (2) a neural reading comprehension model using the\npre-trained bi-directional transformer BERT. Our experiments on ARCD indicate\nthe effectiveness of our approach with our BERT-based reader achieving a 61.3\nF1 score, and our open domain system SOQAL achieving a 27.6 F1 score."
  },
  {
    "id": "arxiv-251",
    "title": "Convolutional Filtering in Simplicial Complexes",
    "abstract": "This paper proposes convolutional filtering for data whose structure can be\nmodeled by a simplicial complex (SC). SCs are mathematical tools that not only\ncapture pairwise relationships as graphs but account also for higher-order\nnetwork structures. These filters are built by following the shift-and-sum\nprinciple of the convolution operation and rely on the Hodge-Laplacians to\nshift the signal within the simplex. But since in SCs we have also\ninter-simplex coupling, we use the incidence matrices to transfer the signal in\nadjacent simplices and build a filter bank to jointly filter signals from\ndifferent levels. We prove some interesting properties for the proposed filter\nbank, including permutation and orientation equivariance, a computational\ncomplexity that is linear in the SC dimension, and a spectral interpretation\nusing the simplicial Fourier transform. We illustrate the proposed approach\nwith numerical experiments.",
    "text": "Convolutional Filtering in Simplicial Complexes\n\nThis paper proposes convolutional filtering for data whose structure can be\nmodeled by a simplicial complex (SC). SCs are mathematical tools that not only\ncapture pairwise relationships as graphs but account also for higher-order\nnetwork structures. These filters are built by following the shift-and-sum\nprinciple of the convolution operation and rely on the Hodge-Laplacians to\nshift the signal within the simplex. But since in SCs we have also\ninter-simplex coupling, we use the incidence matrices to transfer the signal in\nadjacent simplices and build a filter bank to jointly filter signals from\ndifferent levels. We prove some interesting properties for the proposed filter\nbank, including permutation and orientation equivariance, a computational\ncomplexity that is linear in the SC dimension, and a spectral interpretation\nusing the simplicial Fourier transform. We illustrate the proposed approach\nwith numerical experiments."
  },
  {
    "id": "arxiv-252",
    "title": "Guided Exploration of Data Summaries",
    "abstract": "Data summarization is the process of producing interpretable and\nrepresentative subsets of an input dataset. It is usually performed following a\none-shot process with the purpose of finding the best summary. A useful summary\ncontains k individually uniform sets that are collectively diverse to be\nrepresentative. Uniformity addresses interpretability and diversity addresses\nrepresentativity. Finding such as summary is a difficult task when data is\nhighly diverse and large. We examine the applicability of Exploratory Data\nAnalysis (EDA) to data summarization and formalize Eda4Sum, the problem of\nguided exploration of data summaries that seeks to sequentially produce\nconnected summaries with the goal of maximizing their cumulative utility.\nEdA4Sum generalizes one-shot summarization. We propose to solve it with one of\ntwo approaches: (i) Top1Sum which chooses the most useful summary at each step;\n(ii) RLSum which trains a policy with Deep Reinforcement Learning that rewards\nan agent for finding a diverse and new collection of uniform sets at each step.\nWe compare these approaches with one-shot summarization and top-performing EDA\nsolutions. We run extensive experiments on three large datasets. Our results\ndemonstrate the superiority of our approaches for summarizing very large data,\nand the need to provide guidance to domain experts.",
    "text": "Guided Exploration of Data Summaries\n\nData summarization is the process of producing interpretable and\nrepresentative subsets of an input dataset. It is usually performed following a\none-shot process with the purpose of finding the best summary. A useful summary\ncontains k individually uniform sets that are collectively diverse to be\nrepresentative. Uniformity addresses interpretability and diversity addresses\nrepresentativity. Finding such as summary is a difficult task when data is\nhighly diverse and large. We examine the applicability of Exploratory Data\nAnalysis (EDA) to data summarization and formalize Eda4Sum, the problem of\nguided exploration of data summaries that seeks to sequentially produce\nconnected summaries with the goal of maximizing their cumulative utility.\nEdA4Sum generalizes one-shot summarization. We propose to solve it with one of\ntwo approaches: (i) Top1Sum which chooses the most useful summary at each step;\n(ii) RLSum which trains a policy with Deep Reinforcement Learning that rewards\nan agent for finding a diverse and new collection of uniform sets at each step.\nWe compare these approaches with one-shot summarization and top-performing EDA\nsolutions. We run extensive experiments on three large datasets. Our results\ndemonstrate the superiority of our approaches for summarizing very large data,\nand the need to provide guidance to domain experts."
  },
  {
    "id": "arxiv-253",
    "title": "Examining and Combating Spurious Features under Distribution Shift",
    "abstract": "A central goal of machine learning is to learn robust representations that\ncapture the causal relationship between inputs features and output labels.\nHowever, minimizing empirical risk over finite or biased datasets often results\nin models latching on to spurious correlations between the training\ninput/output pairs that are not fundamental to the problem at hand. In this\npaper, we define and analyze robust and spurious representations using the\ninformation-theoretic concept of minimal sufficient statistics. We prove that\neven when there is only bias of the input distribution (i.e. covariate shift),\nmodels can still pick up spurious features from their training data. Group\ndistributionally robust optimization (DRO) provides an effective tool to\nalleviate covariate shift by minimizing the worst-case training loss over a set\nof pre-defined groups. Inspired by our analysis, we demonstrate that group DRO\ncan fail when groups do not directly account for various spurious correlations\nthat occur in the data. To address this, we further propose to minimize the\nworst-case losses over a more flexible set of distributions that are defined on\nthe joint distribution of groups and instances, instead of treating each group\nas a whole at optimization time. Through extensive experiments on one image and\ntwo language tasks, we show that our model is significantly more robust than\ncomparable baselines under various partitions. Our code is available at\nhttps://github.com/violet-zct/group-conditional-DRO.",
    "text": "Examining and Combating Spurious Features under Distribution Shift\n\nA central goal of machine learning is to learn robust representations that\ncapture the causal relationship between inputs features and output labels.\nHowever, minimizing empirical risk over finite or biased datasets often results\nin models latching on to spurious correlations between the training\ninput/output pairs that are not fundamental to the problem at hand. In this\npaper, we define and analyze robust and spurious representations using the\ninformation-theoretic concept of minimal sufficient statistics. We prove that\neven when there is only bias of the input distribution (i.e. covariate shift),\nmodels can still pick up spurious features from their training data. Group\ndistributionally robust optimization (DRO) provides an effective tool to\nalleviate covariate shift by minimizing the worst-case training loss over a set\nof pre-defined groups. Inspired by our analysis, we demonstrate that group DRO\ncan fail when groups do not directly account for various spurious correlations\nthat occur in the data. To address this, we further propose to minimize the\nworst-case losses over a more flexible set of distributions that are defined on\nthe joint distribution of groups and instances, instead of treating each group\nas a whole at optimization time. Through extensive experiments on one image and\ntwo language tasks, we show that our model is significantly more robust than\ncomparable baselines under various partitions. Our code is available at\nhttps://github.com/violet-zct/group-conditional-DRO."
  },
  {
    "id": "arxiv-254",
    "title": "UNO: Uncertainty-aware Noisy-Or Multimodal Fusion for Unanticipated\n  Input Degradation",
    "abstract": "The fusion of multiple sensor modalities, especially through deep learning\narchitectures, has been an active area of study. However, an under-explored\naspect of such work is whether the methods can be robust to degradations across\ntheir input modalities, especially when they must generalize to degradations\nnot seen during training. In this work, we propose an uncertainty-aware fusion\nscheme to effectively fuse inputs that might suffer from a range of known and\nunknown degradations. Specifically, we analyze a number of uncertainty\nmeasures, each of which captures a different aspect of uncertainty, and we\npropose a novel way to fuse degraded inputs by scaling modality-specific output\nsoftmax probabilities. We additionally propose a novel data-dependent spatial\ntemperature scaling method to complement these existing uncertainty measures.\nFinally, we integrate the uncertainty-scaled output from each modality using a\nprobabilistic noisy-or fusion method. In a photo-realistic simulation\nenvironment (AirSim), we show that our method achieves significantly better\nresults on a semantic segmentation task, compared to state-of-art fusion\narchitectures, on a range of degradations (e.g. fog, snow, frost, and various\nother types of noise), some of which are unknown during training. We\nspecifically improve upon the state-of-art[1] by 28% in mean IoU on various\ndegradations. [1] Abhinav Valada, Rohit Mohan, and Wolfram Burgard.\nSelf-Supervised Model Adaptation for Multimodal Semantic Segmentation. In:\narXiv e-prints, arXiv:1808.03833 (Aug. 2018), arXiv:1808.03833. arXiv:\n1808.03833 [cs.CV].",
    "text": "UNO: Uncertainty-aware Noisy-Or Multimodal Fusion for Unanticipated\n  Input Degradation\n\nThe fusion of multiple sensor modalities, especially through deep learning\narchitectures, has been an active area of study. However, an under-explored\naspect of such work is whether the methods can be robust to degradations across\ntheir input modalities, especially when they must generalize to degradations\nnot seen during training. In this work, we propose an uncertainty-aware fusion\nscheme to effectively fuse inputs that might suffer from a range of known and\nunknown degradations. Specifically, we analyze a number of uncertainty\nmeasures, each of which captures a different aspect of uncertainty, and we\npropose a novel way to fuse degraded inputs by scaling modality-specific output\nsoftmax probabilities. We additionally propose a novel data-dependent spatial\ntemperature scaling method to complement these existing uncertainty measures.\nFinally, we integrate the uncertainty-scaled output from each modality using a\nprobabilistic noisy-or fusion method. In a photo-realistic simulation\nenvironment (AirSim), we show that our method achieves significantly better\nresults on a semantic segmentation task, compared to state-of-art fusion\narchitectures, on a range of degradations (e.g. fog, snow, frost, and various\nother types of noise), some of which are unknown during training. We\nspecifically improve upon the state-of-art[1] by 28% in mean IoU on various\ndegradations. [1] Abhinav Valada, Rohit Mohan, and Wolfram Burgard.\nSelf-Supervised Model Adaptation for Multimodal Semantic Segmentation. In:\narXiv e-prints, arXiv:1808.03833 (Aug. 2018), arXiv:1808.03833. arXiv:\n1808.03833 [cs.CV]."
  },
  {
    "id": "arxiv-255",
    "title": "Learning Product Graphs Underlying Smooth Graph Signals",
    "abstract": "Real-world data is often times associated with irregular structures that can\nanalytically be represented as graphs. Having access to this graph, which is\nsometimes trivially evident from domain knowledge, provides a better\nrepresentation of the data and facilitates various information processing\ntasks. However, in cases where the underlying graph is unavailable, it needs to\nbe learned from the data itself for data representation, data processing and\ninference purposes. Existing literature on learning graphs from data has mostly\nconsidered arbitrary graphs, whereas the graphs generating real-world data tend\nto have additional structure that can be incorporated in the graph learning\nprocedure. Structure-aware graph learning methods require learning fewer\nparameters and have the potential to reduce computational, memory and sample\ncomplexities. In light of this, the focus of this paper is to devise a method\nto learn structured graphs from data that are given in the form of product\ngraphs. Product graphs arise naturally in many real-world datasets and provide\nan efficient and compact representation of large-scale graphs through several\nsmaller factor graphs. To this end, first the graph learning problem is posed\nas a linear program, which (on average) outperforms the state-of-the-art graph\nlearning algorithms. This formulation is of independent interest itself as it\nshows that graph learning is possible through a simple linear program.\nAfterwards, an alternating minimization-based algorithm aimed at learning\nvarious types of product graphs is proposed, and local convergence guarantees\nto the true solution are established for this algorithm. Finally the\nperformance gains, reduced sample complexity, and inference capabilities of the\nproposed algorithm over existing methods are also validated through numerical\nsimulations on synthetic and real datasets.",
    "text": "Learning Product Graphs Underlying Smooth Graph Signals\n\nReal-world data is often times associated with irregular structures that can\nanalytically be represented as graphs. Having access to this graph, which is\nsometimes trivially evident from domain knowledge, provides a better\nrepresentation of the data and facilitates various information processing\ntasks. However, in cases where the underlying graph is unavailable, it needs to\nbe learned from the data itself for data representation, data processing and\ninference purposes. Existing literature on learning graphs from data has mostly\nconsidered arbitrary graphs, whereas the graphs generating real-world data tend\nto have additional structure that can be incorporated in the graph learning\nprocedure. Structure-aware graph learning methods require learning fewer\nparameters and have the potential to reduce computational, memory and sample\ncomplexities. In light of this, the focus of this paper is to devise a method\nto learn structured graphs from data that are given in the form of product\ngraphs. Product graphs arise naturally in many real-world datasets and provide\nan efficient and compact representation of large-scale graphs through several\nsmaller factor graphs. To this end, first the graph learning problem is posed\nas a linear program, which (on average) outperforms the state-of-the-art graph\nlearning algorithms. This formulation is of independent interest itself as it\nshows that graph learning is possible through a simple linear program.\nAfterwards, an alternating minimization-based algorithm aimed at learning\nvarious types of product graphs is proposed, and local convergence guarantees\nto the true solution are established for this algorithm. Finally the\nperformance gains, reduced sample complexity, and inference capabilities of the\nproposed algorithm over existing methods are also validated through numerical\nsimulations on synthetic and real datasets."
  },
  {
    "id": "arxiv-256",
    "title": "Temporal Factorization of 3D Convolutional Kernels",
    "abstract": "3D convolutional neural networks are difficult to train because they are\nparameter-expensive and data-hungry. To solve these problems we propose a\nsimple technique for learning 3D convolutional kernels efficiently requiring\nless training data. We achieve this by factorizing the 3D kernel along the\ntemporal dimension, reducing the number of parameters and making training from\ndata more efficient. Additionally we introduce a novel dataset called\nVideo-MNIST to demonstrate the performance of our method. Our method\nsignificantly outperforms the conventional 3D convolution in the low data\nregime (1 to 5 videos per class). Finally, our model achieves competitive\nresults in the high data regime (>10 videos per class) using up to 45% fewer\nparameters.",
    "text": "Temporal Factorization of 3D Convolutional Kernels\n\n3D convolutional neural networks are difficult to train because they are\nparameter-expensive and data-hungry. To solve these problems we propose a\nsimple technique for learning 3D convolutional kernels efficiently requiring\nless training data. We achieve this by factorizing the 3D kernel along the\ntemporal dimension, reducing the number of parameters and making training from\ndata more efficient. Additionally we introduce a novel dataset called\nVideo-MNIST to demonstrate the performance of our method. Our method\nsignificantly outperforms the conventional 3D convolution in the low data\nregime (1 to 5 videos per class). Finally, our model achieves competitive\nresults in the high data regime (>10 videos per class) using up to 45% fewer\nparameters."
  },
  {
    "id": "arxiv-257",
    "title": "Predict better with less training data using a QNN",
    "abstract": "Over the past decade, machine learning revolutionized vision-based quality\nassessment for which convolutional neural networks (CNNs) have now become the\nstandard. In this paper, we consider a potential next step in this development\nand describe a quanvolutional neural network (QNN) algorithm that efficiently\nmaps classical image data to quantum states and allows for reliable image\nanalysis. We practically demonstrate how to leverage quantum devices in\ncomputer vision and how to introduce quantum convolutions into classical CNNs.\nDealing with a real world use case in industrial quality control, we implement\nour hybrid QNN model within the PennyLane framework and empirically observe it\nto achieve better predictions using much fewer training data than classical\nCNNs. In other words, we empirically observe a genuine quantum advantage for an\nindustrial application where the advantage is due to superior data encoding.",
    "text": "Predict better with less training data using a QNN\n\nOver the past decade, machine learning revolutionized vision-based quality\nassessment for which convolutional neural networks (CNNs) have now become the\nstandard. In this paper, we consider a potential next step in this development\nand describe a quanvolutional neural network (QNN) algorithm that efficiently\nmaps classical image data to quantum states and allows for reliable image\nanalysis. We practically demonstrate how to leverage quantum devices in\ncomputer vision and how to introduce quantum convolutions into classical CNNs.\nDealing with a real world use case in industrial quality control, we implement\nour hybrid QNN model within the PennyLane framework and empirically observe it\nto achieve better predictions using much fewer training data than classical\nCNNs. In other words, we empirically observe a genuine quantum advantage for an\nindustrial application where the advantage is due to superior data encoding."
  },
  {
    "id": "arxiv-258",
    "title": "BATS: Best Action Trajectory Stitching",
    "abstract": "The problem of offline reinforcement learning focuses on learning a good\npolicy from a log of environment interactions. Past efforts for developing\nalgorithms in this area have revolved around introducing constraints to online\nreinforcement learning algorithms to ensure the actions of the learned policy\nare constrained to the logged data. In this work, we explore an alternative\napproach by planning on the fixed dataset directly. Specifically, we introduce\nan algorithm which forms a tabular Markov Decision Process (MDP) over the\nlogged data by adding new transitions to the dataset. We do this by using\nlearned dynamics models to plan short trajectories between states. Since exact\nvalue iteration can be performed on this constructed MDP, it becomes easy to\nidentify which trajectories are advantageous to add to the MDP. Crucially,\nsince most transitions in this MDP come from the logged data, trajectories from\nthe MDP can be rolled out for long periods with confidence. We prove that this\nproperty allows one to make upper and lower bounds on the value function up to\nappropriate distance metrics. Finally, we demonstrate empirically how\nalgorithms that uniformly constrain the learned policy to the entire dataset\ncan result in unwanted behavior, and we show an example in which simply\nbehavior cloning the optimal policy of the MDP created by our algorithm avoids\nthis problem.",
    "text": "BATS: Best Action Trajectory Stitching\n\nThe problem of offline reinforcement learning focuses on learning a good\npolicy from a log of environment interactions. Past efforts for developing\nalgorithms in this area have revolved around introducing constraints to online\nreinforcement learning algorithms to ensure the actions of the learned policy\nare constrained to the logged data. In this work, we explore an alternative\napproach by planning on the fixed dataset directly. Specifically, we introduce\nan algorithm which forms a tabular Markov Decision Process (MDP) over the\nlogged data by adding new transitions to the dataset. We do this by using\nlearned dynamics models to plan short trajectories between states. Since exact\nvalue iteration can be performed on this constructed MDP, it becomes easy to\nidentify which trajectories are advantageous to add to the MDP. Crucially,\nsince most transitions in this MDP come from the logged data, trajectories from\nthe MDP can be rolled out for long periods with confidence. We prove that this\nproperty allows one to make upper and lower bounds on the value function up to\nappropriate distance metrics. Finally, we demonstrate empirically how\nalgorithms that uniformly constrain the learned policy to the entire dataset\ncan result in unwanted behavior, and we show an example in which simply\nbehavior cloning the optimal policy of the MDP created by our algorithm avoids\nthis problem."
  },
  {
    "id": "arxiv-259",
    "title": "Synthetic Data and Simulators for Recommendation Systems: Current State\n  and Future Directions",
    "abstract": "Synthetic data and simulators have the potential to markedly improve the\nperformance and robustness of recommendation systems. These approaches have\nalready had a beneficial impact in other machine-learning driven fields. We\nidentify and discuss a key trade-off between data fidelity and privacy in the\npast work on synthetic data and simulators for recommendation systems. For the\nimportant use case of predicting algorithm rankings on real data from synthetic\ndata, we provide motivation and current successes versus limitations. Finally\nwe outline a number of exciting future directions for recommendation systems\nthat we believe deserve further attention and work, including mixing real and\nsynthetic data, feedback in dataset generation, robust simulations, and\nprivacy-preserving methods.",
    "text": "Synthetic Data and Simulators for Recommendation Systems: Current State\n  and Future Directions\n\nSynthetic data and simulators have the potential to markedly improve the\nperformance and robustness of recommendation systems. These approaches have\nalready had a beneficial impact in other machine-learning driven fields. We\nidentify and discuss a key trade-off between data fidelity and privacy in the\npast work on synthetic data and simulators for recommendation systems. For the\nimportant use case of predicting algorithm rankings on real data from synthetic\ndata, we provide motivation and current successes versus limitations. Finally\nwe outline a number of exciting future directions for recommendation systems\nthat we believe deserve further attention and work, including mixing real and\nsynthetic data, feedback in dataset generation, robust simulations, and\nprivacy-preserving methods."
  },
  {
    "id": "arxiv-260",
    "title": "Anomaly Subsequence Detection with Dynamic Local Density for Time Series",
    "abstract": "Anomaly subsequence detection is to detect inconsistent data, which always\ncontains important information, among time series. Due to the high\ndimensionality of the time series, traditional anomaly detection often requires\na large time overhead; furthermore, even if the dimensionality reduction\ntechniques can improve the efficiency, they will lose some information and\nsuffer from time drift and parameter tuning. In this paper, we propose a new\nanomaly subsequence detection with Dynamic Local Density Estimation (DLDE) to\nimprove the detection effect without losing the trend information by\ndynamically dividing the time series using Time Split Tree. In order to avoid\nthe impact of the hash function and the randomness of dynamic time segments,\nensemble learning is used. Experimental results on different types of data sets\nverify that the proposed model outperforms the state-of-art methods, and the\naccuracy has big improvement.",
    "text": "Anomaly Subsequence Detection with Dynamic Local Density for Time Series\n\nAnomaly subsequence detection is to detect inconsistent data, which always\ncontains important information, among time series. Due to the high\ndimensionality of the time series, traditional anomaly detection often requires\na large time overhead; furthermore, even if the dimensionality reduction\ntechniques can improve the efficiency, they will lose some information and\nsuffer from time drift and parameter tuning. In this paper, we propose a new\nanomaly subsequence detection with Dynamic Local Density Estimation (DLDE) to\nimprove the detection effect without losing the trend information by\ndynamically dividing the time series using Time Split Tree. In order to avoid\nthe impact of the hash function and the randomness of dynamic time segments,\nensemble learning is used. Experimental results on different types of data sets\nverify that the proposed model outperforms the state-of-art methods, and the\naccuracy has big improvement."
  },
  {
    "id": "arxiv-261",
    "title": "RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality",
    "abstract": "Compared to convolutional layers, fully-connected (FC) layers are better at\nmodeling the long-range dependencies but worse at capturing the local patterns,\nhence usually less favored for image recognition. In this paper, we propose a\nmethodology, Locality Injection, to incorporate local priors into an FC layer\nvia merging the trained parameters of a parallel conv kernel into the FC\nkernel. Locality Injection can be viewed as a novel Structural\nRe-parameterization method since it equivalently converts the structures via\ntransforming the parameters. Based on that, we propose a multi-layer-perceptron\n(MLP) block named RepMLP Block, which uses three FC layers to extract features,\nand a novel architecture named RepMLPNet. The hierarchical design distinguishes\nRepMLPNet from the other concurrently proposed vision MLPs. As it produces\nfeature maps of different levels, it qualifies as a backbone model for\ndownstream tasks like semantic segmentation. Our results reveal that 1)\nLocality Injection is a general methodology for MLP models; 2) RepMLPNet has\nfavorable accuracy-efficiency trade-off compared to the other MLPs; 3)\nRepMLPNet is the first MLP that seamlessly transfer to Cityscapes semantic\nsegmentation. The code and models are available at\nhttps://github.com/DingXiaoH/RepMLP.",
    "text": "RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality\n\nCompared to convolutional layers, fully-connected (FC) layers are better at\nmodeling the long-range dependencies but worse at capturing the local patterns,\nhence usually less favored for image recognition. In this paper, we propose a\nmethodology, Locality Injection, to incorporate local priors into an FC layer\nvia merging the trained parameters of a parallel conv kernel into the FC\nkernel. Locality Injection can be viewed as a novel Structural\nRe-parameterization method since it equivalently converts the structures via\ntransforming the parameters. Based on that, we propose a multi-layer-perceptron\n(MLP) block named RepMLP Block, which uses three FC layers to extract features,\nand a novel architecture named RepMLPNet. The hierarchical design distinguishes\nRepMLPNet from the other concurrently proposed vision MLPs. As it produces\nfeature maps of different levels, it qualifies as a backbone model for\ndownstream tasks like semantic segmentation. Our results reveal that 1)\nLocality Injection is a general methodology for MLP models; 2) RepMLPNet has\nfavorable accuracy-efficiency trade-off compared to the other MLPs; 3)\nRepMLPNet is the first MLP that seamlessly transfer to Cityscapes semantic\nsegmentation. The code and models are available at\nhttps://github.com/DingXiaoH/RepMLP."
  },
  {
    "id": "arxiv-262",
    "title": "Deep Attribute Networks",
    "abstract": "Obtaining compact and discriminative features is one of the major challenges\nin many of the real-world image classification tasks such as face verification\nand object recognition. One possible approach is to represent input image on\nthe basis of high-level features that carry semantic meaning which humans can\nunderstand. In this paper, a model coined deep attribute network (DAN) is\nproposed to address this issue. For an input image, the model outputs the\nattributes of the input image without performing any classification. The\nefficacy of the proposed model is evaluated on unconstrained face verification\nand real-world object recognition tasks using the LFW and the a-PASCAL\ndatasets. We demonstrate the potential of deep learning for attribute-based\nclassification by showing comparable results with existing state-of-the-art\nresults. Once properly trained, the DAN is fast and does away with calculating\nlow-level features which are maybe unreliable and computationally expensive.",
    "text": "Deep Attribute Networks\n\nObtaining compact and discriminative features is one of the major challenges\nin many of the real-world image classification tasks such as face verification\nand object recognition. One possible approach is to represent input image on\nthe basis of high-level features that carry semantic meaning which humans can\nunderstand. In this paper, a model coined deep attribute network (DAN) is\nproposed to address this issue. For an input image, the model outputs the\nattributes of the input image without performing any classification. The\nefficacy of the proposed model is evaluated on unconstrained face verification\nand real-world object recognition tasks using the LFW and the a-PASCAL\ndatasets. We demonstrate the potential of deep learning for attribute-based\nclassification by showing comparable results with existing state-of-the-art\nresults. Once properly trained, the DAN is fast and does away with calculating\nlow-level features which are maybe unreliable and computationally expensive."
  },
  {
    "id": "arxiv-263",
    "title": "Predicting Yelp Star Reviews Based on Network Structure with Deep\n  Learning",
    "abstract": "In this paper, we tackle the real-world problem of predicting Yelp\nstar-review rating based on business features (such as images, descriptions),\nuser features (average previous ratings), and, of particular interest, network\nproperties (which businesses has a user rated before). We compare multiple\nmodels on different sets of features -- from simple linear regression on\nnetwork features only to deep learning models on network and item features.\n  In recent years, breakthroughs in deep learning have led to increased\naccuracy in common supervised learning tasks, such as image classification,\ncaptioning, and language understanding. However, the idea of combining deep\nlearning with network feature and structure appears to be novel. While the\nproblem of predicting future interactions in a network has been studied at\nlength, these approaches have often ignored either node-specific data or global\nstructure.\n  We demonstrate that taking a mixed approach combining both node-level\nfeatures and network information can effectively be used to predict Yelp-review\nstar ratings. We evaluate on the Yelp dataset by splitting our data along the\ntime dimension (as would naturally occur in the real-world) and comparing our\nmodel against others which do no take advantage of the network structure and/or\ndeep learning.",
    "text": "Predicting Yelp Star Reviews Based on Network Structure with Deep\n  Learning\n\nIn this paper, we tackle the real-world problem of predicting Yelp\nstar-review rating based on business features (such as images, descriptions),\nuser features (average previous ratings), and, of particular interest, network\nproperties (which businesses has a user rated before). We compare multiple\nmodels on different sets of features -- from simple linear regression on\nnetwork features only to deep learning models on network and item features.\n  In recent years, breakthroughs in deep learning have led to increased\naccuracy in common supervised learning tasks, such as image classification,\ncaptioning, and language understanding. However, the idea of combining deep\nlearning with network feature and structure appears to be novel. While the\nproblem of predicting future interactions in a network has been studied at\nlength, these approaches have often ignored either node-specific data or global\nstructure.\n  We demonstrate that taking a mixed approach combining both node-level\nfeatures and network information can effectively be used to predict Yelp-review\nstar ratings. We evaluate on the Yelp dataset by splitting our data along the\ntime dimension (as would naturally occur in the real-world) and comparing our\nmodel against others which do no take advantage of the network structure and/or\ndeep learning."
  },
  {
    "id": "arxiv-264",
    "title": "$\\{\\text{PF}\\}^2\\text{ES}$: Parallel Feasible Pareto Frontier Entropy\n  Search for Multi-Objective Bayesian Optimization Under Unknown Constraints",
    "abstract": "We present Parallel Feasible Pareto Frontier Entropy Search\n($\\{\\text{PF}\\}^2$ES) -- a novel information-theoretic acquisition function for\nmulti-objective Bayesian optimization. Although information-theoretic\napproaches regularly provide state-of-the-art optimization, they are not yet\nwidely used in the context of constrained multi-objective optimization. Due to\nthe complexity of characterizing mutual information between candidate\nevaluations and (feasible) Pareto frontiers, existing approaches must employ\nsevere approximations that significantly hamper their performance. By instead\nusing a variational lower bound, $\\{\\text{PF}\\}^2$ES provides a low cost and\naccurate estimate of the mutual information for the parallel setting (where\nmultiple evaluations must be chosen for each optimization step). Moreover, we\nare able to interpret our proposed acquisition function by exploring direct\nlinks with other popular multi-objective acquisition functions. We benchmark\n$\\{\\text{PF}\\}^2$ES across synthetic and real-life problems, demonstrating its\ncompetitive performance for batch optimization across synthetic and real-world\nproblems including vehicle and electronic filter design.",
    "text": "$\\{\\text{PF}\\}^2\\text{ES}$: Parallel Feasible Pareto Frontier Entropy\n  Search for Multi-Objective Bayesian Optimization Under Unknown Constraints\n\nWe present Parallel Feasible Pareto Frontier Entropy Search\n($\\{\\text{PF}\\}^2$ES) -- a novel information-theoretic acquisition function for\nmulti-objective Bayesian optimization. Although information-theoretic\napproaches regularly provide state-of-the-art optimization, they are not yet\nwidely used in the context of constrained multi-objective optimization. Due to\nthe complexity of characterizing mutual information between candidate\nevaluations and (feasible) Pareto frontiers, existing approaches must employ\nsevere approximations that significantly hamper their performance. By instead\nusing a variational lower bound, $\\{\\text{PF}\\}^2$ES provides a low cost and\naccurate estimate of the mutual information for the parallel setting (where\nmultiple evaluations must be chosen for each optimization step). Moreover, we\nare able to interpret our proposed acquisition function by exploring direct\nlinks with other popular multi-objective acquisition functions. We benchmark\n$\\{\\text{PF}\\}^2$ES across synthetic and real-life problems, demonstrating its\ncompetitive performance for batch optimization across synthetic and real-world\nproblems including vehicle and electronic filter design."
  },
  {
    "id": "arxiv-265",
    "title": "Tuning Particle Accelerators with Safety Constraints using Bayesian\n  Optimization",
    "abstract": "Tuning machine parameters of particle accelerators is a repetitive and\ntime-consuming task, that is challenging to automate. While many off-the-shelf\noptimization algorithms are available, in practice their use is limited because\nmost methods do not account for safety-critical constraints that apply to each\niteration, including loss signals or step-size limitations. One notable\nexception is safe Bayesian optimization, which is a data-driven tuning approach\nfor global optimization with noisy feedback. We propose and evaluate a step\nsize-limited variant of safe Bayesian optimization on two research faculties of\nthe Paul Scherrer Institut (PSI): a) the Swiss Free Electron Laser (SwissFEL)\nand b) the High-Intensity Proton Accelerator (HIPA). We report promising\nexperimental results on both machines, tuning up to 16 parameters subject to\nmore than 200 constraints.",
    "text": "Tuning Particle Accelerators with Safety Constraints using Bayesian\n  Optimization\n\nTuning machine parameters of particle accelerators is a repetitive and\ntime-consuming task, that is challenging to automate. While many off-the-shelf\noptimization algorithms are available, in practice their use is limited because\nmost methods do not account for safety-critical constraints that apply to each\niteration, including loss signals or step-size limitations. One notable\nexception is safe Bayesian optimization, which is a data-driven tuning approach\nfor global optimization with noisy feedback. We propose and evaluate a step\nsize-limited variant of safe Bayesian optimization on two research faculties of\nthe Paul Scherrer Institut (PSI): a) the Swiss Free Electron Laser (SwissFEL)\nand b) the High-Intensity Proton Accelerator (HIPA). We report promising\nexperimental results on both machines, tuning up to 16 parameters subject to\nmore than 200 constraints."
  },
  {
    "id": "arxiv-266",
    "title": "Stochastic Dual Coordinate Ascent with Adaptive Probabilities",
    "abstract": "This paper introduces AdaSDCA: an adaptive variant of stochastic dual\ncoordinate ascent (SDCA) for solving the regularized empirical risk\nminimization problems. Our modification consists in allowing the method\nadaptively change the probability distribution over the dual variables\nthroughout the iterative process. AdaSDCA achieves provably better complexity\nbound than SDCA with the best fixed probability distribution, known as\nimportance sampling. However, it is of a theoretical character as it is\nexpensive to implement. We also propose AdaSDCA+: a practical variant which in\nour experiments outperforms existing non-adaptive methods.",
    "text": "Stochastic Dual Coordinate Ascent with Adaptive Probabilities\n\nThis paper introduces AdaSDCA: an adaptive variant of stochastic dual\ncoordinate ascent (SDCA) for solving the regularized empirical risk\nminimization problems. Our modification consists in allowing the method\nadaptively change the probability distribution over the dual variables\nthroughout the iterative process. AdaSDCA achieves provably better complexity\nbound than SDCA with the best fixed probability distribution, known as\nimportance sampling. However, it is of a theoretical character as it is\nexpensive to implement. We also propose AdaSDCA+: a practical variant which in\nour experiments outperforms existing non-adaptive methods."
  },
  {
    "id": "arxiv-267",
    "title": "Random Features for Compositional Kernels",
    "abstract": "We describe and analyze a simple random feature scheme (RFS) from prescribed\ncompositional kernels. The compositional kernels we use are inspired by the\nstructure of convolutional neural networks and kernels. The resulting scheme\nyields sparse and efficiently computable features. Each random feature can be\nrepresented as an algebraic expression over a small number of (random) paths in\na composition tree. Thus, compositional random features can be stored\ncompactly. The discrete nature of the generation process enables de-duplication\nof repeated features, further compacting the representation and increasing the\ndiversity of the embeddings. Our approach complements and can be combined with\nprevious random feature schemes.",
    "text": "Random Features for Compositional Kernels\n\nWe describe and analyze a simple random feature scheme (RFS) from prescribed\ncompositional kernels. The compositional kernels we use are inspired by the\nstructure of convolutional neural networks and kernels. The resulting scheme\nyields sparse and efficiently computable features. Each random feature can be\nrepresented as an algebraic expression over a small number of (random) paths in\na composition tree. Thus, compositional random features can be stored\ncompactly. The discrete nature of the generation process enables de-duplication\nof repeated features, further compacting the representation and increasing the\ndiversity of the embeddings. Our approach complements and can be combined with\nprevious random feature schemes."
  },
  {
    "id": "arxiv-268",
    "title": "Investigating Biases in Textual Entailment Datasets",
    "abstract": "The ability to understand logical relationships between sentences is an\nimportant task in language understanding. To aid in progress for this task,\nresearchers have collected datasets for machine learning and evaluation of\ncurrent systems. However, like in the crowdsourced Visual Question Answering\n(VQA) task, some biases in the data inevitably occur. In our experiments, we\nfind that performing classification on just the hypotheses on the SNLI dataset\nyields an accuracy of 64%. We analyze the bias extent in the SNLI and the\nMultiNLI dataset, discuss its implication, and propose a simple method to\nreduce the biases in the datasets.",
    "text": "Investigating Biases in Textual Entailment Datasets\n\nThe ability to understand logical relationships between sentences is an\nimportant task in language understanding. To aid in progress for this task,\nresearchers have collected datasets for machine learning and evaluation of\ncurrent systems. However, like in the crowdsourced Visual Question Answering\n(VQA) task, some biases in the data inevitably occur. In our experiments, we\nfind that performing classification on just the hypotheses on the SNLI dataset\nyields an accuracy of 64%. We analyze the bias extent in the SNLI and the\nMultiNLI dataset, discuss its implication, and propose a simple method to\nreduce the biases in the datasets."
  },
  {
    "id": "arxiv-269",
    "title": "Bayesian Modeling of Intersectional Fairness: The Variance of Bias",
    "abstract": "Intersectionality is a framework that analyzes how interlocking systems of\npower and oppression affect individuals along overlapping dimensions including\nrace, gender, sexual orientation, class, and disability. Intersectionality\ntheory therefore implies it is important that fairness in artificial\nintelligence systems be protected with regard to multi-dimensional protected\nattributes. However, the measurement of fairness becomes statistically\nchallenging in the multi-dimensional setting due to data sparsity, which\nincreases rapidly in the number of dimensions, and in the values per dimension.\nWe present a Bayesian probabilistic modeling approach for the reliable,\ndata-efficient estimation of fairness with multi-dimensional protected\nattributes, which we apply to two existing intersectional fairness metrics.\nExperimental results on census data and the COMPAS criminal justice recidivism\ndataset demonstrate the utility of our methodology, and show that Bayesian\nmethods are valuable for the modeling and measurement of fairness in an\nintersectional context.",
    "text": "Bayesian Modeling of Intersectional Fairness: The Variance of Bias\n\nIntersectionality is a framework that analyzes how interlocking systems of\npower and oppression affect individuals along overlapping dimensions including\nrace, gender, sexual orientation, class, and disability. Intersectionality\ntheory therefore implies it is important that fairness in artificial\nintelligence systems be protected with regard to multi-dimensional protected\nattributes. However, the measurement of fairness becomes statistically\nchallenging in the multi-dimensional setting due to data sparsity, which\nincreases rapidly in the number of dimensions, and in the values per dimension.\nWe present a Bayesian probabilistic modeling approach for the reliable,\ndata-efficient estimation of fairness with multi-dimensional protected\nattributes, which we apply to two existing intersectional fairness metrics.\nExperimental results on census data and the COMPAS criminal justice recidivism\ndataset demonstrate the utility of our methodology, and show that Bayesian\nmethods are valuable for the modeling and measurement of fairness in an\nintersectional context."
  },
  {
    "id": "arxiv-270",
    "title": "Anomaly Detection using Principles of Human Perception",
    "abstract": "In the fields of statistics and unsupervised machine learning a fundamental\nand well-studied problem is anomaly detection. Anomalies are difficult to\ndefine, yet many algorithms have been proposed. Underlying the approaches is\nthe nebulous understanding that anomalies are rare, unusual or inconsistent\nwith the majority of data. The present work provides a philosophical treatise\nto clearly define anomalies and develops an algorithm for their efficient\ndetection with minimal user intervention. Inspired by the Gestalt School of\nPsychology and the Helmholtz principle of human perception, anomalies are\nassumed to be observations that are unexpected to occur with respect to certain\ngroupings made by the majority of the data. Under appropriate random variable\nmodelling anomalies are directly found in a set of data by a uniform and\nindependent random assumption of the distribution of constituent elements of\nthe observations, with anomalies corresponding to those observations where the\nexpectation of the number of occurrences of the elements in a given view is\n$<1$. Starting from fundamental principles of human perception an unsupervised\nanomaly detection algorithm is developed that is simple, real-time and\nparameter-free. Experiments suggest it as a competing choice for univariate\ndata with promising results on the detection of global anomalies in\nmultivariate data.",
    "text": "Anomaly Detection using Principles of Human Perception\n\nIn the fields of statistics and unsupervised machine learning a fundamental\nand well-studied problem is anomaly detection. Anomalies are difficult to\ndefine, yet many algorithms have been proposed. Underlying the approaches is\nthe nebulous understanding that anomalies are rare, unusual or inconsistent\nwith the majority of data. The present work provides a philosophical treatise\nto clearly define anomalies and develops an algorithm for their efficient\ndetection with minimal user intervention. Inspired by the Gestalt School of\nPsychology and the Helmholtz principle of human perception, anomalies are\nassumed to be observations that are unexpected to occur with respect to certain\ngroupings made by the majority of the data. Under appropriate random variable\nmodelling anomalies are directly found in a set of data by a uniform and\nindependent random assumption of the distribution of constituent elements of\nthe observations, with anomalies corresponding to those observations where the\nexpectation of the number of occurrences of the elements in a given view is\n$<1$. Starting from fundamental principles of human perception an unsupervised\nanomaly detection algorithm is developed that is simple, real-time and\nparameter-free. Experiments suggest it as a competing choice for univariate\ndata with promising results on the detection of global anomalies in\nmultivariate data."
  },
  {
    "id": "arxiv-271",
    "title": "Kernel Reconstruction ICA for Sparse Representation",
    "abstract": "Independent Component Analysis (ICA) is an effective unsupervised tool to\nlearn statistically independent representation. However, ICA is not only\nsensitive to whitening but also difficult to learn an over-complete basis.\nConsequently, ICA with soft Reconstruction cost(RICA) was presented to learn\nsparse representations with over-complete basis even on unwhitened data.\nWhereas RICA is infeasible to represent the data with nonlinear structure due\nto its intrinsic linearity. In addition, RICA is essentially an unsupervised\nmethod and can not utilize the class information. In this paper, we propose a\nkernel ICA model with reconstruction constraint (kRICA) to capture the\nnonlinear features. To bring in the class information, we further extend the\nunsupervised kRICA to a supervised one by introducing a discrimination\nconstraint, namely d-kRICA. This constraint leads to learn a structured basis\nconsisted of basis vectors from different basis subsets corresponding to\ndifferent class labels. Then each subset will sparsely represent well for its\nown class but not for the others. Furthermore, data samples belonging to the\nsame class will have similar representations, and thereby the learned sparse\nrepresentations can take more discriminative power. Experimental results\nvalidate the effectiveness of kRICA and d-kRICA for image classification.",
    "text": "Kernel Reconstruction ICA for Sparse Representation\n\nIndependent Component Analysis (ICA) is an effective unsupervised tool to\nlearn statistically independent representation. However, ICA is not only\nsensitive to whitening but also difficult to learn an over-complete basis.\nConsequently, ICA with soft Reconstruction cost(RICA) was presented to learn\nsparse representations with over-complete basis even on unwhitened data.\nWhereas RICA is infeasible to represent the data with nonlinear structure due\nto its intrinsic linearity. In addition, RICA is essentially an unsupervised\nmethod and can not utilize the class information. In this paper, we propose a\nkernel ICA model with reconstruction constraint (kRICA) to capture the\nnonlinear features. To bring in the class information, we further extend the\nunsupervised kRICA to a supervised one by introducing a discrimination\nconstraint, namely d-kRICA. This constraint leads to learn a structured basis\nconsisted of basis vectors from different basis subsets corresponding to\ndifferent class labels. Then each subset will sparsely represent well for its\nown class but not for the others. Furthermore, data samples belonging to the\nsame class will have similar representations, and thereby the learned sparse\nrepresentations can take more discriminative power. Experimental results\nvalidate the effectiveness of kRICA and d-kRICA for image classification."
  },
  {
    "id": "arxiv-272",
    "title": "Neuro-Symbolic Artificial Intelligence (AI) for Intent based Semantic\n  Communication",
    "abstract": "Intent-based networks that integrate sophisticated machine reasoning\ntechnologies will be a cornerstone of future wireless 6G systems. Intent-based\ncommunication requires the network to consider the semantics (meanings) and\neffectiveness (at end-user) of the data transmission. This is essential if 6G\nsystems are to communicate reliably with fewer bits while simultaneously\nproviding connectivity to heterogeneous users. In this paper, contrary to state\nof the art, which lacks explainability of data, the framework of neuro-symbolic\nartificial intelligence (NeSy AI) is proposed as a pillar for learning causal\nstructure behind the observed data. In particular, the emerging concept of\ngenerative flow networks (GFlowNet) is leveraged for the first time in a\nwireless system to learn the probabilistic structure which generates the data.\nFurther, a novel optimization problem for learning the optimal encoding and\ndecoding functions is rigorously formulated with the intent of achieving higher\nsemantic reliability. Novel analytical formulations are developed to define key\nmetrics for semantic message transmission, including semantic distortion,\nsemantic similarity, and semantic reliability. These semantic measure functions\nrely on the proposed definition of semantic content of the knowledge base and\nthis information measure is reflective of the nodes' reasoning capabilities.\nSimulation results validate the ability to communicate efficiently (with less\nbits but same semantics) and significantly better compared to a conventional\nsystem which does not exploit the reasoning capabilities.",
    "text": "Neuro-Symbolic Artificial Intelligence (AI) for Intent based Semantic\n  Communication\n\nIntent-based networks that integrate sophisticated machine reasoning\ntechnologies will be a cornerstone of future wireless 6G systems. Intent-based\ncommunication requires the network to consider the semantics (meanings) and\neffectiveness (at end-user) of the data transmission. This is essential if 6G\nsystems are to communicate reliably with fewer bits while simultaneously\nproviding connectivity to heterogeneous users. In this paper, contrary to state\nof the art, which lacks explainability of data, the framework of neuro-symbolic\nartificial intelligence (NeSy AI) is proposed as a pillar for learning causal\nstructure behind the observed data. In particular, the emerging concept of\ngenerative flow networks (GFlowNet) is leveraged for the first time in a\nwireless system to learn the probabilistic structure which generates the data.\nFurther, a novel optimization problem for learning the optimal encoding and\ndecoding functions is rigorously formulated with the intent of achieving higher\nsemantic reliability. Novel analytical formulations are developed to define key\nmetrics for semantic message transmission, including semantic distortion,\nsemantic similarity, and semantic reliability. These semantic measure functions\nrely on the proposed definition of semantic content of the knowledge base and\nthis information measure is reflective of the nodes' reasoning capabilities.\nSimulation results validate the ability to communicate efficiently (with less\nbits but same semantics) and significantly better compared to a conventional\nsystem which does not exploit the reasoning capabilities."
  },
  {
    "id": "arxiv-273",
    "title": "Mix and Match: Learning-free Controllable Text Generation using Energy\n  Language Models",
    "abstract": "Recent work on controlled text generation has either required attribute-based\nfine-tuning of the base language model (LM), or has restricted the\nparameterization of the attribute discriminator to be compatible with the base\nautoregressive LM. In this work, we propose Mix and Match LM, a global\nscore-based alternative for controllable text generation that combines\narbitrary pre-trained black-box models for achieving the desired attributes in\nthe generated text without involving any fine-tuning or structural assumptions\nabout the black-box models. We interpret the task of controllable generation as\ndrawing samples from an energy-based model whose energy values are a linear\ncombination of scores from black-box models that are separately responsible for\nfluency, the control attribute, and faithfulness to any conditioning context.\nWe use a Metropolis-Hastings sampling scheme to sample from this energy-based\nmodel using bidirectional context and global attribute features. We validate\nthe effectiveness of our approach on various controlled generation and\nstyle-based text revision tasks by outperforming recently proposed methods that\ninvolve extra training, fine-tuning, or restrictive assumptions over the form\nof models.",
    "text": "Mix and Match: Learning-free Controllable Text Generation using Energy\n  Language Models\n\nRecent work on controlled text generation has either required attribute-based\nfine-tuning of the base language model (LM), or has restricted the\nparameterization of the attribute discriminator to be compatible with the base\nautoregressive LM. In this work, we propose Mix and Match LM, a global\nscore-based alternative for controllable text generation that combines\narbitrary pre-trained black-box models for achieving the desired attributes in\nthe generated text without involving any fine-tuning or structural assumptions\nabout the black-box models. We interpret the task of controllable generation as\ndrawing samples from an energy-based model whose energy values are a linear\ncombination of scores from black-box models that are separately responsible for\nfluency, the control attribute, and faithfulness to any conditioning context.\nWe use a Metropolis-Hastings sampling scheme to sample from this energy-based\nmodel using bidirectional context and global attribute features. We validate\nthe effectiveness of our approach on various controlled generation and\nstyle-based text revision tasks by outperforming recently proposed methods that\ninvolve extra training, fine-tuning, or restrictive assumptions over the form\nof models."
  },
  {
    "id": "arxiv-274",
    "title": "On Efficient Multilevel Clustering via Wasserstein Distances",
    "abstract": "We propose a novel approach to the problem of multilevel clustering, which\naims to simultaneously partition data in each group and discover grouping\npatterns among groups in a potentially large hierarchically structured corpus\nof data. Our method involves a joint optimization formulation over several\nspaces of discrete probability measures, which are endowed with Wasserstein\ndistance metrics. We propose several variants of this problem, which admit fast\noptimization algorithms, by exploiting the connection to the problem of finding\nWasserstein barycenters. Consistency properties are established for the\nestimates of both local and global clusters. Finally, experimental results with\nboth synthetic and real data are presented to demonstrate the flexibility and\nscalability of the proposed approach.",
    "text": "On Efficient Multilevel Clustering via Wasserstein Distances\n\nWe propose a novel approach to the problem of multilevel clustering, which\naims to simultaneously partition data in each group and discover grouping\npatterns among groups in a potentially large hierarchically structured corpus\nof data. Our method involves a joint optimization formulation over several\nspaces of discrete probability measures, which are endowed with Wasserstein\ndistance metrics. We propose several variants of this problem, which admit fast\noptimization algorithms, by exploiting the connection to the problem of finding\nWasserstein barycenters. Consistency properties are established for the\nestimates of both local and global clusters. Finally, experimental results with\nboth synthetic and real data are presented to demonstrate the flexibility and\nscalability of the proposed approach."
  },
  {
    "id": "arxiv-275",
    "title": "Self-supervised learning for autonomous vehicles perception: A\n  conciliation between analytical and learning methods",
    "abstract": "Nowadays, supervised deep learning techniques yield the best state-of-the-art\nprediction performances for a wide variety of computer vision tasks. However,\nsuch supervised techniques generally require a large amount of manually labeled\ntraining data. In the context of autonomous vehicles perception, this\nrequirement is critical, as the distribution of sensor data can continuously\nchange and include several unexpected variations. It turns out that a category\nof learning techniques, referred to as self-supervised learning (SSL), consists\nof replacing the manual labeling effort by an automatic labeling process.\nThanks to their ability to learn on the application time and in varying\nenvironments, state-of-the-art SSL techniques provide a valid alternative to\nsupervised learning for a variety of different tasks, including long-range\ntraversable area segmentation, moving obstacle instance segmentation, long-term\nmoving obstacle tracking, or depth map prediction. In this tutorial-style\narticle, we present an overview and a general formalization of the concept of\nself-supervised learning (SSL) for autonomous vehicles perception. This\nformalization provides helpful guidelines for developing novel frameworks based\non generic SSL principles. Moreover, it enables to point out significant\nchallenges in the design of future SSL systems.",
    "text": "Self-supervised learning for autonomous vehicles perception: A\n  conciliation between analytical and learning methods\n\nNowadays, supervised deep learning techniques yield the best state-of-the-art\nprediction performances for a wide variety of computer vision tasks. However,\nsuch supervised techniques generally require a large amount of manually labeled\ntraining data. In the context of autonomous vehicles perception, this\nrequirement is critical, as the distribution of sensor data can continuously\nchange and include several unexpected variations. It turns out that a category\nof learning techniques, referred to as self-supervised learning (SSL), consists\nof replacing the manual labeling effort by an automatic labeling process.\nThanks to their ability to learn on the application time and in varying\nenvironments, state-of-the-art SSL techniques provide a valid alternative to\nsupervised learning for a variety of different tasks, including long-range\ntraversable area segmentation, moving obstacle instance segmentation, long-term\nmoving obstacle tracking, or depth map prediction. In this tutorial-style\narticle, we present an overview and a general formalization of the concept of\nself-supervised learning (SSL) for autonomous vehicles perception. This\nformalization provides helpful guidelines for developing novel frameworks based\non generic SSL principles. Moreover, it enables to point out significant\nchallenges in the design of future SSL systems."
  },
  {
    "id": "arxiv-276",
    "title": "Faster and better: a machine learning approach to corner detection",
    "abstract": "The repeatability and efficiency of a corner detector determines how likely\nit is to be useful in a real-world application. The repeatability is importand\nbecause the same scene viewed from different positions should yield features\nwhich correspond to the same real-world 3D locations [Schmid et al 2000]. The\nefficiency is important because this determines whether the detector combined\nwith further processing can operate at frame rate.\n  Three advances are described in this paper. First, we present a new heuristic\nfor feature detection, and using machine learning we derive a feature detector\nfrom this which can fully process live PAL video using less than 5% of the\navailable processing time. By comparison, most other detectors cannot even\noperate at frame rate (Harris detector 115%, SIFT 195%). Second, we generalize\nthe detector, allowing it to be optimized for repeatability, with little loss\nof efficiency. Third, we carry out a rigorous comparison of corner detectors\nbased on the above repeatability criterion applied to 3D scenes. We show that\ndespite being principally constructed for speed, on these stringent tests, our\nheuristic detector significantly outperforms existing feature detectors.\nFinally, the comparison demonstrates that using machine learning produces\nsignificant improvements in repeatability, yielding a detector that is both\nvery fast and very high quality.",
    "text": "Faster and better: a machine learning approach to corner detection\n\nThe repeatability and efficiency of a corner detector determines how likely\nit is to be useful in a real-world application. The repeatability is importand\nbecause the same scene viewed from different positions should yield features\nwhich correspond to the same real-world 3D locations [Schmid et al 2000]. The\nefficiency is important because this determines whether the detector combined\nwith further processing can operate at frame rate.\n  Three advances are described in this paper. First, we present a new heuristic\nfor feature detection, and using machine learning we derive a feature detector\nfrom this which can fully process live PAL video using less than 5% of the\navailable processing time. By comparison, most other detectors cannot even\noperate at frame rate (Harris detector 115%, SIFT 195%). Second, we generalize\nthe detector, allowing it to be optimized for repeatability, with little loss\nof efficiency. Third, we carry out a rigorous comparison of corner detectors\nbased on the above repeatability criterion applied to 3D scenes. We show that\ndespite being principally constructed for speed, on these stringent tests, our\nheuristic detector significantly outperforms existing feature detectors.\nFinally, the comparison demonstrates that using machine learning produces\nsignificant improvements in repeatability, yielding a detector that is both\nvery fast and very high quality."
  },
  {
    "id": "arxiv-277",
    "title": "Matrix Cofactorization for Joint Representation Learning and Supervised\n  Classification -- Application to Hyperspectral Image Analysis",
    "abstract": "Supervised classification and representation learning are two widely used\nclasses of methods to analyze multivariate images. Although complementary,\nthese methods have been scarcely considered jointly in a hierarchical modeling.\nIn this paper, a method coupling these two approaches is designed using a\nmatrix cofactorization formulation. Each task is modeled as a factorization\nmatrix problem and a term relating both coding matrices is then introduced to\ndrive an appropriate coupling. The link can be interpreted as a clustering\noperation over a low-dimensional representation vectors. The attribution\nvectors of the clustering are then used as features vectors for the\nclassification task, i.e., the coding vectors of the corresponding\nfactorization problem. A proximal gradient descent algorithm, ensuring\nconvergence to a critical point of the objective function, is then derived to\nsolve the resulting non-convex non-smooth optimization problem. An evaluation\nof the proposed method is finally conducted both on synthetic and real data in\nthe specific context of hyperspectral image interpretation, unifying two\nstandard analysis techniques, namely unmixing and classification.",
    "text": "Matrix Cofactorization for Joint Representation Learning and Supervised\n  Classification -- Application to Hyperspectral Image Analysis\n\nSupervised classification and representation learning are two widely used\nclasses of methods to analyze multivariate images. Although complementary,\nthese methods have been scarcely considered jointly in a hierarchical modeling.\nIn this paper, a method coupling these two approaches is designed using a\nmatrix cofactorization formulation. Each task is modeled as a factorization\nmatrix problem and a term relating both coding matrices is then introduced to\ndrive an appropriate coupling. The link can be interpreted as a clustering\noperation over a low-dimensional representation vectors. The attribution\nvectors of the clustering are then used as features vectors for the\nclassification task, i.e., the coding vectors of the corresponding\nfactorization problem. A proximal gradient descent algorithm, ensuring\nconvergence to a critical point of the objective function, is then derived to\nsolve the resulting non-convex non-smooth optimization problem. An evaluation\nof the proposed method is finally conducted both on synthetic and real data in\nthe specific context of hyperspectral image interpretation, unifying two\nstandard analysis techniques, namely unmixing and classification."
  },
  {
    "id": "arxiv-278",
    "title": "Live Multi-Streaming and Donation Recommendations via Coupled\n  Donation-Response Tensor Factorization",
    "abstract": "In contrast to traditional online videos, live multi-streaming supports\nreal-time social interactions between multiple streamers and viewers, such as\ndonations. However, donation and multi-streaming channel recommendations are\nchallenging due to complicated streamer and viewer relations, asymmetric\ncommunications, and the tradeoff between personal interests and group\ninteractions. In this paper, we introduce Multi-Stream Party (MSP) and\nformulate a new multi-streaming recommendation problem, called Donation and MSP\nRecommendation (DAMRec). We propose Multi-stream Party Recommender System\n(MARS) to extract latent features via socio-temporal coupled donation-response\ntensor factorization for donation and MSP recommendations. Experimental results\non Twitch and Douyu manifest that MARS significantly outperforms existing\nrecommenders by at least 38.8% in terms of hit ratio and mean average\nprecision.",
    "text": "Live Multi-Streaming and Donation Recommendations via Coupled\n  Donation-Response Tensor Factorization\n\nIn contrast to traditional online videos, live multi-streaming supports\nreal-time social interactions between multiple streamers and viewers, such as\ndonations. However, donation and multi-streaming channel recommendations are\nchallenging due to complicated streamer and viewer relations, asymmetric\ncommunications, and the tradeoff between personal interests and group\ninteractions. In this paper, we introduce Multi-Stream Party (MSP) and\nformulate a new multi-streaming recommendation problem, called Donation and MSP\nRecommendation (DAMRec). We propose Multi-stream Party Recommender System\n(MARS) to extract latent features via socio-temporal coupled donation-response\ntensor factorization for donation and MSP recommendations. Experimental results\non Twitch and Douyu manifest that MARS significantly outperforms existing\nrecommenders by at least 38.8% in terms of hit ratio and mean average\nprecision."
  },
  {
    "id": "arxiv-279",
    "title": "Scenario Submodular Cover",
    "abstract": "Many problems in Machine Learning can be modeled as submodular optimization\nproblems. Recent work has focused on stochastic or adaptive versions of these\nproblems. We consider the Scenario Submodular Cover problem, which is a\ncounterpart to the Stochastic Submodular Cover problem studied by Golovin and\nKrause. In Scenario Submodular Cover, the goal is to produce a cover with\nminimum expected cost, where the expectation is with respect to an empirical\njoint distribution, given as input by a weighted sample of realizations. In\ncontrast, in Stochastic Submodular Cover, the variables of the input\ndistribution are assumed to be independent, and the distribution of each\nvariable is given as input. Building on algorithms developed by Cicalese et al.\nand Golovin and Krause for related problems, we give two approximation\nalgorithms for Scenario Submodular Cover over discrete distributions. The first\nachieves an approximation factor of O(log Qm), where m is the size of the\nsample and Q is the goal utility. The second, simpler algorithm achieves an\napproximation bound of O(log QW), where Q is the goal utility and W is the sum\nof the integer weights. (Both bounds assume an integer-valued utility\nfunction.) Our results yield approximation bounds for other problems involving\nnon-independent distributions that are explicitly specified by their support.",
    "text": "Scenario Submodular Cover\n\nMany problems in Machine Learning can be modeled as submodular optimization\nproblems. Recent work has focused on stochastic or adaptive versions of these\nproblems. We consider the Scenario Submodular Cover problem, which is a\ncounterpart to the Stochastic Submodular Cover problem studied by Golovin and\nKrause. In Scenario Submodular Cover, the goal is to produce a cover with\nminimum expected cost, where the expectation is with respect to an empirical\njoint distribution, given as input by a weighted sample of realizations. In\ncontrast, in Stochastic Submodular Cover, the variables of the input\ndistribution are assumed to be independent, and the distribution of each\nvariable is given as input. Building on algorithms developed by Cicalese et al.\nand Golovin and Krause for related problems, we give two approximation\nalgorithms for Scenario Submodular Cover over discrete distributions. The first\nachieves an approximation factor of O(log Qm), where m is the size of the\nsample and Q is the goal utility. The second, simpler algorithm achieves an\napproximation bound of O(log QW), where Q is the goal utility and W is the sum\nof the integer weights. (Both bounds assume an integer-valued utility\nfunction.) Our results yield approximation bounds for other problems involving\nnon-independent distributions that are explicitly specified by their support."
  },
  {
    "id": "arxiv-280",
    "title": "Heuristic Dynamic Programming for Adaptive Virtual Synchronous\n  Generators",
    "abstract": "In this paper a neural network heuristic dynamic programing (HDP) is used for\noptimal control of the virtual inertia based control of grid connected three\nphase inverters. It is shown that the conventional virtual inertia controllers\nare not suited for non inductive grids. A neural network based controller is\nproposed to adapt to any impedance angle. Applying an adaptive dynamic\nprogramming controller instead of a supervised controlled method enables the\nsystem to adjust itself to different conditions. The proposed HDP consists of\ntwo subnetworks, critic network and action network. These networks can be\ntrained during the same training cycle to decrease the training time. The\nsimulation results confirm that the proposed neural network HDP controller\nperforms better than the traditional direct fed voltage and reactive power\ncontrollers in virtual inertia control schemes.",
    "text": "Heuristic Dynamic Programming for Adaptive Virtual Synchronous\n  Generators\n\nIn this paper a neural network heuristic dynamic programing (HDP) is used for\noptimal control of the virtual inertia based control of grid connected three\nphase inverters. It is shown that the conventional virtual inertia controllers\nare not suited for non inductive grids. A neural network based controller is\nproposed to adapt to any impedance angle. Applying an adaptive dynamic\nprogramming controller instead of a supervised controlled method enables the\nsystem to adjust itself to different conditions. The proposed HDP consists of\ntwo subnetworks, critic network and action network. These networks can be\ntrained during the same training cycle to decrease the training time. The\nsimulation results confirm that the proposed neural network HDP controller\nperforms better than the traditional direct fed voltage and reactive power\ncontrollers in virtual inertia control schemes."
  },
  {
    "id": "arxiv-281",
    "title": "Evaluating model calibration in classification",
    "abstract": "Probabilistic classifiers output a probability distribution on target classes\nrather than just a class prediction. Besides providing a clear separation of\nprediction and decision making, the main advantage of probabilistic models is\ntheir ability to represent uncertainty about predictions. In safety-critical\napplications, it is pivotal for a model to possess an adequate sense of\nuncertainty, which for probabilistic classifiers translates into outputting\nprobability distributions that are consistent with the empirical frequencies\nobserved from realized outcomes. A classifier with such a property is called\ncalibrated. In this work, we develop a general theoretical calibration\nevaluation framework grounded in probability theory, and point out subtleties\npresent in model calibration evaluation that lead to refined interpretations of\nexisting evaluation techniques. Lastly, we propose new ways to quantify and\nvisualize miscalibration in probabilistic classification, including novel\nmultidimensional reliability diagrams.",
    "text": "Evaluating model calibration in classification\n\nProbabilistic classifiers output a probability distribution on target classes\nrather than just a class prediction. Besides providing a clear separation of\nprediction and decision making, the main advantage of probabilistic models is\ntheir ability to represent uncertainty about predictions. In safety-critical\napplications, it is pivotal for a model to possess an adequate sense of\nuncertainty, which for probabilistic classifiers translates into outputting\nprobability distributions that are consistent with the empirical frequencies\nobserved from realized outcomes. A classifier with such a property is called\ncalibrated. In this work, we develop a general theoretical calibration\nevaluation framework grounded in probability theory, and point out subtleties\npresent in model calibration evaluation that lead to refined interpretations of\nexisting evaluation techniques. Lastly, we propose new ways to quantify and\nvisualize miscalibration in probabilistic classification, including novel\nmultidimensional reliability diagrams."
  },
  {
    "id": "arxiv-282",
    "title": "Improved Deep Speaker Feature Learning for Text-Dependent Speaker\n  Recognition",
    "abstract": "A deep learning approach has been proposed recently to derive speaker\nidentifies (d-vector) by a deep neural network (DNN). This approach has been\napplied to text-dependent speaker recognition tasks and shows reasonable\nperformance gains when combined with the conventional i-vector approach.\nAlthough promising, the existing d-vector implementation still can not compete\nwith the i-vector baseline. This paper presents two improvements for the deep\nlearning approach: a phonedependent DNN structure to normalize phone variation,\nand a new scoring approach based on dynamic time warping (DTW). Experiments on\na text-dependent speaker recognition task demonstrated that the proposed\nmethods can provide considerable performance improvement over the existing\nd-vector implementation.",
    "text": "Improved Deep Speaker Feature Learning for Text-Dependent Speaker\n  Recognition\n\nA deep learning approach has been proposed recently to derive speaker\nidentifies (d-vector) by a deep neural network (DNN). This approach has been\napplied to text-dependent speaker recognition tasks and shows reasonable\nperformance gains when combined with the conventional i-vector approach.\nAlthough promising, the existing d-vector implementation still can not compete\nwith the i-vector baseline. This paper presents two improvements for the deep\nlearning approach: a phonedependent DNN structure to normalize phone variation,\nand a new scoring approach based on dynamic time warping (DTW). Experiments on\na text-dependent speaker recognition task demonstrated that the proposed\nmethods can provide considerable performance improvement over the existing\nd-vector implementation."
  },
  {
    "id": "arxiv-283",
    "title": "An Insect-Inspired Randomly, Weighted Neural Network with Random Fourier\n  Features For Neuro-Symbolic Relational Learning",
    "abstract": "Insects, such as fruit flies and honey bees, can solve simple associative\nlearning tasks and learn abstract concepts such as \"sameness\" and \"difference\",\nwhich is viewed as a higher-order cognitive function and typically thought to\ndepend on top-down neocortical processing. Empirical research with fruit flies\nstrongly supports that a randomized representational architecture is used in\nolfactory processing in insect brains. Based on these results, we propose a\nRandomly Weighted Feature Network (RWFN) that incorporates randomly drawn,\nuntrained weights in an encoder that uses an adapted linear model as a decoder.\nThe randomized projections between input neurons and higher-order processing\ncenters in the input brain is mimicked in RWFN by a single-hidden-layer neural\nnetwork that specially structures latent representations in the hidden layer\nusing random Fourier features that better represent complex relationships\nbetween inputs using kernel approximation. Because of this special\nrepresentation, RWFNs can effectively learn the degree of relationship among\ninputs by training only a linear decoder model. We compare the performance of\nRWFNs to LTNs for Semantic Image Interpretation (SII) tasks that have been used\nas a representative example of how LTNs utilize reasoning over first-order\nlogic to surpass the performance of solely data-driven methods. We demonstrate\nthat compared to LTNs, RWFNs can achieve better or similar performance for both\nobject classification and detection of the part-of relations between objects in\nSII tasks while using much far fewer learnable parameters (1:62 ratio) and a\nfaster learning process (1:2 ratio of running speed). Furthermore, we show that\nbecause the randomized weights do not depend on the data, several decoders can\nshare a single randomized encoder, giving RWFNs a unique economy of spatial\nscale for simultaneous classification tasks.",
    "text": "An Insect-Inspired Randomly, Weighted Neural Network with Random Fourier\n  Features For Neuro-Symbolic Relational Learning\n\nInsects, such as fruit flies and honey bees, can solve simple associative\nlearning tasks and learn abstract concepts such as \"sameness\" and \"difference\",\nwhich is viewed as a higher-order cognitive function and typically thought to\ndepend on top-down neocortical processing. Empirical research with fruit flies\nstrongly supports that a randomized representational architecture is used in\nolfactory processing in insect brains. Based on these results, we propose a\nRandomly Weighted Feature Network (RWFN) that incorporates randomly drawn,\nuntrained weights in an encoder that uses an adapted linear model as a decoder.\nThe randomized projections between input neurons and higher-order processing\ncenters in the input brain is mimicked in RWFN by a single-hidden-layer neural\nnetwork that specially structures latent representations in the hidden layer\nusing random Fourier features that better represent complex relationships\nbetween inputs using kernel approximation. Because of this special\nrepresentation, RWFNs can effectively learn the degree of relationship among\ninputs by training only a linear decoder model. We compare the performance of\nRWFNs to LTNs for Semantic Image Interpretation (SII) tasks that have been used\nas a representative example of how LTNs utilize reasoning over first-order\nlogic to surpass the performance of solely data-driven methods. We demonstrate\nthat compared to LTNs, RWFNs can achieve better or similar performance for both\nobject classification and detection of the part-of relations between objects in\nSII tasks while using much far fewer learnable parameters (1:62 ratio) and a\nfaster learning process (1:2 ratio of running speed). Furthermore, we show that\nbecause the randomized weights do not depend on the data, several decoders can\nshare a single randomized encoder, giving RWFNs a unique economy of spatial\nscale for simultaneous classification tasks."
  },
  {
    "id": "arxiv-284",
    "title": "Game on Random Environment, Mean-field Langevin System and Neural\n  Networks",
    "abstract": "In this paper we study a type of games regularized by the relative entropy,\nwhere the players' strategies are coupled through a random environment\nvariable. Besides the existence and the uniqueness of equilibria of such games,\nwe prove that the marginal laws of the corresponding mean-field Langevin\nsystems can converge towards the games' equilibria in different settings. As\napplications, the dynamic games can be treated as games on a random environment\nwhen one treats the time horizon as the environment. In practice, our results\ncan be applied to analysing the stochastic gradient descent algorithm for deep\nneural networks in the context of supervised learning as well as for the\ngenerative adversarial networks.",
    "text": "Game on Random Environment, Mean-field Langevin System and Neural\n  Networks\n\nIn this paper we study a type of games regularized by the relative entropy,\nwhere the players' strategies are coupled through a random environment\nvariable. Besides the existence and the uniqueness of equilibria of such games,\nwe prove that the marginal laws of the corresponding mean-field Langevin\nsystems can converge towards the games' equilibria in different settings. As\napplications, the dynamic games can be treated as games on a random environment\nwhen one treats the time horizon as the environment. In practice, our results\ncan be applied to analysing the stochastic gradient descent algorithm for deep\nneural networks in the context of supervised learning as well as for the\ngenerative adversarial networks."
  },
  {
    "id": "arxiv-285",
    "title": "A Quasi-isometric Embedding Algorithm",
    "abstract": "The Whitney embedding theorem gives an upper bound on the smallest embedding\ndimension of a manifold. If a data set lies on a manifold, a random projection\ninto this reduced dimension will retain the manifold structure. Here we present\nan algorithm to find a projection that distorts the data as little as possible.",
    "text": "A Quasi-isometric Embedding Algorithm\n\nThe Whitney embedding theorem gives an upper bound on the smallest embedding\ndimension of a manifold. If a data set lies on a manifold, a random projection\ninto this reduced dimension will retain the manifold structure. Here we present\nan algorithm to find a projection that distorts the data as little as possible."
  },
  {
    "id": "arxiv-286",
    "title": "Structure-Aware Audio-to-Score Alignment using Progressively Dilated\n  Convolutional Neural Networks",
    "abstract": "The identification of structural differences between a music performance and\nthe score is a challenging yet integral step of audio-to-score alignment, an\nimportant subtask of music information retrieval. We present a novel method to\ndetect such differences between the score and performance for a given piece of\nmusic using progressively dilated convolutional neural networks. Our method\nincorporates varying dilation rates at different layers to capture both\nshort-term and long-term context, and can be employed successfully in the\npresence of limited annotated data. We conduct experiments on audio recordings\nof real performances that differ structurally from the score, and our results\ndemonstrate that our models outperform standard methods for structure-aware\naudio-to-score alignment.",
    "text": "Structure-Aware Audio-to-Score Alignment using Progressively Dilated\n  Convolutional Neural Networks\n\nThe identification of structural differences between a music performance and\nthe score is a challenging yet integral step of audio-to-score alignment, an\nimportant subtask of music information retrieval. We present a novel method to\ndetect such differences between the score and performance for a given piece of\nmusic using progressively dilated convolutional neural networks. Our method\nincorporates varying dilation rates at different layers to capture both\nshort-term and long-term context, and can be employed successfully in the\npresence of limited annotated data. We conduct experiments on audio recordings\nof real performances that differ structurally from the score, and our results\ndemonstrate that our models outperform standard methods for structure-aware\naudio-to-score alignment."
  },
  {
    "id": "arxiv-287",
    "title": "Gaussian Kernel in Quantum Learning",
    "abstract": "The Gaussian kernel is a very popular kernel function used in many machine\nlearning algorithms, especially in support vector machines (SVMs). It is more\noften used than polynomial kernels when learning from nonlinear datasets, and\nis usually employed in formulating the classical SVM for nonlinear problems. In\n[3], Rebentrost et al. discussed an elegant quantum version of a least square\nsupport vector machine using quantum polynomial kernels, which is exponentially\nfaster than the classical counterpart. This paper demonstrates a quantum\nversion of the Gaussian kernel and analyzes its runtime complexity using the\nquantum random access memory (QRAM) in the context of quantum SVM. Our analysis\nshows that the runtime computational complexity of the quantum Gaussian kernel\nseems to be significantly faster as compared to its classical version.",
    "text": "Gaussian Kernel in Quantum Learning\n\nThe Gaussian kernel is a very popular kernel function used in many machine\nlearning algorithms, especially in support vector machines (SVMs). It is more\noften used than polynomial kernels when learning from nonlinear datasets, and\nis usually employed in formulating the classical SVM for nonlinear problems. In\n[3], Rebentrost et al. discussed an elegant quantum version of a least square\nsupport vector machine using quantum polynomial kernels, which is exponentially\nfaster than the classical counterpart. This paper demonstrates a quantum\nversion of the Gaussian kernel and analyzes its runtime complexity using the\nquantum random access memory (QRAM) in the context of quantum SVM. Our analysis\nshows that the runtime computational complexity of the quantum Gaussian kernel\nseems to be significantly faster as compared to its classical version."
  },
  {
    "id": "arxiv-288",
    "title": "Improved Soft Actor-Critic: Mixing Prioritized Off-Policy Samples with\n  On-Policy Experience",
    "abstract": "Soft Actor-Critic (SAC) is an off-policy actor-critic reinforcement learning\nalgorithm, essentially based on entropy regularization. SAC trains a policy by\nmaximizing the trade-off between expected return and entropy (randomness in the\npolicy). It has achieved state-of-the-art performance on a range of\ncontinuous-control benchmark tasks, outperforming prior on-policy and\noff-policy methods. SAC works in an off-policy fashion where data are sampled\nuniformly from past experiences (stored in a buffer) using which parameters of\nthe policy and value function networks are updated. We propose certain crucial\nmodifications for boosting the performance of SAC and make it more sample\nefficient. In our proposed improved SAC, we firstly introduce a new\nprioritization scheme for selecting better samples from the experience replay\nbuffer. Secondly we use a mixture of the prioritized off-policy data with the\nlatest on-policy data for training the policy and the value function networks.\nWe compare our approach with the vanilla SAC and some recent variants of SAC\nand show that our approach outperforms the said algorithmic benchmarks. It is\ncomparatively more stable and sample efficient when tested on a number of\ncontinuous control tasks in MuJoCo environments.",
    "text": "Improved Soft Actor-Critic: Mixing Prioritized Off-Policy Samples with\n  On-Policy Experience\n\nSoft Actor-Critic (SAC) is an off-policy actor-critic reinforcement learning\nalgorithm, essentially based on entropy regularization. SAC trains a policy by\nmaximizing the trade-off between expected return and entropy (randomness in the\npolicy). It has achieved state-of-the-art performance on a range of\ncontinuous-control benchmark tasks, outperforming prior on-policy and\noff-policy methods. SAC works in an off-policy fashion where data are sampled\nuniformly from past experiences (stored in a buffer) using which parameters of\nthe policy and value function networks are updated. We propose certain crucial\nmodifications for boosting the performance of SAC and make it more sample\nefficient. In our proposed improved SAC, we firstly introduce a new\nprioritization scheme for selecting better samples from the experience replay\nbuffer. Secondly we use a mixture of the prioritized off-policy data with the\nlatest on-policy data for training the policy and the value function networks.\nWe compare our approach with the vanilla SAC and some recent variants of SAC\nand show that our approach outperforms the said algorithmic benchmarks. It is\ncomparatively more stable and sample efficient when tested on a number of\ncontinuous control tasks in MuJoCo environments."
  },
  {
    "id": "arxiv-289",
    "title": "Panoptic nuScenes: A Large-Scale Benchmark for LiDAR Panoptic\n  Segmentation and Tracking",
    "abstract": "Panoptic scene understanding and tracking of dynamic agents are essential for\nrobots and automated vehicles to navigate in urban environments. As LiDARs\nprovide accurate illumination-independent geometric depictions of the scene,\nperforming these tasks using LiDAR point clouds provides reliable predictions.\nHowever, existing datasets lack diversity in the type of urban scenes and have\na limited number of dynamic object instances which hinders both learning of\nthese tasks as well as credible benchmarking of the developed methods. In this\npaper, we introduce the large-scale Panoptic nuScenes benchmark dataset that\nextends our popular nuScenes dataset with point-wise groundtruth annotations\nfor semantic segmentation, panoptic segmentation, and panoptic tracking tasks.\nTo facilitate comparison, we provide several strong baselines for each of these\ntasks on our proposed dataset. Moreover, we analyze the drawbacks of the\nexisting metrics for panoptic tracking and propose the novel instance-centric\nPAT metric that addresses the concerns. We present exhaustive experiments that\ndemonstrate the utility of Panoptic nuScenes compared to existing datasets and\nmake the online evaluation server available at nuScenes.org. We believe that\nthis extension will accelerate the research of novel methods for scene\nunderstanding of dynamic urban environments.",
    "text": "Panoptic nuScenes: A Large-Scale Benchmark for LiDAR Panoptic\n  Segmentation and Tracking\n\nPanoptic scene understanding and tracking of dynamic agents are essential for\nrobots and automated vehicles to navigate in urban environments. As LiDARs\nprovide accurate illumination-independent geometric depictions of the scene,\nperforming these tasks using LiDAR point clouds provides reliable predictions.\nHowever, existing datasets lack diversity in the type of urban scenes and have\na limited number of dynamic object instances which hinders both learning of\nthese tasks as well as credible benchmarking of the developed methods. In this\npaper, we introduce the large-scale Panoptic nuScenes benchmark dataset that\nextends our popular nuScenes dataset with point-wise groundtruth annotations\nfor semantic segmentation, panoptic segmentation, and panoptic tracking tasks.\nTo facilitate comparison, we provide several strong baselines for each of these\ntasks on our proposed dataset. Moreover, we analyze the drawbacks of the\nexisting metrics for panoptic tracking and propose the novel instance-centric\nPAT metric that addresses the concerns. We present exhaustive experiments that\ndemonstrate the utility of Panoptic nuScenes compared to existing datasets and\nmake the online evaluation server available at nuScenes.org. We believe that\nthis extension will accelerate the research of novel methods for scene\nunderstanding of dynamic urban environments."
  },
  {
    "id": "arxiv-290",
    "title": "The Sample Complexity of Meta Sparse Regression",
    "abstract": "This paper addresses the meta-learning problem in sparse linear regression\nwith infinite tasks. We assume that the learner can access several similar\ntasks. The goal of the learner is to transfer knowledge from the prior tasks to\na similar but novel task. For p parameters, size of the support set k , and l\nsamples per task, we show that T \\in O (( k log(p) ) /l ) tasks are sufficient\nin order to recover the common support of all tasks. With the recovered\nsupport, we can greatly reduce the sample complexity for estimating the\nparameter of the novel task, i.e., l \\in O (1) with respect to T and p . We\nalso prove that our rates are minimax optimal. A key difference between\nmeta-learning and the classical multi-task learning, is that meta-learning\nfocuses only on the recovery of the parameters of the novel task, while\nmulti-task learning estimates the parameter of all tasks, which requires l to\ngrow with T . Instead, our efficient meta-learning estimator allows for l to be\nconstant with respect to T (i.e., few-shot learning).",
    "text": "The Sample Complexity of Meta Sparse Regression\n\nThis paper addresses the meta-learning problem in sparse linear regression\nwith infinite tasks. We assume that the learner can access several similar\ntasks. The goal of the learner is to transfer knowledge from the prior tasks to\na similar but novel task. For p parameters, size of the support set k , and l\nsamples per task, we show that T \\in O (( k log(p) ) /l ) tasks are sufficient\nin order to recover the common support of all tasks. With the recovered\nsupport, we can greatly reduce the sample complexity for estimating the\nparameter of the novel task, i.e., l \\in O (1) with respect to T and p . We\nalso prove that our rates are minimax optimal. A key difference between\nmeta-learning and the classical multi-task learning, is that meta-learning\nfocuses only on the recovery of the parameters of the novel task, while\nmulti-task learning estimates the parameter of all tasks, which requires l to\ngrow with T . Instead, our efficient meta-learning estimator allows for l to be\nconstant with respect to T (i.e., few-shot learning)."
  },
  {
    "id": "arxiv-291",
    "title": "Symbolic Brittleness in Sequence Models: on Systematic Generalization in\n  Symbolic Mathematics",
    "abstract": "Neural sequence models trained with maximum likelihood estimation have led to\nbreakthroughs in many tasks, where success is defined by the gap between\ntraining and test performance. However, their ability to achieve stronger forms\nof generalization remains unclear. We consider the problem of symbolic\nmathematical integration, as it requires generalizing systematically beyond the\ntest set. We develop a methodology for evaluating generalization that takes\nadvantage of the problem domain's structure and access to a verifier. Despite\npromising in-distribution performance of sequence-to-sequence models in this\ndomain, we demonstrate challenges in achieving robustness, compositionality,\nand out-of-distribution generalization, through both carefully constructed\nmanual test suites and a genetic algorithm that automatically finds large\ncollections of failures in a controllable manner. Our investigation highlights\nthe difficulty of generalizing well with the predominant modeling and learning\napproach, and the importance of evaluating beyond the test set, across\ndifferent aspects of generalization.",
    "text": "Symbolic Brittleness in Sequence Models: on Systematic Generalization in\n  Symbolic Mathematics\n\nNeural sequence models trained with maximum likelihood estimation have led to\nbreakthroughs in many tasks, where success is defined by the gap between\ntraining and test performance. However, their ability to achieve stronger forms\nof generalization remains unclear. We consider the problem of symbolic\nmathematical integration, as it requires generalizing systematically beyond the\ntest set. We develop a methodology for evaluating generalization that takes\nadvantage of the problem domain's structure and access to a verifier. Despite\npromising in-distribution performance of sequence-to-sequence models in this\ndomain, we demonstrate challenges in achieving robustness, compositionality,\nand out-of-distribution generalization, through both carefully constructed\nmanual test suites and a genetic algorithm that automatically finds large\ncollections of failures in a controllable manner. Our investigation highlights\nthe difficulty of generalizing well with the predominant modeling and learning\napproach, and the importance of evaluating beyond the test set, across\ndifferent aspects of generalization."
  },
  {
    "id": "arxiv-292",
    "title": "Keeping Notes: Conditional Natural Language Generation with a Scratchpad\n  Mechanism",
    "abstract": "We introduce the Scratchpad Mechanism, a novel addition to the\nsequence-to-sequence (seq2seq) neural network architecture and demonstrate its\neffectiveness in improving the overall fluency of seq2seq models for natural\nlanguage generation tasks. By enabling the decoder at each time step to write\nto all of the encoder output layers, Scratchpad can employ the encoder as a\n\"scratchpad\" memory to keep track of what has been generated so far and thereby\nguide future generation. We evaluate Scratchpad in the context of three\nwell-studied natural language generation tasks --- Machine Translation,\nQuestion Generation, and Text Summarization --- and obtain state-of-the-art or\ncomparable performance on standard datasets for each task. Qualitative\nassessments in the form of human judgements (question generation), attention\nvisualization (MT), and sample output (summarization) provide further evidence\nof the ability of Scratchpad to generate fluent and expressive output.",
    "text": "Keeping Notes: Conditional Natural Language Generation with a Scratchpad\n  Mechanism\n\nWe introduce the Scratchpad Mechanism, a novel addition to the\nsequence-to-sequence (seq2seq) neural network architecture and demonstrate its\neffectiveness in improving the overall fluency of seq2seq models for natural\nlanguage generation tasks. By enabling the decoder at each time step to write\nto all of the encoder output layers, Scratchpad can employ the encoder as a\n\"scratchpad\" memory to keep track of what has been generated so far and thereby\nguide future generation. We evaluate Scratchpad in the context of three\nwell-studied natural language generation tasks --- Machine Translation,\nQuestion Generation, and Text Summarization --- and obtain state-of-the-art or\ncomparable performance on standard datasets for each task. Qualitative\nassessments in the form of human judgements (question generation), attention\nvisualization (MT), and sample output (summarization) provide further evidence\nof the ability of Scratchpad to generate fluent and expressive output."
  },
  {
    "id": "arxiv-293",
    "title": "Estimating Mixture Models via Mixtures of Polynomials",
    "abstract": "Mixture modeling is a general technique for making any simple model more\nexpressive through weighted combination. This generality and simplicity in part\nexplains the success of the Expectation Maximization (EM) algorithm, in which\nupdates are easy to derive for a wide class of mixture models. However, the\nlikelihood of a mixture model is non-convex, so EM has no known global\nconvergence guarantees. Recently, method of moments approaches offer global\nguarantees for some mixture models, but they do not extend easily to the range\nof mixture models that exist. In this work, we present Polymom, an unifying\nframework based on method of moments in which estimation procedures are easily\nderivable, just as in EM. Polymom is applicable when the moments of a single\nmixture component are polynomials of the parameters. Our key observation is\nthat the moments of the mixture model are a mixture of these polynomials, which\nallows us to cast estimation as a Generalized Moment Problem. We solve its\nrelaxations using semidefinite optimization, and then extract parameters using\nideas from computer algebra. This framework allows us to draw insights and\napply tools from convex optimization, computer algebra and the theory of\nmoments to study problems in statistical estimation.",
    "text": "Estimating Mixture Models via Mixtures of Polynomials\n\nMixture modeling is a general technique for making any simple model more\nexpressive through weighted combination. This generality and simplicity in part\nexplains the success of the Expectation Maximization (EM) algorithm, in which\nupdates are easy to derive for a wide class of mixture models. However, the\nlikelihood of a mixture model is non-convex, so EM has no known global\nconvergence guarantees. Recently, method of moments approaches offer global\nguarantees for some mixture models, but they do not extend easily to the range\nof mixture models that exist. In this work, we present Polymom, an unifying\nframework based on method of moments in which estimation procedures are easily\nderivable, just as in EM. Polymom is applicable when the moments of a single\nmixture component are polynomials of the parameters. Our key observation is\nthat the moments of the mixture model are a mixture of these polynomials, which\nallows us to cast estimation as a Generalized Moment Problem. We solve its\nrelaxations using semidefinite optimization, and then extract parameters using\nideas from computer algebra. This framework allows us to draw insights and\napply tools from convex optimization, computer algebra and the theory of\nmoments to study problems in statistical estimation."
  },
  {
    "id": "arxiv-294",
    "title": "Double-Base Asymmetric AdaBoost",
    "abstract": "Based on the use of different exponential bases to define class-dependent\nerror bounds, a new and highly efficient asymmetric boosting scheme, coined as\nAdaBoostDB (Double-Base), is proposed. Supported by a fully theoretical\nderivation procedure, unlike most of the other approaches in the literature,\nour algorithm preserves all the formal guarantees and properties of original\n(cost-insensitive) AdaBoost, similarly to the state-of-the-art Cost-Sensitive\nAdaBoost algorithm. However, the key advantage of AdaBoostDB is that our novel\nderivation scheme enables an extremely efficient conditional search procedure,\ndramatically improving and simplifying the training phase of the algorithm.\nExperiments, both over synthetic and real datasets, reveal that AdaBoostDB is\nable to save over 99% training time with regard to Cost-Sensitive AdaBoost,\nproviding the same cost-sensitive results. This computational advantage of\nAdaBoostDB can make a difference in problems managing huge pools of weak\nclassifiers in which boosting techniques are commonly used.",
    "text": "Double-Base Asymmetric AdaBoost\n\nBased on the use of different exponential bases to define class-dependent\nerror bounds, a new and highly efficient asymmetric boosting scheme, coined as\nAdaBoostDB (Double-Base), is proposed. Supported by a fully theoretical\nderivation procedure, unlike most of the other approaches in the literature,\nour algorithm preserves all the formal guarantees and properties of original\n(cost-insensitive) AdaBoost, similarly to the state-of-the-art Cost-Sensitive\nAdaBoost algorithm. However, the key advantage of AdaBoostDB is that our novel\nderivation scheme enables an extremely efficient conditional search procedure,\ndramatically improving and simplifying the training phase of the algorithm.\nExperiments, both over synthetic and real datasets, reveal that AdaBoostDB is\nable to save over 99% training time with regard to Cost-Sensitive AdaBoost,\nproviding the same cost-sensitive results. This computational advantage of\nAdaBoostDB can make a difference in problems managing huge pools of weak\nclassifiers in which boosting techniques are commonly used."
  },
  {
    "id": "arxiv-295",
    "title": "Generalizing Hamiltonian Monte Carlo with Neural Networks",
    "abstract": "We present a general-purpose method to train Markov chain Monte Carlo\nkernels, parameterized by deep neural networks, that converge and mix quickly\nto their target distribution. Our method generalizes Hamiltonian Monte Carlo\nand is trained to maximize expected squared jumped distance, a proxy for mixing\nspeed. We demonstrate large empirical gains on a collection of simple but\nchallenging distributions, for instance achieving a 106x improvement in\neffective sample size in one case, and mixing when standard HMC makes no\nmeasurable progress in a second. Finally, we show quantitative and qualitative\ngains on a real-world task: latent-variable generative modeling. We release an\nopen source TensorFlow implementation of the algorithm.",
    "text": "Generalizing Hamiltonian Monte Carlo with Neural Networks\n\nWe present a general-purpose method to train Markov chain Monte Carlo\nkernels, parameterized by deep neural networks, that converge and mix quickly\nto their target distribution. Our method generalizes Hamiltonian Monte Carlo\nand is trained to maximize expected squared jumped distance, a proxy for mixing\nspeed. We demonstrate large empirical gains on a collection of simple but\nchallenging distributions, for instance achieving a 106x improvement in\neffective sample size in one case, and mixing when standard HMC makes no\nmeasurable progress in a second. Finally, we show quantitative and qualitative\ngains on a real-world task: latent-variable generative modeling. We release an\nopen source TensorFlow implementation of the algorithm."
  },
  {
    "id": "arxiv-296",
    "title": "Intervention Efficient Algorithm for Two-Stage Causal MDPs",
    "abstract": "We study Markov Decision Processes (MDP) wherein states correspond to causal\ngraphs that stochastically generate rewards. In this setup, the learner's goal\nis to identify atomic interventions that lead to high rewards by intervening on\nvariables at each state. Generalizing the recent causal-bandit framework, the\ncurrent work develops (simple) regret minimization guarantees for two-stage\ncausal MDPs, with parallel causal graph at each state. We propose an algorithm\nthat achieves an instance dependent regret bound. A key feature of our\nalgorithm is that it utilizes convex optimization to address the exploration\nproblem. We identify classes of instances wherein our regret guarantee is\nessentially tight, and experimentally validate our theoretical results.",
    "text": "Intervention Efficient Algorithm for Two-Stage Causal MDPs\n\nWe study Markov Decision Processes (MDP) wherein states correspond to causal\ngraphs that stochastically generate rewards. In this setup, the learner's goal\nis to identify atomic interventions that lead to high rewards by intervening on\nvariables at each state. Generalizing the recent causal-bandit framework, the\ncurrent work develops (simple) regret minimization guarantees for two-stage\ncausal MDPs, with parallel causal graph at each state. We propose an algorithm\nthat achieves an instance dependent regret bound. A key feature of our\nalgorithm is that it utilizes convex optimization to address the exploration\nproblem. We identify classes of instances wherein our regret guarantee is\nessentially tight, and experimentally validate our theoretical results."
  },
  {
    "id": "arxiv-297",
    "title": "Semi-supervised Optimal Transport with Self-paced Ensemble for\n  Cross-hospital Sepsis Early Detection",
    "abstract": "The utilization of computer technology to solve problems in medical scenarios\nhas attracted considerable attention in recent years, which still has great\npotential and space for exploration. Among them, machine learning has been\nwidely used in the prediction, diagnosis and even treatment of Sepsis. However,\nstate-of-the-art methods require large amounts of labeled medical data for\nsupervised learning. In real-world applications, the lack of labeled data will\ncause enormous obstacles if one hospital wants to deploy a new Sepsis detection\nsystem. Different from the supervised learning setting, we need to use known\ninformation (e.g., from another hospital with rich labeled data) to help build\na model with acceptable performance, i.e., transfer learning. In this paper, we\npropose a semi-supervised optimal transport with self-paced ensemble framework\nfor Sepsis early detection, called SPSSOT, to transfer knowledge from the other\nthat has rich labeled data. In SPSSOT, we first extract the same clinical\nindicators from the source domain (e.g., hospital with rich labeled data) and\nthe target domain (e.g., hospital with little labeled data), then we combine\nthe semi-supervised domain adaptation based on optimal transport theory with\nself-paced under-sampling to avoid a negative transfer possibly caused by\ncovariate shift and class imbalance. On the whole, SPSSOT is an end-to-end\ntransfer learning method for Sepsis early detection which can automatically\nselect suitable samples from two domains respectively according to the number\nof iterations and align feature space of two domains. Extensive experiments on\ntwo open clinical datasets demonstrate that comparing with other methods, our\nproposed SPSSOT, can significantly improve the AUC values with only 1% labeled\ndata in the target domain in two transfer learning scenarios, MIMIC\n$rightarrow$ Challenge and Challenge $rightarrow$ MIMIC.",
    "text": "Semi-supervised Optimal Transport with Self-paced Ensemble for\n  Cross-hospital Sepsis Early Detection\n\nThe utilization of computer technology to solve problems in medical scenarios\nhas attracted considerable attention in recent years, which still has great\npotential and space for exploration. Among them, machine learning has been\nwidely used in the prediction, diagnosis and even treatment of Sepsis. However,\nstate-of-the-art methods require large amounts of labeled medical data for\nsupervised learning. In real-world applications, the lack of labeled data will\ncause enormous obstacles if one hospital wants to deploy a new Sepsis detection\nsystem. Different from the supervised learning setting, we need to use known\ninformation (e.g., from another hospital with rich labeled data) to help build\na model with acceptable performance, i.e., transfer learning. In this paper, we\npropose a semi-supervised optimal transport with self-paced ensemble framework\nfor Sepsis early detection, called SPSSOT, to transfer knowledge from the other\nthat has rich labeled data. In SPSSOT, we first extract the same clinical\nindicators from the source domain (e.g., hospital with rich labeled data) and\nthe target domain (e.g., hospital with little labeled data), then we combine\nthe semi-supervised domain adaptation based on optimal transport theory with\nself-paced under-sampling to avoid a negative transfer possibly caused by\ncovariate shift and class imbalance. On the whole, SPSSOT is an end-to-end\ntransfer learning method for Sepsis early detection which can automatically\nselect suitable samples from two domains respectively according to the number\nof iterations and align feature space of two domains. Extensive experiments on\ntwo open clinical datasets demonstrate that comparing with other methods, our\nproposed SPSSOT, can significantly improve the AUC values with only 1% labeled\ndata in the target domain in two transfer learning scenarios, MIMIC\n$rightarrow$ Challenge and Challenge $rightarrow$ MIMIC."
  },
  {
    "id": "arxiv-298",
    "title": "Learning from Mistakes -- A Framework for Neural Architecture Search",
    "abstract": "Learning from one's mistakes is an effective human learning technique where\nthe learners focus more on the topics where mistakes were made, so as to deepen\ntheir understanding. In this paper, we investigate if this human learning\nstrategy can be applied in machine learning. We propose a novel machine\nlearning method called Learning From Mistakes (LFM), wherein the learner\nimproves its ability to learn by focusing more on the mistakes during revision.\nWe formulate LFM as a three-stage optimization problem: 1) learner learns; 2)\nlearner re-learns focusing on the mistakes, and; 3) learner validates its\nlearning. We develop an efficient algorithm to solve the LFM problem. We apply\nthe LFM framework to neural architecture search on CIFAR-10, CIFAR-100, and\nImagenet. Experimental results strongly demonstrate the effectiveness of our\nmodel.",
    "text": "Learning from Mistakes -- A Framework for Neural Architecture Search\n\nLearning from one's mistakes is an effective human learning technique where\nthe learners focus more on the topics where mistakes were made, so as to deepen\ntheir understanding. In this paper, we investigate if this human learning\nstrategy can be applied in machine learning. We propose a novel machine\nlearning method called Learning From Mistakes (LFM), wherein the learner\nimproves its ability to learn by focusing more on the mistakes during revision.\nWe formulate LFM as a three-stage optimization problem: 1) learner learns; 2)\nlearner re-learns focusing on the mistakes, and; 3) learner validates its\nlearning. We develop an efficient algorithm to solve the LFM problem. We apply\nthe LFM framework to neural architecture search on CIFAR-10, CIFAR-100, and\nImagenet. Experimental results strongly demonstrate the effectiveness of our\nmodel."
  },
  {
    "id": "arxiv-299",
    "title": "Spectral Algorithms for Computing Fair Support Vector Machines",
    "abstract": "Classifiers and rating scores are prone to implicitly codifying biases, which\nmay be present in the training data, against protected classes (i.e., age,\ngender, or race). So it is important to understand how to design classifiers\nand scores that prevent discrimination in predictions. This paper develops\ncomputationally tractable algorithms for designing accurate but fair support\nvector machines (SVM's). Our approach imposes a constraint on the covariance\nmatrices conditioned on each protected class, which leads to a nonconvex\nquadratic constraint in the SVM formulation. We develop iterative algorithms to\ncompute fair linear and kernel SVM's, which solve a sequence of relaxations\nconstructed using a spectral decomposition of the nonconvex constraint. Its\neffectiveness in achieving high prediction accuracy while ensuring fairness is\nshown through numerical experiments on several data sets.",
    "text": "Spectral Algorithms for Computing Fair Support Vector Machines\n\nClassifiers and rating scores are prone to implicitly codifying biases, which\nmay be present in the training data, against protected classes (i.e., age,\ngender, or race). So it is important to understand how to design classifiers\nand scores that prevent discrimination in predictions. This paper develops\ncomputationally tractable algorithms for designing accurate but fair support\nvector machines (SVM's). Our approach imposes a constraint on the covariance\nmatrices conditioned on each protected class, which leads to a nonconvex\nquadratic constraint in the SVM formulation. We develop iterative algorithms to\ncompute fair linear and kernel SVM's, which solve a sequence of relaxations\nconstructed using a spectral decomposition of the nonconvex constraint. Its\neffectiveness in achieving high prediction accuracy while ensuring fairness is\nshown through numerical experiments on several data sets."
  },
  {
    "id": "arxiv-300",
    "title": "Learning to Act by Predicting the Future",
    "abstract": "We present an approach to sensorimotor control in immersive environments. Our\napproach utilizes a high-dimensional sensory stream and a lower-dimensional\nmeasurement stream. The cotemporal structure of these streams provides a rich\nsupervisory signal, which enables training a sensorimotor control model by\ninteracting with the environment. The model is trained using supervised\nlearning techniques, but without extraneous supervision. It learns to act based\non raw sensory input from a complex three-dimensional environment. The\npresented formulation enables learning without a fixed goal at training time,\nand pursuing dynamically changing goals at test time. We conduct extensive\nexperiments in three-dimensional simulations based on the classical\nfirst-person game Doom. The results demonstrate that the presented approach\noutperforms sophisticated prior formulations, particularly on challenging\ntasks. The results also show that trained models successfully generalize across\nenvironments and goals. A model trained using the presented approach won the\nFull Deathmatch track of the Visual Doom AI Competition, which was held in\npreviously unseen environments.",
    "text": "Learning to Act by Predicting the Future\n\nWe present an approach to sensorimotor control in immersive environments. Our\napproach utilizes a high-dimensional sensory stream and a lower-dimensional\nmeasurement stream. The cotemporal structure of these streams provides a rich\nsupervisory signal, which enables training a sensorimotor control model by\ninteracting with the environment. The model is trained using supervised\nlearning techniques, but without extraneous supervision. It learns to act based\non raw sensory input from a complex three-dimensional environment. The\npresented formulation enables learning without a fixed goal at training time,\nand pursuing dynamically changing goals at test time. We conduct extensive\nexperiments in three-dimensional simulations based on the classical\nfirst-person game Doom. The results demonstrate that the presented approach\noutperforms sophisticated prior formulations, particularly on challenging\ntasks. The results also show that trained models successfully generalize across\nenvironments and goals. A model trained using the presented approach won the\nFull Deathmatch track of the Visual Doom AI Competition, which was held in\npreviously unseen environments."
  },
  {
    "id": "arxiv-301",
    "title": "Imagining new futures beyond predictive systems in child welfare: A qualitative study with impacted stakeholders",
    "abstract": "Child welfare agencies across the United States are turning to data-driven\npredictive technologies (commonly called predictive analytics) which use\ngovernment administrative data to assist workers' decision-making. While some\nprior work has explored impacted stakeholders' concerns with current uses of\ndata-driven predictive risk models (PRMs), less work has asked stakeholders\nwhether such tools ought to be used in the first place. In this work, we\nconducted a set of seven design workshops with 35 stakeholders who have been\nimpacted by the child welfare system or who work in it to understand their\nbeliefs and concerns around PRMs, and to engage them in imagining new uses of\ndata and technologies in the child welfare system. We found that participants\nworried current PRMs perpetuate or exacerbate existing problems in child\nwelfare. Participants suggested new ways to use data and data-driven tools to\nbetter support impacted communities and suggested paths to mitigate possible\nharms of these tools. Participants also suggested low-tech or no-tech\nalternatives to PRMs to address problems in child welfare. Our study sheds\nlight on how researchers and designers can work in solidarity with impacted\ncommunities, possibly to circumvent or oppose child welfare agencies.",
    "text": "Imagining new futures beyond predictive systems in child welfare: A qualitative study with impacted stakeholders\n\nChild welfare agencies across the United States are turning to data-driven\npredictive technologies (commonly called predictive analytics) which use\ngovernment administrative data to assist workers' decision-making. While some\nprior work has explored impacted stakeholders' concerns with current uses of\ndata-driven predictive risk models (PRMs), less work has asked stakeholders\nwhether such tools ought to be used in the first place. In this work, we\nconducted a set of seven design workshops with 35 stakeholders who have been\nimpacted by the child welfare system or who work in it to understand their\nbeliefs and concerns around PRMs, and to engage them in imagining new uses of\ndata and technologies in the child welfare system. We found that participants\nworried current PRMs perpetuate or exacerbate existing problems in child\nwelfare. Participants suggested new ways to use data and data-driven tools to\nbetter support impacted communities and suggested paths to mitigate possible\nharms of these tools. Participants also suggested low-tech or no-tech\nalternatives to PRMs to address problems in child welfare. Our study sheds\nlight on how researchers and designers can work in solidarity with impacted\ncommunities, possibly to circumvent or oppose child welfare agencies."
  },
  {
    "id": "arxiv-302",
    "title": "ASNI: Adaptive Structured Noise Injection for shallow and deep neural\n  networks",
    "abstract": "Dropout is a regularisation technique in neural network training where unit\nactivations are randomly set to zero with a given probability\n\\emph{independently}. In this work, we propose a generalisation of dropout and\nother multiplicative noise injection schemes for shallow and deep neural\nnetworks, where the random noise applied to different units is not independent\nbut follows a joint distribution that is either fixed or estimated during\ntraining. We provide theoretical insights on why such adaptive structured noise\ninjection (ASNI) may be relevant, and empirically confirm that it helps boost\nthe accuracy of simple feedforward and convolutional neural networks,\ndisentangles the hidden layer representations, and leads to sparser\nrepresentations. Our proposed method is a straightforward modification of the\nclassical dropout and does not require additional computational overhead.",
    "text": "ASNI: Adaptive Structured Noise Injection for shallow and deep neural\n  networks\n\nDropout is a regularisation technique in neural network training where unit\nactivations are randomly set to zero with a given probability\n\\emph{independently}. In this work, we propose a generalisation of dropout and\nother multiplicative noise injection schemes for shallow and deep neural\nnetworks, where the random noise applied to different units is not independent\nbut follows a joint distribution that is either fixed or estimated during\ntraining. We provide theoretical insights on why such adaptive structured noise\ninjection (ASNI) may be relevant, and empirically confirm that it helps boost\nthe accuracy of simple feedforward and convolutional neural networks,\ndisentangles the hidden layer representations, and leads to sparser\nrepresentations. Our proposed method is a straightforward modification of the\nclassical dropout and does not require additional computational overhead."
  },
  {
    "id": "arxiv-303",
    "title": "How to Grow a (Product) Tree: Personalized Category Suggestions for\n  eCommerce Type-Ahead",
    "abstract": "In an attempt to balance precision and recall in the search page, leading\ndigital shops have been effectively nudging users into select category facets\nas early as in the type-ahead suggestions. In this work, we present\nSessionPath, a novel neural network model that improves facet suggestions on\ntwo counts: first, the model is able to leverage session embeddings to provide\nscalable personalization; second, SessionPath predicts facets by explicitly\nproducing a probability distribution at each node in the taxonomy path. We\nbenchmark SessionPath on two partnering shops against count-based and neural\nmodels, and show how business requirements and model behavior can be combined\nin a principled way.",
    "text": "How to Grow a (Product) Tree: Personalized Category Suggestions for\n  eCommerce Type-Ahead\n\nIn an attempt to balance precision and recall in the search page, leading\ndigital shops have been effectively nudging users into select category facets\nas early as in the type-ahead suggestions. In this work, we present\nSessionPath, a novel neural network model that improves facet suggestions on\ntwo counts: first, the model is able to leverage session embeddings to provide\nscalable personalization; second, SessionPath predicts facets by explicitly\nproducing a probability distribution at each node in the taxonomy path. We\nbenchmark SessionPath on two partnering shops against count-based and neural\nmodels, and show how business requirements and model behavior can be combined\nin a principled way."
  },
  {
    "id": "arxiv-304",
    "title": "Settling the Variance of Multi-Agent Policy Gradients",
    "abstract": "Policy gradient (PG) methods are popular reinforcement learning (RL) methods\nwhere a baseline is often applied to reduce the variance of gradient estimates.\nIn multi-agent RL (MARL), although the PG theorem can be naturally extended,\nthe effectiveness of multi-agent PG (MAPG) methods degrades as the variance of\ngradient estimates increases rapidly with the number of agents. In this paper,\nwe offer a rigorous analysis of MAPG methods by, firstly, quantifying the\ncontributions of the number of agents and agents' explorations to the variance\nof MAPG estimators. Based on this analysis, we derive the optimal baseline (OB)\nthat achieves the minimal variance. In comparison to the OB, we measure the\nexcess variance of existing MARL algorithms such as vanilla MAPG and COMA.\nConsidering using deep neural networks, we also propose a surrogate version of\nOB, which can be seamlessly plugged into any existing PG methods in MARL. On\nbenchmarks of Multi-Agent MuJoCo and StarCraft challenges, our OB technique\neffectively stabilises training and improves the performance of multi-agent PPO\nand COMA algorithms by a significant margin.",
    "text": "Settling the Variance of Multi-Agent Policy Gradients\n\nPolicy gradient (PG) methods are popular reinforcement learning (RL) methods\nwhere a baseline is often applied to reduce the variance of gradient estimates.\nIn multi-agent RL (MARL), although the PG theorem can be naturally extended,\nthe effectiveness of multi-agent PG (MAPG) methods degrades as the variance of\ngradient estimates increases rapidly with the number of agents. In this paper,\nwe offer a rigorous analysis of MAPG methods by, firstly, quantifying the\ncontributions of the number of agents and agents' explorations to the variance\nof MAPG estimators. Based on this analysis, we derive the optimal baseline (OB)\nthat achieves the minimal variance. In comparison to the OB, we measure the\nexcess variance of existing MARL algorithms such as vanilla MAPG and COMA.\nConsidering using deep neural networks, we also propose a surrogate version of\nOB, which can be seamlessly plugged into any existing PG methods in MARL. On\nbenchmarks of Multi-Agent MuJoCo and StarCraft challenges, our OB technique\neffectively stabilises training and improves the performance of multi-agent PPO\nand COMA algorithms by a significant margin."
  },
  {
    "id": "arxiv-305",
    "title": "Seismic wave propagation and inversion with Neural Operators",
    "abstract": "Seismic wave propagation forms the basis for most aspects of seismological\nresearch, yet solving the wave equation is a major computational burden that\ninhibits the progress of research. This is exacerbated by the fact that new\nsimulations must be performed when the velocity structure or source location is\nperturbed. Here, we explore a prototype framework for learning general\nsolutions using a recently developed machine learning paradigm called Neural\nOperator. A trained Neural Operator can compute a solution in negligible time\nfor any velocity structure or source location. We develop a scheme to train\nNeural Operators on an ensemble of simulations performed with random velocity\nmodels and source locations. As Neural Operators are grid-free, it is possible\nto evaluate solutions on higher resolution velocity models than trained on,\nproviding additional computational efficiency. We illustrate the method with\nthe 2D acoustic wave equation and demonstrate the method's applicability to\nseismic tomography, using reverse mode automatic differentiation to compute\ngradients of the wavefield with respect to the velocity structure. The\ndeveloped procedure is nearly an order of magnitude faster than using\nconventional numerical methods for full waveform inversion.",
    "text": "Seismic wave propagation and inversion with Neural Operators\n\nSeismic wave propagation forms the basis for most aspects of seismological\nresearch, yet solving the wave equation is a major computational burden that\ninhibits the progress of research. This is exacerbated by the fact that new\nsimulations must be performed when the velocity structure or source location is\nperturbed. Here, we explore a prototype framework for learning general\nsolutions using a recently developed machine learning paradigm called Neural\nOperator. A trained Neural Operator can compute a solution in negligible time\nfor any velocity structure or source location. We develop a scheme to train\nNeural Operators on an ensemble of simulations performed with random velocity\nmodels and source locations. As Neural Operators are grid-free, it is possible\nto evaluate solutions on higher resolution velocity models than trained on,\nproviding additional computational efficiency. We illustrate the method with\nthe 2D acoustic wave equation and demonstrate the method's applicability to\nseismic tomography, using reverse mode automatic differentiation to compute\ngradients of the wavefield with respect to the velocity structure. The\ndeveloped procedure is nearly an order of magnitude faster than using\nconventional numerical methods for full waveform inversion."
  },
  {
    "id": "arxiv-306",
    "title": "Total Nitrogen Estimation in Agricultural Soils via Aerial Multispectral\n  Imaging and LIBS",
    "abstract": "Measuring soil health indicators is an important and challenging task that\naffects farmers' decisions on timing, placement, and quantity of fertilizers\napplied in the farms. Most existing methods to measure soil health indicators\n(SHIs) are in-lab wet chemistry or spectroscopy-based methods, which require\nsignificant human input and effort, time-consuming, costly, and are\nlow-throughput in nature. To address this challenge, we develop an artificial\nintelligence (AI)-driven near real-time unmanned aerial vehicle (UAV)-based\nmultispectral sensing (UMS) solution to estimate total nitrogen (TN) of the\nsoil, an important macro-nutrient or SHI that directly affects the crop health.\nAccurate prediction of soil TN can significantly increase crop yield through\ninformed decision making on the timing of seed planting, and fertilizer\nquantity and timing. We train two machine learning models including multi-layer\nperceptron and support vector machine to predict the soil nitrogen using a\nsuite of data classes including multispectral characteristics of the soil and\ncrops in red, near-infrared, and green spectral bands, computed vegetation\nindices, and environmental variables including air temperature and relative\nhumidity. To generate the ground-truth data or the training data for the\nmachine learning models, we measure the total nitrogen of the soil samples\n(collected from a farm) using laser-induced breakdown spectroscopy (LIBS).",
    "text": "Total Nitrogen Estimation in Agricultural Soils via Aerial Multispectral\n  Imaging and LIBS\n\nMeasuring soil health indicators is an important and challenging task that\naffects farmers' decisions on timing, placement, and quantity of fertilizers\napplied in the farms. Most existing methods to measure soil health indicators\n(SHIs) are in-lab wet chemistry or spectroscopy-based methods, which require\nsignificant human input and effort, time-consuming, costly, and are\nlow-throughput in nature. To address this challenge, we develop an artificial\nintelligence (AI)-driven near real-time unmanned aerial vehicle (UAV)-based\nmultispectral sensing (UMS) solution to estimate total nitrogen (TN) of the\nsoil, an important macro-nutrient or SHI that directly affects the crop health.\nAccurate prediction of soil TN can significantly increase crop yield through\ninformed decision making on the timing of seed planting, and fertilizer\nquantity and timing. We train two machine learning models including multi-layer\nperceptron and support vector machine to predict the soil nitrogen using a\nsuite of data classes including multispectral characteristics of the soil and\ncrops in red, near-infrared, and green spectral bands, computed vegetation\nindices, and environmental variables including air temperature and relative\nhumidity. To generate the ground-truth data or the training data for the\nmachine learning models, we measure the total nitrogen of the soil samples\n(collected from a farm) using laser-induced breakdown spectroscopy (LIBS)."
  },
  {
    "id": "arxiv-307",
    "title": "Deep learning surrogate models for spatial and visual connectivity",
    "abstract": "Spatial and visual connectivity are important metrics when developing\nworkplace layouts. Calculating those metrics in real-time can be difficult,\ndepending on the size of the floor plan being analysed and the resolution of\nthe analyses. This paper investigates the possibility of considerably speeding\nup the outcomes of such computationally intensive simulations by using machine\nlearning to create models capable of identifying the spatial and visual\nconnectivity potential of a space. To that end we present the entire process of\ninvestigating different machine learning models and a pipeline for training\nthem on such task, from the incorporation of a bespoke spatial and visual\nconnectivity analysis engine through a distributed computation pipeline, to the\nprocess of synthesizing training data and evaluating the performance of\ndifferent neural networks.",
    "text": "Deep learning surrogate models for spatial and visual connectivity\n\nSpatial and visual connectivity are important metrics when developing\nworkplace layouts. Calculating those metrics in real-time can be difficult,\ndepending on the size of the floor plan being analysed and the resolution of\nthe analyses. This paper investigates the possibility of considerably speeding\nup the outcomes of such computationally intensive simulations by using machine\nlearning to create models capable of identifying the spatial and visual\nconnectivity potential of a space. To that end we present the entire process of\ninvestigating different machine learning models and a pipeline for training\nthem on such task, from the incorporation of a bespoke spatial and visual\nconnectivity analysis engine through a distributed computation pipeline, to the\nprocess of synthesizing training data and evaluating the performance of\ndifferent neural networks."
  },
  {
    "id": "arxiv-308",
    "title": "STCGAT: Spatial-temporal causal networks for complex urban road traffic\n  flow prediction",
    "abstract": "Traffic forecasting is an essential component of intelligent transportation\nsystems. However, traffic data are highly nonlinear and have complex spatial\ncorrelations between road nodes. Therefore, it is incredibly challenging to dig\ndeeper into the underlying Spatial-temporal relationships from the complex\ntraffic data. Existing approaches usually use fixed traffic road network\ntopology maps and independent time series modules to capture Spatial-temporal\ncorrelations, ignoring the dynamic changes of traffic road networks and the\ninherent temporal causal relationships between traffic events. Therefore, a new\nprediction model is proposed in this study. The model dynamically captures the\nspatial dependence of the traffic network through a Graph Attention\nNetwork(GAT) and then analyzes the causal relationship of the traffic data\nusing our proposed Causal Temporal Convolutional Network(CTCN) to obtain the\noverall temporal dependence. We conducted extensive comparison experiments with\nother traffic prediction methods on two real traffic datasets to evaluate the\nmodel's prediction performance. Compared with the best experimental results of\ndifferent prediction methods, the prediction performance of our approach is\nimproved by more than 50%. You can get our source code and data through\nhttps://github.com/zhangshqii/STCGAT.",
    "text": "STCGAT: Spatial-temporal causal networks for complex urban road traffic\n  flow prediction\n\nTraffic forecasting is an essential component of intelligent transportation\nsystems. However, traffic data are highly nonlinear and have complex spatial\ncorrelations between road nodes. Therefore, it is incredibly challenging to dig\ndeeper into the underlying Spatial-temporal relationships from the complex\ntraffic data. Existing approaches usually use fixed traffic road network\ntopology maps and independent time series modules to capture Spatial-temporal\ncorrelations, ignoring the dynamic changes of traffic road networks and the\ninherent temporal causal relationships between traffic events. Therefore, a new\nprediction model is proposed in this study. The model dynamically captures the\nspatial dependence of the traffic network through a Graph Attention\nNetwork(GAT) and then analyzes the causal relationship of the traffic data\nusing our proposed Causal Temporal Convolutional Network(CTCN) to obtain the\noverall temporal dependence. We conducted extensive comparison experiments with\nother traffic prediction methods on two real traffic datasets to evaluate the\nmodel's prediction performance. Compared with the best experimental results of\ndifferent prediction methods, the prediction performance of our approach is\nimproved by more than 50%. You can get our source code and data through\nhttps://github.com/zhangshqii/STCGAT."
  },
  {
    "id": "arxiv-309",
    "title": "FAME: Feature-Based Adversarial Meta-Embeddings for Robust Input\n  Representations",
    "abstract": "Combining several embeddings typically improves performance in downstream\ntasks as different embeddings encode different information. It has been shown\nthat even models using embeddings from transformers still benefit from the\ninclusion of standard word embeddings. However, the combination of embeddings\nof different types and dimensions is challenging. As an alternative to\nattention-based meta-embeddings, we propose feature-based adversarial\nmeta-embeddings (FAME) with an attention function that is guided by features\nreflecting word-specific properties, such as shape and frequency, and show that\nthis is beneficial to handle subword-based embeddings. In addition, FAME uses\nadversarial training to optimize the mappings of differently-sized embeddings\nto the same space. We demonstrate that FAME works effectively across languages\nand domains for sequence labeling and sentence classification, in particular in\nlow-resource settings. FAME sets the new state of the art for POS tagging in 27\nlanguages, various NER settings and question classification in different\ndomains.",
    "text": "FAME: Feature-Based Adversarial Meta-Embeddings for Robust Input\n  Representations\n\nCombining several embeddings typically improves performance in downstream\ntasks as different embeddings encode different information. It has been shown\nthat even models using embeddings from transformers still benefit from the\ninclusion of standard word embeddings. However, the combination of embeddings\nof different types and dimensions is challenging. As an alternative to\nattention-based meta-embeddings, we propose feature-based adversarial\nmeta-embeddings (FAME) with an attention function that is guided by features\nreflecting word-specific properties, such as shape and frequency, and show that\nthis is beneficial to handle subword-based embeddings. In addition, FAME uses\nadversarial training to optimize the mappings of differently-sized embeddings\nto the same space. We demonstrate that FAME works effectively across languages\nand domains for sequence labeling and sentence classification, in particular in\nlow-resource settings. FAME sets the new state of the art for POS tagging in 27\nlanguages, various NER settings and question classification in different\ndomains."
  },
  {
    "id": "arxiv-310",
    "title": "A Novel Meta-Heuristic Optimization Algorithm Inspired by the Spread of\n  Viruses",
    "abstract": "According to the no-free-lunch theorem, there is no single meta-heuristic\nalgorithm that can optimally solve all optimization problems. This motivates\nmany researchers to continuously develop new optimization algorithms. In this\npaper, a novel nature-inspired meta-heuristic optimization algorithm called\nvirus spread optimization (VSO) is proposed. VSO loosely mimics the spread of\nviruses among hosts, and can be effectively applied to solving many challenging\nand continuous optimization problems. We devise a new representation scheme and\nviral operations that are radically different from previously proposed\nvirus-based optimization algorithms. First, the viral RNA of each host in VSO\ndenotes a potential solution for which different viral operations will help to\ndiversify the searching strategies in order to largely enhance the solution\nquality. In addition, an imported infection mechanism, inheriting the searched\noptima from another colony, is introduced to possibly avoid the prematuration\nof any potential solution in solving complex problems. VSO has an excellent\ncapability to conduct adaptive neighborhood searches around the discovered\noptima for achieving better solutions. Furthermore, with a flexible infection\nmechanism, VSO can quickly escape from local optima. To clearly demonstrate\nboth its effectiveness and efficiency, VSO is critically evaluated on a series\nof well-known benchmark functions. Moreover, VSO is validated on its\napplicability through two real-world examples including the financial portfolio\noptimization and optimization of hyper-parameters of support vector machines\nfor classification problems. The results show that VSO has attained superior\nperformance in terms of solution fitness, convergence rate, scalability,\nreliability, and flexibility when compared to those results of the conventional\nas well as state-of-the-art meta-heuristic optimization algorithms.",
    "text": "A Novel Meta-Heuristic Optimization Algorithm Inspired by the Spread of\n  Viruses\n\nAccording to the no-free-lunch theorem, there is no single meta-heuristic\nalgorithm that can optimally solve all optimization problems. This motivates\nmany researchers to continuously develop new optimization algorithms. In this\npaper, a novel nature-inspired meta-heuristic optimization algorithm called\nvirus spread optimization (VSO) is proposed. VSO loosely mimics the spread of\nviruses among hosts, and can be effectively applied to solving many challenging\nand continuous optimization problems. We devise a new representation scheme and\nviral operations that are radically different from previously proposed\nvirus-based optimization algorithms. First, the viral RNA of each host in VSO\ndenotes a potential solution for which different viral operations will help to\ndiversify the searching strategies in order to largely enhance the solution\nquality. In addition, an imported infection mechanism, inheriting the searched\noptima from another colony, is introduced to possibly avoid the prematuration\nof any potential solution in solving complex problems. VSO has an excellent\ncapability to conduct adaptive neighborhood searches around the discovered\noptima for achieving better solutions. Furthermore, with a flexible infection\nmechanism, VSO can quickly escape from local optima. To clearly demonstrate\nboth its effectiveness and efficiency, VSO is critically evaluated on a series\nof well-known benchmark functions. Moreover, VSO is validated on its\napplicability through two real-world examples including the financial portfolio\noptimization and optimization of hyper-parameters of support vector machines\nfor classification problems. The results show that VSO has attained superior\nperformance in terms of solution fitness, convergence rate, scalability,\nreliability, and flexibility when compared to those results of the conventional\nas well as state-of-the-art meta-heuristic optimization algorithms."
  },
  {
    "id": "arxiv-311",
    "title": "Deep Negative Volume Segmentation",
    "abstract": "Clinical examination of three-dimensional image data of compound anatomical\nobjects, such as complex joints, remains a tedious process, demanding the time\nand the expertise of physicians. For instance, automation of the segmentation\ntask of the TMJ (temporomandibular joint) has been hindered by its compound\nthree-dimensional shape, multiple overlaid textures, an abundance of\nsurrounding irregularities in the skull, and a virtually omnidirectional range\nof the jaw's motion - all of which extend the manual annotation process to more\nthan an hour per patient. To address the challenge, we invent a new angle to\nthe 3D segmentation task: namely, we propose to segment empty spaces between\nall the tissues surrounding the object - the so-called negative volume\nsegmentation. Our approach is an end-to-end pipeline that comprises a V-Net for\nbone segmentation, a 3D volume construction by inflation of the reconstructed\nbone head in all directions along the normal vector to its mesh faces.\nEventually confined within the skull bones, the inflated surface occupies the\nentire \"negative\" space in the joint, effectively providing a\ngeometrical/topological metric of the joint's health. We validate the idea on\nthe CT scans in a 50-patient dataset, annotated by experts in maxillofacial\nmedicine, quantitatively compare the asymmetry given the left and the right\nnegative volumes, and automate the entire framework for clinical adoption.",
    "text": "Deep Negative Volume Segmentation\n\nClinical examination of three-dimensional image data of compound anatomical\nobjects, such as complex joints, remains a tedious process, demanding the time\nand the expertise of physicians. For instance, automation of the segmentation\ntask of the TMJ (temporomandibular joint) has been hindered by its compound\nthree-dimensional shape, multiple overlaid textures, an abundance of\nsurrounding irregularities in the skull, and a virtually omnidirectional range\nof the jaw's motion - all of which extend the manual annotation process to more\nthan an hour per patient. To address the challenge, we invent a new angle to\nthe 3D segmentation task: namely, we propose to segment empty spaces between\nall the tissues surrounding the object - the so-called negative volume\nsegmentation. Our approach is an end-to-end pipeline that comprises a V-Net for\nbone segmentation, a 3D volume construction by inflation of the reconstructed\nbone head in all directions along the normal vector to its mesh faces.\nEventually confined within the skull bones, the inflated surface occupies the\nentire \"negative\" space in the joint, effectively providing a\ngeometrical/topological metric of the joint's health. We validate the idea on\nthe CT scans in a 50-patient dataset, annotated by experts in maxillofacial\nmedicine, quantitatively compare the asymmetry given the left and the right\nnegative volumes, and automate the entire framework for clinical adoption."
  },
  {
    "id": "arxiv-312",
    "title": "A Hierarchical Spectral Method for Extreme Classification",
    "abstract": "Extreme classification problems are multiclass and multilabel classification\nproblems where the number of outputs is so large that straightforward\nstrategies are neither statistically nor computationally viable. One strategy\nfor dealing with the computational burden is via a tree decomposition of the\noutput space. While this typically leads to training and inference that scales\nsublinearly with the number of outputs, it also results in reduced statistical\nperformance. In this work, we identify two shortcomings of tree decomposition\nmethods, and describe two heuristic mitigations. We compose these with an\neigenvalue technique for constructing the tree. The end result is a\ncomputationally efficient algorithm that provides good statistical performance\non several extreme data sets.",
    "text": "A Hierarchical Spectral Method for Extreme Classification\n\nExtreme classification problems are multiclass and multilabel classification\nproblems where the number of outputs is so large that straightforward\nstrategies are neither statistically nor computationally viable. One strategy\nfor dealing with the computational burden is via a tree decomposition of the\noutput space. While this typically leads to training and inference that scales\nsublinearly with the number of outputs, it also results in reduced statistical\nperformance. In this work, we identify two shortcomings of tree decomposition\nmethods, and describe two heuristic mitigations. We compose these with an\neigenvalue technique for constructing the tree. The end result is a\ncomputationally efficient algorithm that provides good statistical performance\non several extreme data sets."
  },
  {
    "id": "arxiv-313",
    "title": "Shamap: Shape-based Manifold Learning",
    "abstract": "For manifold learning, it is assumed that high-dimensional sample/data points\nare embedded on a low-dimensional manifold. Usually, distances among samples\nare computed to capture an underlying data structure. Here we propose a metric\naccording to angular changes along a geodesic line, thereby reflecting the\nunderlying shape-oriented information or a topological similarity between high-\nand low-dimensional representations of a data cloud. Our results demonstrate\nthe feasibility and merits of the proposed dimensionality reduction scheme.",
    "text": "Shamap: Shape-based Manifold Learning\n\nFor manifold learning, it is assumed that high-dimensional sample/data points\nare embedded on a low-dimensional manifold. Usually, distances among samples\nare computed to capture an underlying data structure. Here we propose a metric\naccording to angular changes along a geodesic line, thereby reflecting the\nunderlying shape-oriented information or a topological similarity between high-\nand low-dimensional representations of a data cloud. Our results demonstrate\nthe feasibility and merits of the proposed dimensionality reduction scheme."
  },
  {
    "id": "arxiv-314",
    "title": "Automated Polysomnography Analysis for Detection of Non-Apneic and\n  Non-Hypopneic Arousals using Feature Engineering and a Bidirectional LSTM\n  Network",
    "abstract": "Objective: The aim of this study is to develop an automated classification\nalgorithm for polysomnography (PSG) recordings to detect non-apneic and\nnon-hypopneic arousals. Our particular focus is on detecting the respiratory\neffort-related arousals (RERAs) which are very subtle respiratory events that\ndo not meet the criteria for apnea or hypopnea, and are more challenging to\ndetect. Methods: The proposed algorithm is based on a bidirectional long\nshort-term memory (BiLSTM) classifier and 465 multi-domain features, extracted\nfrom multimodal clinical time series. The features consist of a set of\nphysiology-inspired features (n = 75), obtained by multiple steps of feature\nselection and expert analysis, and a set of physiology-agnostic features (n =\n390), derived from scattering transform. Results: The proposed algorithm is\nvalidated on the 2018 PhysioNet challenge dataset. The overall performance in\nterms of the area under the precision-recall curve (AUPRC) is 0.50 on the\nhidden test dataset. This result is tied for the second-best score during the\nfollow-up and official phases of the 2018 PhysioNet challenge. Conclusions: The\nresults demonstrate that it is possible to automatically detect subtle\nnon-apneic/non-hypopneic arousal events from PSG recordings. Significance:\nAutomatic detection of subtle respiratory events such as RERAs together with\nother non-apneic/non-hypopneic arousals will allow detailed annotations of\nlarge PSG databases. This contributes to a better retrospective analysis of\nsleep data, which may also improve the quality of treatment.",
    "text": "Automated Polysomnography Analysis for Detection of Non-Apneic and\n  Non-Hypopneic Arousals using Feature Engineering and a Bidirectional LSTM\n  Network\n\nObjective: The aim of this study is to develop an automated classification\nalgorithm for polysomnography (PSG) recordings to detect non-apneic and\nnon-hypopneic arousals. Our particular focus is on detecting the respiratory\neffort-related arousals (RERAs) which are very subtle respiratory events that\ndo not meet the criteria for apnea or hypopnea, and are more challenging to\ndetect. Methods: The proposed algorithm is based on a bidirectional long\nshort-term memory (BiLSTM) classifier and 465 multi-domain features, extracted\nfrom multimodal clinical time series. The features consist of a set of\nphysiology-inspired features (n = 75), obtained by multiple steps of feature\nselection and expert analysis, and a set of physiology-agnostic features (n =\n390), derived from scattering transform. Results: The proposed algorithm is\nvalidated on the 2018 PhysioNet challenge dataset. The overall performance in\nterms of the area under the precision-recall curve (AUPRC) is 0.50 on the\nhidden test dataset. This result is tied for the second-best score during the\nfollow-up and official phases of the 2018 PhysioNet challenge. Conclusions: The\nresults demonstrate that it is possible to automatically detect subtle\nnon-apneic/non-hypopneic arousal events from PSG recordings. Significance:\nAutomatic detection of subtle respiratory events such as RERAs together with\nother non-apneic/non-hypopneic arousals will allow detailed annotations of\nlarge PSG databases. This contributes to a better retrospective analysis of\nsleep data, which may also improve the quality of treatment."
  },
  {
    "id": "arxiv-315",
    "title": "Low-rank Dictionary Learning for Unsupervised Feature Selection",
    "abstract": "There exist many high-dimensional data in real-world applications such as\nbiology, computer vision, and social networks. Feature selection approaches are\ndevised to confront with high-dimensional data challenges with the aim of\nefficient learning technologies as well as reduction of models complexity. Due\nto the hardship of labeling on these datasets, there are a variety of\napproaches on feature selection process in an unsupervised setting by\nconsidering some important characteristics of data. In this paper, we introduce\na novel unsupervised feature selection approach by applying dictionary learning\nideas in a low-rank representation. Dictionary learning in a low-rank\nrepresentation not only enables us to provide a new representation, but it also\nmaintains feature correlation. Then, spectral analysis is employed to preserve\nsample similarities. Finally, a unified objective function for unsupervised\nfeature selection is proposed in a sparse way by an $\\ell_{2,1}$-norm\nregularization. Furthermore, an efficient numerical algorithm is designed to\nsolve the corresponding optimization problem. We demonstrate the performance of\nthe proposed method based on a variety of standard datasets from different\napplied domains. Our experimental findings reveal that the proposed method\noutperforms the state-of-the-art algorithm.",
    "text": "Low-rank Dictionary Learning for Unsupervised Feature Selection\n\nThere exist many high-dimensional data in real-world applications such as\nbiology, computer vision, and social networks. Feature selection approaches are\ndevised to confront with high-dimensional data challenges with the aim of\nefficient learning technologies as well as reduction of models complexity. Due\nto the hardship of labeling on these datasets, there are a variety of\napproaches on feature selection process in an unsupervised setting by\nconsidering some important characteristics of data. In this paper, we introduce\na novel unsupervised feature selection approach by applying dictionary learning\nideas in a low-rank representation. Dictionary learning in a low-rank\nrepresentation not only enables us to provide a new representation, but it also\nmaintains feature correlation. Then, spectral analysis is employed to preserve\nsample similarities. Finally, a unified objective function for unsupervised\nfeature selection is proposed in a sparse way by an $\\ell_{2,1}$-norm\nregularization. Furthermore, an efficient numerical algorithm is designed to\nsolve the corresponding optimization problem. We demonstrate the performance of\nthe proposed method based on a variety of standard datasets from different\napplied domains. Our experimental findings reveal that the proposed method\noutperforms the state-of-the-art algorithm."
  },
  {
    "id": "arxiv-316",
    "title": "Risk-Constrained Reinforcement Learning with Percentile Risk Criteria",
    "abstract": "In many sequential decision-making problems one is interested in minimizing\nan expected cumulative cost while taking into account \\emph{risk}, i.e.,\nincreased awareness of events of small probability and high consequences.\nAccordingly, the objective of this paper is to present efficient reinforcement\nlearning algorithms for risk-constrained Markov decision processes (MDPs),\nwhere risk is represented via a chance constraint or a constraint on the\nconditional value-at-risk (CVaR) of the cumulative cost. We collectively refer\nto such problems as percentile risk-constrained MDPs.\n  Specifically, we first derive a formula for computing the gradient of the\nLagrangian function for percentile risk-constrained MDPs. Then, we devise\npolicy gradient and actor-critic algorithms that (1) estimate such gradient,\n(2) update the policy in the descent direction, and (3) update the Lagrange\nmultiplier in the ascent direction. For these algorithms we prove convergence\nto locally optimal policies. Finally, we demonstrate the effectiveness of our\nalgorithms in an optimal stopping problem and an online marketing application.",
    "text": "Risk-Constrained Reinforcement Learning with Percentile Risk Criteria\n\nIn many sequential decision-making problems one is interested in minimizing\nan expected cumulative cost while taking into account \\emph{risk}, i.e.,\nincreased awareness of events of small probability and high consequences.\nAccordingly, the objective of this paper is to present efficient reinforcement\nlearning algorithms for risk-constrained Markov decision processes (MDPs),\nwhere risk is represented via a chance constraint or a constraint on the\nconditional value-at-risk (CVaR) of the cumulative cost. We collectively refer\nto such problems as percentile risk-constrained MDPs.\n  Specifically, we first derive a formula for computing the gradient of the\nLagrangian function for percentile risk-constrained MDPs. Then, we devise\npolicy gradient and actor-critic algorithms that (1) estimate such gradient,\n(2) update the policy in the descent direction, and (3) update the Lagrange\nmultiplier in the ascent direction. For these algorithms we prove convergence\nto locally optimal policies. Finally, we demonstrate the effectiveness of our\nalgorithms in an optimal stopping problem and an online marketing application."
  },
  {
    "id": "arxiv-317",
    "title": "Attention Word Embedding",
    "abstract": "Word embedding models learn semantically rich vector representations of words\nand are widely used to initialize natural processing language (NLP) models. The\npopular continuous bag-of-words (CBOW) model of word2vec learns a vector\nembedding by masking a given word in a sentence and then using the other words\nas a context to predict it. A limitation of CBOW is that it equally weights the\ncontext words when making a prediction, which is inefficient, since some words\nhave higher predictive value than others. We tackle this inefficiency by\nintroducing the Attention Word Embedding (AWE) model, which integrates the\nattention mechanism into the CBOW model. We also propose AWE-S, which\nincorporates subword information. We demonstrate that AWE and AWE-S outperform\nthe state-of-the-art word embedding models both on a variety of word similarity\ndatasets and when used for initialization of NLP models.",
    "text": "Attention Word Embedding\n\nWord embedding models learn semantically rich vector representations of words\nand are widely used to initialize natural processing language (NLP) models. The\npopular continuous bag-of-words (CBOW) model of word2vec learns a vector\nembedding by masking a given word in a sentence and then using the other words\nas a context to predict it. A limitation of CBOW is that it equally weights the\ncontext words when making a prediction, which is inefficient, since some words\nhave higher predictive value than others. We tackle this inefficiency by\nintroducing the Attention Word Embedding (AWE) model, which integrates the\nattention mechanism into the CBOW model. We also propose AWE-S, which\nincorporates subword information. We demonstrate that AWE and AWE-S outperform\nthe state-of-the-art word embedding models both on a variety of word similarity\ndatasets and when used for initialization of NLP models."
  },
  {
    "id": "arxiv-318",
    "title": "Estimating Feature-Label Dependence Using Gini Distance Statistics",
    "abstract": "Identifying statistical dependence between the features and the label is a\nfundamental problem in supervised learning. This paper presents a framework for\nestimating dependence between numerical features and a categorical label using\ngeneralized Gini distance, an energy distance in reproducing kernel Hilbert\nspaces (RKHS). Two Gini distance based dependence measures are explored: Gini\ndistance covariance and Gini distance correlation. Unlike Pearson covariance\nand correlation, which do not characterize independence, the above Gini\ndistance based measures define dependence as well as independence of random\nvariables. The test statistics are simple to calculate and do not require\nprobability density estimation. Uniform convergence bounds and asymptotic\nbounds are derived for the test statistics. Comparisons with distance\ncovariance statistics are provided. It is shown that Gini distance statistics\nconverge faster than distance covariance statistics in the uniform convergence\nbounds, hence tighter upper bounds on both Type I and Type II errors. Moreover,\nthe probability of Gini distance covariance statistic under-performing the\ndistance covariance statistic in Type II error decreases to 0 exponentially\nwith the increase of the sample size. Extensive experimental results are\npresented to demonstrate the performance of the proposed method.",
    "text": "Estimating Feature-Label Dependence Using Gini Distance Statistics\n\nIdentifying statistical dependence between the features and the label is a\nfundamental problem in supervised learning. This paper presents a framework for\nestimating dependence between numerical features and a categorical label using\ngeneralized Gini distance, an energy distance in reproducing kernel Hilbert\nspaces (RKHS). Two Gini distance based dependence measures are explored: Gini\ndistance covariance and Gini distance correlation. Unlike Pearson covariance\nand correlation, which do not characterize independence, the above Gini\ndistance based measures define dependence as well as independence of random\nvariables. The test statistics are simple to calculate and do not require\nprobability density estimation. Uniform convergence bounds and asymptotic\nbounds are derived for the test statistics. Comparisons with distance\ncovariance statistics are provided. It is shown that Gini distance statistics\nconverge faster than distance covariance statistics in the uniform convergence\nbounds, hence tighter upper bounds on both Type I and Type II errors. Moreover,\nthe probability of Gini distance covariance statistic under-performing the\ndistance covariance statistic in Type II error decreases to 0 exponentially\nwith the increase of the sample size. Extensive experimental results are\npresented to demonstrate the performance of the proposed method."
  },
  {
    "id": "arxiv-319",
    "title": "A Simple Fix for Convolutional Neural Network via Coordinate Embedding",
    "abstract": "Convolutional Neural Networks (CNN) has been widely applied in the realm of\ncomputer vision. However, given the fact that CNN models are translation\ninvariant, they are not aware of the coordinate information of each pixel. Thus\nthe generalization ability of CNN will be limited since the coordinate\ninformation is crucial for a model to learn affine transformations which\ndirectly operate on the coordinate of each pixel. In this project, we proposed\na simple approach to incorporate the coordinate information to the CNN model\nthrough coordinate embedding. Our approach does not change the downstream model\narchitecture and can be easily applied to the pre-trained models for the task\nlike object detection. Our experiments on the German Traffic Sign Detection\nBenchmark show that our approach not only significantly improve the model\nperformance but also have better robustness with respect to the affine\ntransformation.",
    "text": "A Simple Fix for Convolutional Neural Network via Coordinate Embedding\n\nConvolutional Neural Networks (CNN) has been widely applied in the realm of\ncomputer vision. However, given the fact that CNN models are translation\ninvariant, they are not aware of the coordinate information of each pixel. Thus\nthe generalization ability of CNN will be limited since the coordinate\ninformation is crucial for a model to learn affine transformations which\ndirectly operate on the coordinate of each pixel. In this project, we proposed\na simple approach to incorporate the coordinate information to the CNN model\nthrough coordinate embedding. Our approach does not change the downstream model\narchitecture and can be easily applied to the pre-trained models for the task\nlike object detection. Our experiments on the German Traffic Sign Detection\nBenchmark show that our approach not only significantly improve the model\nperformance but also have better robustness with respect to the affine\ntransformation."
  },
  {
    "id": "arxiv-320",
    "title": "Inadequacy of Linear Methods for Minimal Sensor Placement and Feature\n  Selection in Nonlinear Systems; a New Approach Using Secants",
    "abstract": "Sensor placement and feature selection are critical steps in engineering,\nmodeling, and data science that share a common mathematical theme: the selected\nmeasurements should enable solution of an inverse problem. Most real-world\nsystems of interest are nonlinear, yet the majority of available techniques for\nfeature selection and sensor placement rely on assumptions of linearity or\nsimple statistical models. We show that when these assumptions are violated,\nstandard techniques can lead to costly over-sensing without guaranteeing that\nthe desired information can be recovered from the measurements. In order to\nremedy these problems, we introduce a novel data-driven approach for sensor\nplacement and feature selection for a general type of nonlinear inverse problem\nbased on the information contained in secant vectors between data points. Using\nthe secant-based approach, we develop three efficient greedy algorithms that\neach provide different types of robust, near-minimal reconstruction guarantees.\nWe demonstrate them on two problems where linear techniques consistently fail:\nsensor placement to reconstruct a fluid flow formed by a complicated\nshock-mixing layer interaction and selecting fundamental manifold learning\ncoordinates on a torus.",
    "text": "Inadequacy of Linear Methods for Minimal Sensor Placement and Feature\n  Selection in Nonlinear Systems; a New Approach Using Secants\n\nSensor placement and feature selection are critical steps in engineering,\nmodeling, and data science that share a common mathematical theme: the selected\nmeasurements should enable solution of an inverse problem. Most real-world\nsystems of interest are nonlinear, yet the majority of available techniques for\nfeature selection and sensor placement rely on assumptions of linearity or\nsimple statistical models. We show that when these assumptions are violated,\nstandard techniques can lead to costly over-sensing without guaranteeing that\nthe desired information can be recovered from the measurements. In order to\nremedy these problems, we introduce a novel data-driven approach for sensor\nplacement and feature selection for a general type of nonlinear inverse problem\nbased on the information contained in secant vectors between data points. Using\nthe secant-based approach, we develop three efficient greedy algorithms that\neach provide different types of robust, near-minimal reconstruction guarantees.\nWe demonstrate them on two problems where linear techniques consistently fail:\nsensor placement to reconstruct a fluid flow formed by a complicated\nshock-mixing layer interaction and selecting fundamental manifold learning\ncoordinates on a torus."
  },
  {
    "id": "arxiv-321",
    "title": "Scattering Networks for Hybrid Representation Learning",
    "abstract": "Scattering networks are a class of designed Convolutional Neural Networks\n(CNNs) with fixed weights. We argue they can serve as generic representations\nfor modelling images. In particular, by working in scattering space, we achieve\ncompetitive results both for supervised and unsupervised learning tasks, while\nmaking progress towards constructing more interpretable CNNs. For supervised\nlearning, we demonstrate that the early layers of CNNs do not necessarily need\nto be learned, and can be replaced with a scattering network instead. Indeed,\nusing hybrid architectures, we achieve the best results with predefined\nrepresentations to-date, while being competitive with end-to-end learned CNNs.\nSpecifically, even applying a shallow cascade of small-windowed scattering\ncoefficients followed by 1$\\times$1-convolutions results in AlexNet accuracy on\nthe ILSVRC2012 classification task. Moreover, by combining scattering networks\nwith deep residual networks, we achieve a single-crop top-5 error of 11.4% on\nILSVRC2012. Also, we show they can yield excellent performance in the small\nsample regime on CIFAR-10 and STL-10 datasets, exceeding their end-to-end\ncounterparts, through their ability to incorporate geometrical priors. For\nunsupervised learning, scattering coefficients can be a competitive\nrepresentation that permits image recovery. We use this fact to train hybrid\nGANs to generate images. Finally, we empirically analyze several properties\nrelated to stability and reconstruction of images from scattering coefficients.",
    "text": "Scattering Networks for Hybrid Representation Learning\n\nScattering networks are a class of designed Convolutional Neural Networks\n(CNNs) with fixed weights. We argue they can serve as generic representations\nfor modelling images. In particular, by working in scattering space, we achieve\ncompetitive results both for supervised and unsupervised learning tasks, while\nmaking progress towards constructing more interpretable CNNs. For supervised\nlearning, we demonstrate that the early layers of CNNs do not necessarily need\nto be learned, and can be replaced with a scattering network instead. Indeed,\nusing hybrid architectures, we achieve the best results with predefined\nrepresentations to-date, while being competitive with end-to-end learned CNNs.\nSpecifically, even applying a shallow cascade of small-windowed scattering\ncoefficients followed by 1$\\times$1-convolutions results in AlexNet accuracy on\nthe ILSVRC2012 classification task. Moreover, by combining scattering networks\nwith deep residual networks, we achieve a single-crop top-5 error of 11.4% on\nILSVRC2012. Also, we show they can yield excellent performance in the small\nsample regime on CIFAR-10 and STL-10 datasets, exceeding their end-to-end\ncounterparts, through their ability to incorporate geometrical priors. For\nunsupervised learning, scattering coefficients can be a competitive\nrepresentation that permits image recovery. We use this fact to train hybrid\nGANs to generate images. Finally, we empirically analyze several properties\nrelated to stability and reconstruction of images from scattering coefficients."
  },
  {
    "id": "arxiv-322",
    "title": "Multichannel Speech Enhancement by Raw Waveform-mapping using Fully\n  Convolutional Networks",
    "abstract": "In recent years, waveform-mapping-based speech enhancement (SE) methods have\ngarnered significant attention. These methods generally use a deep learning\nmodel to directly process and reconstruct speech waveforms. Because both the\ninput and output are in waveform format, the waveform-mapping-based SE methods\ncan overcome the distortion caused by imperfect phase estimation, which may be\nencountered in spectral-mapping-based SE systems. So far, most\nwaveform-mapping-based SE methods have focused on single-channel tasks. In this\npaper, we propose a novel fully convolutional network (FCN) with Sinc and\ndilated convolutional layers (termed SDFCN) for multichannel SE that operates\nin the time domain. We also propose an extended version of SDFCN, called the\nresidual SDFCN (termed rSDFCN). The proposed methods are evaluated on two\nmultichannel SE tasks, namely the dual-channel inner-ear microphones SE task\nand the distributed microphones SE task. The experimental results confirm the\noutstanding denoising capability of the proposed SE systems on both tasks and\nthe benefits of using the residual architecture on the overall SE performance.",
    "text": "Multichannel Speech Enhancement by Raw Waveform-mapping using Fully\n  Convolutional Networks\n\nIn recent years, waveform-mapping-based speech enhancement (SE) methods have\ngarnered significant attention. These methods generally use a deep learning\nmodel to directly process and reconstruct speech waveforms. Because both the\ninput and output are in waveform format, the waveform-mapping-based SE methods\ncan overcome the distortion caused by imperfect phase estimation, which may be\nencountered in spectral-mapping-based SE systems. So far, most\nwaveform-mapping-based SE methods have focused on single-channel tasks. In this\npaper, we propose a novel fully convolutional network (FCN) with Sinc and\ndilated convolutional layers (termed SDFCN) for multichannel SE that operates\nin the time domain. We also propose an extended version of SDFCN, called the\nresidual SDFCN (termed rSDFCN). The proposed methods are evaluated on two\nmultichannel SE tasks, namely the dual-channel inner-ear microphones SE task\nand the distributed microphones SE task. The experimental results confirm the\noutstanding denoising capability of the proposed SE systems on both tasks and\nthe benefits of using the residual architecture on the overall SE performance."
  },
  {
    "id": "arxiv-323",
    "title": "A Deep Bayesian Neural Network for Cardiac Arrhythmia Classification\n  with Rejection from ECG Recordings",
    "abstract": "With the development of deep learning-based methods, automated classification\nof electrocardiograms (ECGs) has recently gained much attention. Although the\neffectiveness of deep neural networks has been encouraging, the lack of\ninformation given by the outputs restricts clinicians' reexamination. If the\nuncertainty estimation comes along with the classification results,\ncardiologists can pay more attention to \"uncertain\" cases. Our study aims to\nclassify ECGs with rejection based on data uncertainty and model uncertainty.\nWe perform experiments on a real-world 12-lead ECG dataset. First, we estimate\nuncertainties using the Monte Carlo dropout for each classification prediction,\nbased on our Bayesian neural network. Then, we accept predictions with\nuncertainty under a given threshold and provide \"uncertain\" cases for\nclinicians. Furthermore, we perform a simulation experiment using varying\nthresholds. Finally, with the help of a clinician, we conduct case studies to\nexplain the results of large uncertainties and incorrect predictions with small\nuncertainties. The results show that correct predictions are more likely to\nhave smaller uncertainties, and the performance on accepted predictions\nimproves as the accepting ratio decreases (i.e. more rejections). Case studies\nalso help explain why rejection can improve the performance. Our study helps\nneural networks produce more accurate results and provide information on\nuncertainties to better assist clinicians in the diagnosis process. It can also\nenable deep-learning-based ECG interpretation in clinical implementation.",
    "text": "A Deep Bayesian Neural Network for Cardiac Arrhythmia Classification\n  with Rejection from ECG Recordings\n\nWith the development of deep learning-based methods, automated classification\nof electrocardiograms (ECGs) has recently gained much attention. Although the\neffectiveness of deep neural networks has been encouraging, the lack of\ninformation given by the outputs restricts clinicians' reexamination. If the\nuncertainty estimation comes along with the classification results,\ncardiologists can pay more attention to \"uncertain\" cases. Our study aims to\nclassify ECGs with rejection based on data uncertainty and model uncertainty.\nWe perform experiments on a real-world 12-lead ECG dataset. First, we estimate\nuncertainties using the Monte Carlo dropout for each classification prediction,\nbased on our Bayesian neural network. Then, we accept predictions with\nuncertainty under a given threshold and provide \"uncertain\" cases for\nclinicians. Furthermore, we perform a simulation experiment using varying\nthresholds. Finally, with the help of a clinician, we conduct case studies to\nexplain the results of large uncertainties and incorrect predictions with small\nuncertainties. The results show that correct predictions are more likely to\nhave smaller uncertainties, and the performance on accepted predictions\nimproves as the accepting ratio decreases (i.e. more rejections). Case studies\nalso help explain why rejection can improve the performance. Our study helps\nneural networks produce more accurate results and provide information on\nuncertainties to better assist clinicians in the diagnosis process. It can also\nenable deep-learning-based ECG interpretation in clinical implementation."
  },
  {
    "id": "arxiv-324",
    "title": "An Empirical Study on the Generalization Power of Neural Representations\n  Learned via Visual Guessing Games",
    "abstract": "Guessing games are a prototypical instance of the \"learning by interacting\"\nparadigm. This work investigates how well an artificial agent can benefit from\nplaying guessing games when later asked to perform on novel NLP downstream\ntasks such as Visual Question Answering (VQA). We propose two ways to exploit\nplaying guessing games: 1) a supervised learning scenario in which the agent\nlearns to mimic successful guessing games and 2) a novel way for an agent to\nplay by itself, called Self-play via Iterated Experience Learning (SPIEL).\n  We evaluate the ability of both procedures to generalize: an in-domain\nevaluation shows an increased accuracy (+7.79) compared with competitors on the\nevaluation suite CompGuessWhat?!; a transfer evaluation shows improved\nperformance for VQA on the TDIUC dataset in terms of harmonic average accuracy\n(+5.31) thanks to more fine-grained object representations learned via SPIEL.",
    "text": "An Empirical Study on the Generalization Power of Neural Representations\n  Learned via Visual Guessing Games\n\nGuessing games are a prototypical instance of the \"learning by interacting\"\nparadigm. This work investigates how well an artificial agent can benefit from\nplaying guessing games when later asked to perform on novel NLP downstream\ntasks such as Visual Question Answering (VQA). We propose two ways to exploit\nplaying guessing games: 1) a supervised learning scenario in which the agent\nlearns to mimic successful guessing games and 2) a novel way for an agent to\nplay by itself, called Self-play via Iterated Experience Learning (SPIEL).\n  We evaluate the ability of both procedures to generalize: an in-domain\nevaluation shows an increased accuracy (+7.79) compared with competitors on the\nevaluation suite CompGuessWhat?!; a transfer evaluation shows improved\nperformance for VQA on the TDIUC dataset in terms of harmonic average accuracy\n(+5.31) thanks to more fine-grained object representations learned via SPIEL."
  },
  {
    "id": "arxiv-325",
    "title": "Residual Attention based Network for Hand Bone Age Assessment",
    "abstract": "Computerized automatic methods have been employed to boost the productivity\nas well as objectiveness of hand bone age assessment. These approaches make\npredictions according to the whole X-ray images, which include other objects\nthat may introduce distractions. Instead, our framework is inspired by the\nclinical workflow (Tanner-Whitehouse) of hand bone age assessment, which\nfocuses on the key components of the hand. The proposed framework is composed\nof two components: a Mask R-CNN subnet of pixelwise hand segmentation and a\nresidual attention network for hand bone age assessment. The Mask R-CNN subnet\nsegments the hands from X-ray images to avoid the distractions of other objects\n(e.g., X-ray tags). The hierarchical attention components of the residual\nattention subnet force our network to focus on the key components of the X-ray\nimages and generate the final predictions as well as the associated visual\nsupports, which is similar to the assessment procedure of clinicians. We\nevaluate the performance of the proposed pipeline on the RSNA pediatric bone\nage dataset and the results demonstrate its superiority over the previous\nmethods.",
    "text": "Residual Attention based Network for Hand Bone Age Assessment\n\nComputerized automatic methods have been employed to boost the productivity\nas well as objectiveness of hand bone age assessment. These approaches make\npredictions according to the whole X-ray images, which include other objects\nthat may introduce distractions. Instead, our framework is inspired by the\nclinical workflow (Tanner-Whitehouse) of hand bone age assessment, which\nfocuses on the key components of the hand. The proposed framework is composed\nof two components: a Mask R-CNN subnet of pixelwise hand segmentation and a\nresidual attention network for hand bone age assessment. The Mask R-CNN subnet\nsegments the hands from X-ray images to avoid the distractions of other objects\n(e.g., X-ray tags). The hierarchical attention components of the residual\nattention subnet force our network to focus on the key components of the X-ray\nimages and generate the final predictions as well as the associated visual\nsupports, which is similar to the assessment procedure of clinicians. We\nevaluate the performance of the proposed pipeline on the RSNA pediatric bone\nage dataset and the results demonstrate its superiority over the previous\nmethods."
  },
  {
    "id": "arxiv-326",
    "title": "Black-Box Adversarial Attack with Transferable Model-based Embedding",
    "abstract": "We present a new method for black-box adversarial attack. Unlike previous\nmethods that combined transfer-based and scored-based methods by using the\ngradient or initialization of a surrogate white-box model, this new method\ntries to learn a low-dimensional embedding using a pretrained model, and then\nperforms efficient search within the embedding space to attack an unknown\ntarget network. The method produces adversarial perturbations with high level\nsemantic patterns that are easily transferable. We show that this approach can\ngreatly improve the query efficiency of black-box adversarial attack across\ndifferent target network architectures. We evaluate our approach on MNIST,\nImageNet and Google Cloud Vision API, resulting in a significant reduction on\nthe number of queries. We also attack adversarially defended networks on\nCIFAR10 and ImageNet, where our method not only reduces the number of queries,\nbut also improves the attack success rate.",
    "text": "Black-Box Adversarial Attack with Transferable Model-based Embedding\n\nWe present a new method for black-box adversarial attack. Unlike previous\nmethods that combined transfer-based and scored-based methods by using the\ngradient or initialization of a surrogate white-box model, this new method\ntries to learn a low-dimensional embedding using a pretrained model, and then\nperforms efficient search within the embedding space to attack an unknown\ntarget network. The method produces adversarial perturbations with high level\nsemantic patterns that are easily transferable. We show that this approach can\ngreatly improve the query efficiency of black-box adversarial attack across\ndifferent target network architectures. We evaluate our approach on MNIST,\nImageNet and Google Cloud Vision API, resulting in a significant reduction on\nthe number of queries. We also attack adversarially defended networks on\nCIFAR10 and ImageNet, where our method not only reduces the number of queries,\nbut also improves the attack success rate."
  },
  {
    "id": "arxiv-327",
    "title": "SplitGuard: Detecting and Mitigating Training-Hijacking Attacks in Split\n  Learning",
    "abstract": "Distributed deep learning frameworks, such as split learning, have recently\nbeen proposed to enable a group of participants to collaboratively train a deep\nneural network without sharing their raw data. Split learning in particular\nachieves this goal by dividing a neural network between a client and a server\nso that the client computes the initial set of layers, and the server computes\nthe rest. However, this method introduces a unique attack vector for a\nmalicious server attempting to steal the client's private data: the server can\ndirect the client model towards learning a task of its choice. With a concrete\nexample already proposed, such training-hijacking attacks present a significant\nrisk for the data privacy of split learning clients.\n  In this paper, we propose SplitGuard, a method by which a split learning\nclient can detect whether it is being targeted by a training-hijacking attack\nor not. We experimentally evaluate its effectiveness, and discuss in detail\nvarious points related to its use. We conclude that SplitGuard can effectively\ndetect training-hijacking attacks while minimizing the amount of information\nrecovered by the adversaries.",
    "text": "SplitGuard: Detecting and Mitigating Training-Hijacking Attacks in Split\n  Learning\n\nDistributed deep learning frameworks, such as split learning, have recently\nbeen proposed to enable a group of participants to collaboratively train a deep\nneural network without sharing their raw data. Split learning in particular\nachieves this goal by dividing a neural network between a client and a server\nso that the client computes the initial set of layers, and the server computes\nthe rest. However, this method introduces a unique attack vector for a\nmalicious server attempting to steal the client's private data: the server can\ndirect the client model towards learning a task of its choice. With a concrete\nexample already proposed, such training-hijacking attacks present a significant\nrisk for the data privacy of split learning clients.\n  In this paper, we propose SplitGuard, a method by which a split learning\nclient can detect whether it is being targeted by a training-hijacking attack\nor not. We experimentally evaluate its effectiveness, and discuss in detail\nvarious points related to its use. We conclude that SplitGuard can effectively\ndetect training-hijacking attacks while minimizing the amount of information\nrecovered by the adversaries."
  },
  {
    "id": "arxiv-328",
    "title": "Generating Natural Adversarial Hyperspectral examples with a modified\n  Wasserstein GAN",
    "abstract": "Adversarial examples are a hot topic due to their abilities to fool a\nclassifier's prediction. There are two strategies to create such examples, one\nuses the attacked classifier's gradients, while the other only requires access\nto the clas-sifier's prediction. This is particularly appealing when the\nclassifier is not full known (black box model). In this paper, we present a new\nmethod which is able to generate natural adversarial examples from the true\ndata following the second paradigm. Based on Generative Adversarial Networks\n(GANs) [5], it reweights the true data empirical distribution to encourage the\nclassifier to generate ad-versarial examples. We provide a proof of concept of\nour method by generating adversarial hyperspectral signatures on a remote\nsensing dataset.",
    "text": "Generating Natural Adversarial Hyperspectral examples with a modified\n  Wasserstein GAN\n\nAdversarial examples are a hot topic due to their abilities to fool a\nclassifier's prediction. There are two strategies to create such examples, one\nuses the attacked classifier's gradients, while the other only requires access\nto the clas-sifier's prediction. This is particularly appealing when the\nclassifier is not full known (black box model). In this paper, we present a new\nmethod which is able to generate natural adversarial examples from the true\ndata following the second paradigm. Based on Generative Adversarial Networks\n(GANs) [5], it reweights the true data empirical distribution to encourage the\nclassifier to generate ad-versarial examples. We provide a proof of concept of\nour method by generating adversarial hyperspectral signatures on a remote\nsensing dataset."
  },
  {
    "id": "arxiv-329",
    "title": "Mosques Smart Domes System using Machine Learning Algorithms",
    "abstract": "Millions of mosques around the world are suffering some problems such as\nventilation and difficulty getting rid of bacteria, especially in rush hours\nwhere congestion in mosques leads to air pollution and spread of bacteria, in\naddition to unpleasant odors and to a state of discomfort during the pray\ntimes, where in most mosques there are no enough windows to ventilate the\nmosque well. This paper aims to solve these problems by building a model of\nsmart mosques domes using weather features and outside temperatures. Machine\nlearning algorithms such as k Nearest Neighbors and Decision Tree were applied\nto predict the state of the domes open or close. The experiments of this paper\nwere applied on Prophet mosque in Saudi Arabia, which basically contains twenty\nseven manually moving domes. Both machine learning algorithms were tested and\nevaluated using different evaluation methods. After comparing the results for\nboth algorithms, DT algorithm was achieved higher accuracy 98% comparing with\n95% accuracy for kNN algorithm. Finally, the results of this study were\npromising and will be helpful for all mosques to use our proposed model for\ncontrolling domes automatically.",
    "text": "Mosques Smart Domes System using Machine Learning Algorithms\n\nMillions of mosques around the world are suffering some problems such as\nventilation and difficulty getting rid of bacteria, especially in rush hours\nwhere congestion in mosques leads to air pollution and spread of bacteria, in\naddition to unpleasant odors and to a state of discomfort during the pray\ntimes, where in most mosques there are no enough windows to ventilate the\nmosque well. This paper aims to solve these problems by building a model of\nsmart mosques domes using weather features and outside temperatures. Machine\nlearning algorithms such as k Nearest Neighbors and Decision Tree were applied\nto predict the state of the domes open or close. The experiments of this paper\nwere applied on Prophet mosque in Saudi Arabia, which basically contains twenty\nseven manually moving domes. Both machine learning algorithms were tested and\nevaluated using different evaluation methods. After comparing the results for\nboth algorithms, DT algorithm was achieved higher accuracy 98% comparing with\n95% accuracy for kNN algorithm. Finally, the results of this study were\npromising and will be helpful for all mosques to use our proposed model for\ncontrolling domes automatically."
  },
  {
    "id": "arxiv-330",
    "title": "Threshold Designer Adaptation: Improved Adaptation for Designers in\n  Co-creative Systems",
    "abstract": "To best assist human designers with different styles, Machine Learning (ML)\nsystems need to be able to adapt to them. However, there has been relatively\nlittle prior work on how and when to best adapt an ML system to a co-designer.\nIn this paper we present threshold designer adaptation: a novel method for\nadapting a creative ML model to an individual designer. We evaluate our\napproach with a human subject study using a co-creative rhythm game design\ntool. We find that designers prefer our proposed method and produce higher\nquality content in comparison to an existing baseline.",
    "text": "Threshold Designer Adaptation: Improved Adaptation for Designers in\n  Co-creative Systems\n\nTo best assist human designers with different styles, Machine Learning (ML)\nsystems need to be able to adapt to them. However, there has been relatively\nlittle prior work on how and when to best adapt an ML system to a co-designer.\nIn this paper we present threshold designer adaptation: a novel method for\nadapting a creative ML model to an individual designer. We evaluate our\napproach with a human subject study using a co-creative rhythm game design\ntool. We find that designers prefer our proposed method and produce higher\nquality content in comparison to an existing baseline."
  },
  {
    "id": "arxiv-331",
    "title": "Explainable Artificial Intelligence (XAI) for Internet of Things: A Survey",
    "abstract": "Black-box nature of Artificial Intelligence (AI) models do not allow users to\ncomprehend and sometimes trust the output created by such model. In AI\napplications, where not only the results but also the decision paths to the\nresults are critical, such black-box AI models are not sufficient. Explainable\nArtificial Intelligence (XAI) addresses this problem and defines a set of AI\nmodels that are interpretable by the users. Recently, several number of XAI\nmodels have been to address the issues surrounding by lack of interpretability\nand explainability of black-box models in various application areas such as\nhealthcare, military, energy, financial and industrial domains. Although the\nconcept of XAI has gained great deal of attention recently, its integration\ninto the IoT domain has not yet been fully defined. In this paper, we provide\nan in-depth and systematic review of recent studies using XAI models in the\nscope of IoT domain. We categorize the studies according to their methodology\nand applications areas. In addition, we aim to focus on the challenging\nproblems and open issues and give future directions to guide the developers and\nresearchers for prospective future investigations.",
    "text": "Explainable Artificial Intelligence (XAI) for Internet of Things: A Survey\n\nBlack-box nature of Artificial Intelligence (AI) models do not allow users to\ncomprehend and sometimes trust the output created by such model. In AI\napplications, where not only the results but also the decision paths to the\nresults are critical, such black-box AI models are not sufficient. Explainable\nArtificial Intelligence (XAI) addresses this problem and defines a set of AI\nmodels that are interpretable by the users. Recently, several number of XAI\nmodels have been to address the issues surrounding by lack of interpretability\nand explainability of black-box models in various application areas such as\nhealthcare, military, energy, financial and industrial domains. Although the\nconcept of XAI has gained great deal of attention recently, its integration\ninto the IoT domain has not yet been fully defined. In this paper, we provide\nan in-depth and systematic review of recent studies using XAI models in the\nscope of IoT domain. We categorize the studies according to their methodology\nand applications areas. In addition, we aim to focus on the challenging\nproblems and open issues and give future directions to guide the developers and\nresearchers for prospective future investigations."
  },
  {
    "id": "arxiv-332",
    "title": "Domain-Adversarial and Conditional State Space Model for Imitation\n  Learning",
    "abstract": "State representation learning (SRL) in partially observable Markov decision\nprocesses has been studied to learn abstract features of data useful for robot\ncontrol tasks. For SRL, acquiring domain-agnostic states is essential for\nachieving efficient imitation learning. Without these states, imitation\nlearning is hampered by domain-dependent information useless for control.\nHowever, existing methods fail to remove such disturbances from the states when\nthe data from experts and agents show large domain shifts. To overcome this\nissue, we propose a domain-adversarial and conditional state space model\n(DAC-SSM) that enables control systems to obtain domain-agnostic and task- and\ndynamics-aware states. DAC-SSM jointly optimizes the state inference,\nobservation reconstruction, forward dynamics, and reward models. To remove\ndomain-dependent information from the states, the model is trained with domain\ndiscriminators in an adversarial manner, and the reconstruction is conditioned\non domain labels. We experimentally evaluated the model predictive control\nperformance via imitation learning for continuous control of sparse reward\ntasks in simulators and compared it with the performance of the existing SRL\nmethod. The agents from DAC-SSM achieved performance comparable to experts and\nmore than twice the baselines. We conclude domain-agnostic states are essential\nfor imitation learning that has large domain shifts and can be obtained using\nDAC-SSM.",
    "text": "Domain-Adversarial and Conditional State Space Model for Imitation\n  Learning\n\nState representation learning (SRL) in partially observable Markov decision\nprocesses has been studied to learn abstract features of data useful for robot\ncontrol tasks. For SRL, acquiring domain-agnostic states is essential for\nachieving efficient imitation learning. Without these states, imitation\nlearning is hampered by domain-dependent information useless for control.\nHowever, existing methods fail to remove such disturbances from the states when\nthe data from experts and agents show large domain shifts. To overcome this\nissue, we propose a domain-adversarial and conditional state space model\n(DAC-SSM) that enables control systems to obtain domain-agnostic and task- and\ndynamics-aware states. DAC-SSM jointly optimizes the state inference,\nobservation reconstruction, forward dynamics, and reward models. To remove\ndomain-dependent information from the states, the model is trained with domain\ndiscriminators in an adversarial manner, and the reconstruction is conditioned\non domain labels. We experimentally evaluated the model predictive control\nperformance via imitation learning for continuous control of sparse reward\ntasks in simulators and compared it with the performance of the existing SRL\nmethod. The agents from DAC-SSM achieved performance comparable to experts and\nmore than twice the baselines. We conclude domain-agnostic states are essential\nfor imitation learning that has large domain shifts and can be obtained using\nDAC-SSM."
  },
  {
    "id": "arxiv-333",
    "title": "Using Deep Learning Techniques and Inferential Speech Statistics for AI\n  Synthesised Speech Recognition",
    "abstract": "The recent developments in technology have re-warded us with amazing audio\nsynthesis models like TACOTRON and WAVENETS. On the other side, it poses\ngreater threats such as speech clones and deep fakes, that may go undetected.\nTo tackle these alarming situations, there is an urgent need to propose models\nthat can help discriminate a synthesized speech from an actual human speech and\nalso identify the source of such a synthesis. Here, we propose a model based on\nConvolutional Neural Network (CNN) and Bidirectional Recurrent Neural Network\n(BiRNN) that helps to achieve both the aforementioned objectives. The temporal\ndependencies present in AI synthesized speech are exploited using Bidirectional\nRNN and CNN. The model outperforms the state-of-the-art approaches by\nclassifying the AI synthesized audio from real human speech with an error rate\nof 1.9% and detecting the underlying architecture with an accuracy of 97%.",
    "text": "Using Deep Learning Techniques and Inferential Speech Statistics for AI\n  Synthesised Speech Recognition\n\nThe recent developments in technology have re-warded us with amazing audio\nsynthesis models like TACOTRON and WAVENETS. On the other side, it poses\ngreater threats such as speech clones and deep fakes, that may go undetected.\nTo tackle these alarming situations, there is an urgent need to propose models\nthat can help discriminate a synthesized speech from an actual human speech and\nalso identify the source of such a synthesis. Here, we propose a model based on\nConvolutional Neural Network (CNN) and Bidirectional Recurrent Neural Network\n(BiRNN) that helps to achieve both the aforementioned objectives. The temporal\ndependencies present in AI synthesized speech are exploited using Bidirectional\nRNN and CNN. The model outperforms the state-of-the-art approaches by\nclassifying the AI synthesized audio from real human speech with an error rate\nof 1.9% and detecting the underlying architecture with an accuracy of 97%."
  },
  {
    "id": "arxiv-334",
    "title": "Localized Adversarial Domain Generalization",
    "abstract": "Deep learning methods can struggle to handle domain shifts not seen in\ntraining data, which can cause them to not generalize well to unseen domains.\nThis has led to research attention on domain generalization (DG), which aims to\nthe model's generalization ability to out-of-distribution. Adversarial domain\ngeneralization is a popular approach to DG, but conventional approaches (1)\nstruggle to sufficiently align features so that local neighborhoods are mixed\nacross domains; and (2) can suffer from feature space over collapse which can\nthreaten generalization performance. To address these limitations, we propose\nlocalized adversarial domain generalization with space compactness\nmaintenance~(LADG) which constitutes two major contributions. First, we propose\nan adversarial localized classifier as the domain discriminator, along with a\nprincipled primary branch. This constructs a min-max game whereby the aim of\nthe featurizer is to produce locally mixed domains. Second, we propose to use a\ncoding-rate loss to alleviate feature space over collapse. We conduct\ncomprehensive experiments on the Wilds DG benchmark to validate our approach,\nwhere LADG outperforms leading competitors on most datasets.",
    "text": "Localized Adversarial Domain Generalization\n\nDeep learning methods can struggle to handle domain shifts not seen in\ntraining data, which can cause them to not generalize well to unseen domains.\nThis has led to research attention on domain generalization (DG), which aims to\nthe model's generalization ability to out-of-distribution. Adversarial domain\ngeneralization is a popular approach to DG, but conventional approaches (1)\nstruggle to sufficiently align features so that local neighborhoods are mixed\nacross domains; and (2) can suffer from feature space over collapse which can\nthreaten generalization performance. To address these limitations, we propose\nlocalized adversarial domain generalization with space compactness\nmaintenance~(LADG) which constitutes two major contributions. First, we propose\nan adversarial localized classifier as the domain discriminator, along with a\nprincipled primary branch. This constructs a min-max game whereby the aim of\nthe featurizer is to produce locally mixed domains. Second, we propose to use a\ncoding-rate loss to alleviate feature space over collapse. We conduct\ncomprehensive experiments on the Wilds DG benchmark to validate our approach,\nwhere LADG outperforms leading competitors on most datasets."
  },
  {
    "id": "arxiv-335",
    "title": "Using Known Information to Accelerate HyperParameters Optimization Based\n  on SMBO",
    "abstract": "Automl is the key technology for machine learning problem. Current state of\nart hyperparameter optimization methods are based on traditional black-box\noptimization methods like SMBO (SMAC, TPE). The objective function of black-box\noptimization is non-smooth, or time-consuming to evaluate, or in some way\nnoisy. Recent years, many researchers offered the work about the properties of\nhyperparameters. However, traditional hyperparameter optimization methods do\nnot take those information into consideration. In this paper, we use gradient\ninformation and machine learning model analysis information to accelerate\ntraditional hyperparameter optimization methods SMBO. In our L2 norm\nexperiments, our method yielded state-of-the-art performance, and in many cases\noutperformed the previous best configuration approach.",
    "text": "Using Known Information to Accelerate HyperParameters Optimization Based\n  on SMBO\n\nAutoml is the key technology for machine learning problem. Current state of\nart hyperparameter optimization methods are based on traditional black-box\noptimization methods like SMBO (SMAC, TPE). The objective function of black-box\noptimization is non-smooth, or time-consuming to evaluate, or in some way\nnoisy. Recent years, many researchers offered the work about the properties of\nhyperparameters. However, traditional hyperparameter optimization methods do\nnot take those information into consideration. In this paper, we use gradient\ninformation and machine learning model analysis information to accelerate\ntraditional hyperparameter optimization methods SMBO. In our L2 norm\nexperiments, our method yielded state-of-the-art performance, and in many cases\noutperformed the previous best configuration approach."
  },
  {
    "id": "arxiv-336",
    "title": "PyLightcurve-torch: a transit modelling package for deep learning\n  applications in PyTorch",
    "abstract": "We present a new open source python package, based on PyLightcurve and\nPyTorch, tailored for efficient computation and automatic differentiation of\nexoplanetary transits. The classes and functions implemented are fully\nvectorised, natively GPU-compatible and differentiable with respect to the\nstellar and planetary parameters. This makes PyLightcurve-torch suitable for\ntraditional forward computation of transits, but also extends the range of\npossible applications with inference and optimisation algorithms requiring\naccess to the gradients of the physical model. This endeavour is aimed at\nfostering the use of deep learning in exoplanets research, motivated by an ever\nincreasing amount of stellar light curves data and various incentives for the\nimprovement of detection and characterisation techniques.",
    "text": "PyLightcurve-torch: a transit modelling package for deep learning\n  applications in PyTorch\n\nWe present a new open source python package, based on PyLightcurve and\nPyTorch, tailored for efficient computation and automatic differentiation of\nexoplanetary transits. The classes and functions implemented are fully\nvectorised, natively GPU-compatible and differentiable with respect to the\nstellar and planetary parameters. This makes PyLightcurve-torch suitable for\ntraditional forward computation of transits, but also extends the range of\npossible applications with inference and optimisation algorithms requiring\naccess to the gradients of the physical model. This endeavour is aimed at\nfostering the use of deep learning in exoplanets research, motivated by an ever\nincreasing amount of stellar light curves data and various incentives for the\nimprovement of detection and characterisation techniques."
  },
  {
    "id": "arxiv-337",
    "title": "Artificial Neural Network Modeling for Airline Disruption Management",
    "abstract": "Since the 1970s, most airlines have incorporated computerized support for\nmanaging disruptions during flight schedule execution. However, existing\nplatforms for airline disruption management (ADM) employ monolithic system\ndesign methods that rely on the creation of specific rules and requirements\nthrough explicit optimization routines, before a system that meets the\nspecifications is designed. Thus, current platforms for ADM are unable to\nreadily accommodate additional system complexities resulting from the\nintroduction of new capabilities, such as the introduction of unmanned aerial\nsystems (UAS), operations and infrastructure, to the system. To this end, we\nuse historical data on airline scheduling and operations recovery to develop a\nsystem of artificial neural networks (ANNs), which describe a predictive\ntransfer function model (PTFM) for promptly estimating the recovery impact of\ndisruption resolutions at separate phases of flight schedule execution during\nADM. Furthermore, we provide a modular approach for assessing and executing the\nPTFM by employing a parallel ensemble method to develop generative routines\nthat amalgamate the system of ANNs. Our modular approach ensures that current\nindustry standards for tardiness in flight schedule execution during ADM are\nsatisfied, while accurately estimating appropriate time-based performance\nmetrics for the separate phases of flight schedule execution.",
    "text": "Artificial Neural Network Modeling for Airline Disruption Management\n\nSince the 1970s, most airlines have incorporated computerized support for\nmanaging disruptions during flight schedule execution. However, existing\nplatforms for airline disruption management (ADM) employ monolithic system\ndesign methods that rely on the creation of specific rules and requirements\nthrough explicit optimization routines, before a system that meets the\nspecifications is designed. Thus, current platforms for ADM are unable to\nreadily accommodate additional system complexities resulting from the\nintroduction of new capabilities, such as the introduction of unmanned aerial\nsystems (UAS), operations and infrastructure, to the system. To this end, we\nuse historical data on airline scheduling and operations recovery to develop a\nsystem of artificial neural networks (ANNs), which describe a predictive\ntransfer function model (PTFM) for promptly estimating the recovery impact of\ndisruption resolutions at separate phases of flight schedule execution during\nADM. Furthermore, we provide a modular approach for assessing and executing the\nPTFM by employing a parallel ensemble method to develop generative routines\nthat amalgamate the system of ANNs. Our modular approach ensures that current\nindustry standards for tardiness in flight schedule execution during ADM are\nsatisfied, while accurately estimating appropriate time-based performance\nmetrics for the separate phases of flight schedule execution."
  },
  {
    "id": "arxiv-338",
    "title": "Disparities in Social Determinants among Performances of Mortality\n  Prediction with Machine Learning for Sepsis Patients",
    "abstract": "Background Sepsis is one of the most life-threatening circumstances for\ncritically ill patients in the US, while a standardized criteria for sepsis\nidentification is still under development. Disparities in social determinants\nof sepsis patients can interfere with the risk prediction performances using\nmachine learning. Methods Disparities in social determinants, including race,\ngender, marital status, insurance types and languages, among patients\nidentified by six available sepsis criteria were revealed by forest plots.\nSixteen machine learning classifiers were trained to predict in-hospital\nmortality for sepsis patients. The performance of the trained model was tested\non the entire randomly conducted test set and each sub-population built based\non each of the following social determinants: race, gender, marital status,\ninsurance type, and language. Results We analyzed a total of 11,791 critical\ncare patients from the MIMIC-III database. Within the population identified by\neach sepsis identification method, significant differences were observed among\nsub-populations regarding race, marital status, insurance type, and language.\nOn the 5,783 sepsis patients identified by the Sepsis-3 criteria statistically\nsignificant performance decreases for mortality prediction were observed when\napplying the trained machine learning model on Asian and Hispanic patients.\nWith pairwise comparison, we detected performance discrepancies in mortality\nprediction between Asian and White patients, Asians and patients of other\nraces, as well as English-speaking and Spanish-speaking patients. Conclusions\nDisparities in proportions of patients identified by various sepsis criteria\nwere detected among the different social determinant groups. To achieve\naccurate diagnosis, a versatile diagnostic system for sepsis is needed to\novercome the social determinant disparities of patients.",
    "text": "Disparities in Social Determinants among Performances of Mortality\n  Prediction with Machine Learning for Sepsis Patients\n\nBackground Sepsis is one of the most life-threatening circumstances for\ncritically ill patients in the US, while a standardized criteria for sepsis\nidentification is still under development. Disparities in social determinants\nof sepsis patients can interfere with the risk prediction performances using\nmachine learning. Methods Disparities in social determinants, including race,\ngender, marital status, insurance types and languages, among patients\nidentified by six available sepsis criteria were revealed by forest plots.\nSixteen machine learning classifiers were trained to predict in-hospital\nmortality for sepsis patients. The performance of the trained model was tested\non the entire randomly conducted test set and each sub-population built based\non each of the following social determinants: race, gender, marital status,\ninsurance type, and language. Results We analyzed a total of 11,791 critical\ncare patients from the MIMIC-III database. Within the population identified by\neach sepsis identification method, significant differences were observed among\nsub-populations regarding race, marital status, insurance type, and language.\nOn the 5,783 sepsis patients identified by the Sepsis-3 criteria statistically\nsignificant performance decreases for mortality prediction were observed when\napplying the trained machine learning model on Asian and Hispanic patients.\nWith pairwise comparison, we detected performance discrepancies in mortality\nprediction between Asian and White patients, Asians and patients of other\nraces, as well as English-speaking and Spanish-speaking patients. Conclusions\nDisparities in proportions of patients identified by various sepsis criteria\nwere detected among the different social determinant groups. To achieve\naccurate diagnosis, a versatile diagnostic system for sepsis is needed to\novercome the social determinant disparities of patients."
  },
  {
    "id": "arxiv-339",
    "title": "LINDA: Unsupervised Learning to Interpolate in Natural Language\n  Processing",
    "abstract": "Despite the success of mixup in data augmentation, its applicability to\nnatural language processing (NLP) tasks has been limited due to the discrete\nand variable-length nature of natural languages. Recent studies have thus\nrelied on domain-specific heuristics and manually crafted resources, such as\ndictionaries, in order to apply mixup in NLP. In this paper, we instead propose\nan unsupervised learning approach to text interpolation for the purpose of data\naugmentation, to which we refer as \"Learning to INterpolate for Data\nAugmentation\" (LINDA), that does not require any heuristics nor manually\ncrafted resources but learns to interpolate between any pair of natural\nlanguage sentences over a natural language manifold. After empirically\ndemonstrating the LINDA's interpolation capability, we show that LINDA indeed\nallows us to seamlessly apply mixup in NLP and leads to better generalization\nin text classification both in-domain and out-of-domain.",
    "text": "LINDA: Unsupervised Learning to Interpolate in Natural Language\n  Processing\n\nDespite the success of mixup in data augmentation, its applicability to\nnatural language processing (NLP) tasks has been limited due to the discrete\nand variable-length nature of natural languages. Recent studies have thus\nrelied on domain-specific heuristics and manually crafted resources, such as\ndictionaries, in order to apply mixup in NLP. In this paper, we instead propose\nan unsupervised learning approach to text interpolation for the purpose of data\naugmentation, to which we refer as \"Learning to INterpolate for Data\nAugmentation\" (LINDA), that does not require any heuristics nor manually\ncrafted resources but learns to interpolate between any pair of natural\nlanguage sentences over a natural language manifold. After empirically\ndemonstrating the LINDA's interpolation capability, we show that LINDA indeed\nallows us to seamlessly apply mixup in NLP and leads to better generalization\nin text classification both in-domain and out-of-domain."
  },
  {
    "id": "arxiv-340",
    "title": "Continual Multi-task Gaussian Processes",
    "abstract": "We address the problem of continual learning in multi-task Gaussian process\n(GP) models for handling sequential input-output observations. Our approach\nextends the existing prior-posterior recursion of online Bayesian inference,\ni.e.\\ past posterior discoveries become future prior beliefs, to the infinite\nfunctional space setting of GP. For a reason of scalability, we introduce\nvariational inference together with an sparse approximation based on inducing\ninputs. As a consequence, we obtain tractable continual lower-bounds where two\nnovel Kullback-Leibler (KL) divergences intervene in a natural way. The key\ntechnical property of our method is the recursive reconstruction of conditional\nGP priors conditioned on the variational parameters learned so far. To achieve\nthis goal, we introduce a novel factorization of past variational\ndistributions, where the predictive GP equation propagates the posterior\nuncertainty forward. We then demonstrate that it is possible to derive GP\nmodels over many types of sequential observations, either discrete or\ncontinuous and amenable to stochastic optimization. The continual inference\napproach is also applicable to scenarios where potential multi-channel or\nheterogeneous observations might appear. Extensive experiments demonstrate that\nthe method is fully scalable, shows a reliable performance and is robust to\nuncertainty error propagation over a plenty of synthetic and real-world\ndatasets.",
    "text": "Continual Multi-task Gaussian Processes\n\nWe address the problem of continual learning in multi-task Gaussian process\n(GP) models for handling sequential input-output observations. Our approach\nextends the existing prior-posterior recursion of online Bayesian inference,\ni.e.\\ past posterior discoveries become future prior beliefs, to the infinite\nfunctional space setting of GP. For a reason of scalability, we introduce\nvariational inference together with an sparse approximation based on inducing\ninputs. As a consequence, we obtain tractable continual lower-bounds where two\nnovel Kullback-Leibler (KL) divergences intervene in a natural way. The key\ntechnical property of our method is the recursive reconstruction of conditional\nGP priors conditioned on the variational parameters learned so far. To achieve\nthis goal, we introduce a novel factorization of past variational\ndistributions, where the predictive GP equation propagates the posterior\nuncertainty forward. We then demonstrate that it is possible to derive GP\nmodels over many types of sequential observations, either discrete or\ncontinuous and amenable to stochastic optimization. The continual inference\napproach is also applicable to scenarios where potential multi-channel or\nheterogeneous observations might appear. Extensive experiments demonstrate that\nthe method is fully scalable, shows a reliable performance and is robust to\nuncertainty error propagation over a plenty of synthetic and real-world\ndatasets."
  },
  {
    "id": "arxiv-341",
    "title": "Agricultural Plantation Classification using Transfer Learning Approach based on CNN",
    "abstract": "Hyper-spectral images are images captured from a satellite that gives spatial\nand spectral information of specific region.A Hyper-spectral image contains\nmuch more number of channels as compared to a RGB image, hence containing more\ninformation about entities within the image. It makes them well suited for the\nclassification of objects in a snap. In the past years, the efficiency of\nhyper-spectral image recognition has increased significantly with deep\nlearning. The Convolution Neural Network(CNN) and Multi-Layer Perceptron(MLP)\nhas demonstrated to be an excellent process of classifying images. However,\nthey suffer from the issues of long training time and requirement of large\namounts of the labeled data, to achieve the expected outcome. These issues\nbecome more complex while dealing with hyper-spectral images. To decrease the\ntraining time and reduce the dependence on large labeled data-set, we propose\nusing the method of transfer learning.The features learned by CNN and MLP\nmodels are then used by the transfer learning model to solve a new\nclassification problem on an unseen dataset. A detailed comparison of CNN and\nmultiple MLP architectural models is performed, to determine an optimum\narchitecture that suits best the objective. The results show that the scaling\nof layers not always leads to increase in accuracy but often leads to\nover-fitting, and also an increase in the training time.The training time is\nreduced to greater extent by applying the transfer learning approach rather\nthan just approaching the problem by directly training a new model on large\ndata-sets, without much affecting the accuracy.",
    "text": "Agricultural Plantation Classification using Transfer Learning Approach based on CNN\n\nHyper-spectral images are images captured from a satellite that gives spatial\nand spectral information of specific region.A Hyper-spectral image contains\nmuch more number of channels as compared to a RGB image, hence containing more\ninformation about entities within the image. It makes them well suited for the\nclassification of objects in a snap. In the past years, the efficiency of\nhyper-spectral image recognition has increased significantly with deep\nlearning. The Convolution Neural Network(CNN) and Multi-Layer Perceptron(MLP)\nhas demonstrated to be an excellent process of classifying images. However,\nthey suffer from the issues of long training time and requirement of large\namounts of the labeled data, to achieve the expected outcome. These issues\nbecome more complex while dealing with hyper-spectral images. To decrease the\ntraining time and reduce the dependence on large labeled data-set, we propose\nusing the method of transfer learning.The features learned by CNN and MLP\nmodels are then used by the transfer learning model to solve a new\nclassification problem on an unseen dataset. A detailed comparison of CNN and\nmultiple MLP architectural models is performed, to determine an optimum\narchitecture that suits best the objective. The results show that the scaling\nof layers not always leads to increase in accuracy but often leads to\nover-fitting, and also an increase in the training time.The training time is\nreduced to greater extent by applying the transfer learning approach rather\nthan just approaching the problem by directly training a new model on large\ndata-sets, without much affecting the accuracy."
  },
  {
    "id": "arxiv-342",
    "title": "Unsupervised learning for cross-domain medical image synthesis using\n  deformation invariant cycle consistency networks",
    "abstract": "Recently, the cycle-consistent generative adversarial networks (CycleGAN) has\nbeen widely used for synthesis of multi-domain medical images. The\ndomain-specific nonlinear deformations captured by CycleGAN make the\nsynthesized images difficult to be used for some applications, for example,\ngenerating pseudo-CT for PET-MR attenuation correction. This paper presents a\ndeformation-invariant CycleGAN (DicycleGAN) method using deformable\nconvolutional layers and new cycle-consistency losses. Its robustness dealing\nwith data that suffer from domain-specific nonlinear deformations has been\nevaluated through comparison experiments performed on a multi-sequence brain MR\ndataset and a multi-modality abdominal dataset. Our method has displayed its\nability to generate synthesized data that is aligned with the source while\nmaintaining a proper quality of signal compared to CycleGAN-generated data. The\nproposed model also obtained comparable performance with CycleGAN when data\nfrom the source and target domains are alignable through simple affine\ntransformations.",
    "text": "Unsupervised learning for cross-domain medical image synthesis using\n  deformation invariant cycle consistency networks\n\nRecently, the cycle-consistent generative adversarial networks (CycleGAN) has\nbeen widely used for synthesis of multi-domain medical images. The\ndomain-specific nonlinear deformations captured by CycleGAN make the\nsynthesized images difficult to be used for some applications, for example,\ngenerating pseudo-CT for PET-MR attenuation correction. This paper presents a\ndeformation-invariant CycleGAN (DicycleGAN) method using deformable\nconvolutional layers and new cycle-consistency losses. Its robustness dealing\nwith data that suffer from domain-specific nonlinear deformations has been\nevaluated through comparison experiments performed on a multi-sequence brain MR\ndataset and a multi-modality abdominal dataset. Our method has displayed its\nability to generate synthesized data that is aligned with the source while\nmaintaining a proper quality of signal compared to CycleGAN-generated data. The\nproposed model also obtained comparable performance with CycleGAN when data\nfrom the source and target domains are alignable through simple affine\ntransformations."
  },
  {
    "id": "arxiv-343",
    "title": "Data Driven Exploratory Attacks on Black Box Classifiers in Adversarial\n  Domains",
    "abstract": "While modern day web applications aim to create impact at the civilization\nlevel, they have become vulnerable to adversarial activity, where the next\ncyber-attack can take any shape and can originate from anywhere. The increasing\nscale and sophistication of attacks, has prompted the need for a data driven\nsolution, with machine learning forming the core of many cybersecurity systems.\nMachine learning was not designed with security in mind, and the essential\nassumption of stationarity, requiring that the training and testing data follow\nsimilar distributions, is violated in an adversarial domain. In this paper, an\nadversary's view point of a classification based system, is presented. Based on\na formal adversarial model, the Seed-Explore-Exploit framework is presented,\nfor simulating the generation of data driven and reverse engineering attacks on\nclassifiers. Experimental evaluation, on 10 real world datasets and using the\nGoogle Cloud Prediction Platform, demonstrates the innate vulnerability of\nclassifiers and the ease with which evasion can be carried out, without any\nexplicit information about the classifier type, the training data or the\napplication domain. The proposed framework, algorithms and empirical\nevaluation, serve as a white hat analysis of the vulnerabilities, and aim to\nfoster the development of secure machine learning frameworks.",
    "text": "Data Driven Exploratory Attacks on Black Box Classifiers in Adversarial\n  Domains\n\nWhile modern day web applications aim to create impact at the civilization\nlevel, they have become vulnerable to adversarial activity, where the next\ncyber-attack can take any shape and can originate from anywhere. The increasing\nscale and sophistication of attacks, has prompted the need for a data driven\nsolution, with machine learning forming the core of many cybersecurity systems.\nMachine learning was not designed with security in mind, and the essential\nassumption of stationarity, requiring that the training and testing data follow\nsimilar distributions, is violated in an adversarial domain. In this paper, an\nadversary's view point of a classification based system, is presented. Based on\na formal adversarial model, the Seed-Explore-Exploit framework is presented,\nfor simulating the generation of data driven and reverse engineering attacks on\nclassifiers. Experimental evaluation, on 10 real world datasets and using the\nGoogle Cloud Prediction Platform, demonstrates the innate vulnerability of\nclassifiers and the ease with which evasion can be carried out, without any\nexplicit information about the classifier type, the training data or the\napplication domain. The proposed framework, algorithms and empirical\nevaluation, serve as a white hat analysis of the vulnerabilities, and aim to\nfoster the development of secure machine learning frameworks."
  },
  {
    "id": "arxiv-344",
    "title": "Multi-view Contrastive Learning for Online Knowledge Distillation",
    "abstract": "Previous Online Knowledge Distillation (OKD) often carries out mutually\nexchanging probability distributions, but neglects the useful representational\nknowledge. We therefore propose Multi-view Contrastive Learning (MCL) for OKD\nto implicitly capture correlations of feature embeddings encoded by multiple\npeer networks, which provide various views for understanding the input data\ninstances. Benefiting from MCL, we can learn a more discriminative\nrepresentation space for classification than previous OKD methods. Experimental\nresults on image classification demonstrate that our MCL-OKD outperforms other\nstate-of-the-art OKD methods by large margins without sacrificing additional\ninference cost. Codes are available at https://github.com/winycg/MCL-OKD.",
    "text": "Multi-view Contrastive Learning for Online Knowledge Distillation\n\nPrevious Online Knowledge Distillation (OKD) often carries out mutually\nexchanging probability distributions, but neglects the useful representational\nknowledge. We therefore propose Multi-view Contrastive Learning (MCL) for OKD\nto implicitly capture correlations of feature embeddings encoded by multiple\npeer networks, which provide various views for understanding the input data\ninstances. Benefiting from MCL, we can learn a more discriminative\nrepresentation space for classification than previous OKD methods. Experimental\nresults on image classification demonstrate that our MCL-OKD outperforms other\nstate-of-the-art OKD methods by large margins without sacrificing additional\ninference cost. Codes are available at https://github.com/winycg/MCL-OKD."
  },
  {
    "id": "arxiv-345",
    "title": "Benchmarking Active Learning Strategies for Materials Optimization and\n  Discovery",
    "abstract": "Autonomous physical science is revolutionizing materials science. In these\nsystems, machine learning controls experiment design, execution, and analysis\nin a closed loop. Active learning, the machine learning field of optimal\nexperiment design, selects each subsequent experiment to maximize knowledge\ntoward the user goal. Autonomous system performance can be further improved\nwith implementation of scientific machine learning, also known as inductive\nbias-engineered artificial intelligence, which folds prior knowledge of\nphysical laws (e.g., Gibbs phase rule) into the algorithm. As the number,\ndiversity, and uses for active learning strategies grow, there is an associated\ngrowing necessity for real-world reference datasets to benchmark strategies. We\npresent a reference dataset and demonstrate its use to benchmark active\nlearning strategies in the form of various acquisition functions. Active\nlearning strategies are used to rapidly identify materials with optimal\nphysical properties within a ternary materials system. The data is from an\nactual Fe-Co-Ni thin-film library and includes previously acquired experimental\ndata for materials compositions, X-ray diffraction patterns, and two functional\nproperties of magnetic coercivity and the Kerr rotation. Popular active\nlearning methods along with a recent scientific active learning method are\nbenchmarked for their materials optimization performance. We discuss the\nrelationship between algorithm performance, materials search space complexity,\nand the incorporation of prior knowledge.",
    "text": "Benchmarking Active Learning Strategies for Materials Optimization and\n  Discovery\n\nAutonomous physical science is revolutionizing materials science. In these\nsystems, machine learning controls experiment design, execution, and analysis\nin a closed loop. Active learning, the machine learning field of optimal\nexperiment design, selects each subsequent experiment to maximize knowledge\ntoward the user goal. Autonomous system performance can be further improved\nwith implementation of scientific machine learning, also known as inductive\nbias-engineered artificial intelligence, which folds prior knowledge of\nphysical laws (e.g., Gibbs phase rule) into the algorithm. As the number,\ndiversity, and uses for active learning strategies grow, there is an associated\ngrowing necessity for real-world reference datasets to benchmark strategies. We\npresent a reference dataset and demonstrate its use to benchmark active\nlearning strategies in the form of various acquisition functions. Active\nlearning strategies are used to rapidly identify materials with optimal\nphysical properties within a ternary materials system. The data is from an\nactual Fe-Co-Ni thin-film library and includes previously acquired experimental\ndata for materials compositions, X-ray diffraction patterns, and two functional\nproperties of magnetic coercivity and the Kerr rotation. Popular active\nlearning methods along with a recent scientific active learning method are\nbenchmarked for their materials optimization performance. We discuss the\nrelationship between algorithm performance, materials search space complexity,\nand the incorporation of prior knowledge."
  },
  {
    "id": "arxiv-346",
    "title": "Smart Data Representations: Impact on the Accuracy of Deep Neural\n  Networks",
    "abstract": "Deep Neural Networks are able to solve many complex tasks with less\nengineering effort and better performance. However, these networks often use\ndata for training and evaluation without investigating its representation,\ni.e.~the form of the used data. In the present paper, we analyze the impact of\ndata representations on the performance of Deep Neural Networks using energy\ntime series forecasting. Based on an overview of exemplary data\nrepresentations, we select four exemplary data representations and evaluate\nthem using two different Deep Neural Network architectures and three\nforecasting horizons on real-world energy time series. The results show that,\ndepending on the forecast horizon, the same data representations can have a\npositive or negative impact on the accuracy of Deep Neural Networks.",
    "text": "Smart Data Representations: Impact on the Accuracy of Deep Neural\n  Networks\n\nDeep Neural Networks are able to solve many complex tasks with less\nengineering effort and better performance. However, these networks often use\ndata for training and evaluation without investigating its representation,\ni.e.~the form of the used data. In the present paper, we analyze the impact of\ndata representations on the performance of Deep Neural Networks using energy\ntime series forecasting. Based on an overview of exemplary data\nrepresentations, we select four exemplary data representations and evaluate\nthem using two different Deep Neural Network architectures and three\nforecasting horizons on real-world energy time series. The results show that,\ndepending on the forecast horizon, the same data representations can have a\npositive or negative impact on the accuracy of Deep Neural Networks."
  },
  {
    "id": "arxiv-347",
    "title": "On the Suitable Domain for SVM Training in Image Coding",
    "abstract": "Conventional SVM-based image coding methods are founded on independently\nrestricting the distortion in every image coefficient at some particular image\nrepresentation. Geometrically, this implies allowing arbitrary signal\ndistortions in an $n$-dimensional rectangle defined by the\n$\\varepsilon$-insensitivity zone in each dimension of the selected image\nrepresentation domain. Unfortunately, not every image representation domain is\nwell-suited for such a simple, scalar-wise, approach because statistical and/or\nperceptual interactions between the coefficients may exist. These interactions\nimply that scalar approaches may induce distortions that do not follow the\nimage statistics and/or are perceptually annoying. Taking into account these\nrelations would imply using non-rectangular $\\varepsilon$-insensitivity regions\n(allowing coupled distortions in different coefficients), which is beyond the\nconventional SVM formulation.\n  In this paper, we report a condition on the suitable domain for developing\nefficient SVM image coding schemes. We analytically demonstrate that no linear\ndomain fulfills this condition because of the statistical and perceptual\ninter-coefficient relations that exist in these domains. This theoretical\nresult is experimentally confirmed by comparing SVM learning in previously\nreported linear domains and in a recently proposed non-linear perceptual domain\nthat simultaneously reduces the statistical and perceptual relations (so it is\ncloser to fulfilling the proposed condition). These results highlight the\nrelevance of an appropriate choice of the image representation before SVM\nlearning.",
    "text": "On the Suitable Domain for SVM Training in Image Coding\n\nConventional SVM-based image coding methods are founded on independently\nrestricting the distortion in every image coefficient at some particular image\nrepresentation. Geometrically, this implies allowing arbitrary signal\ndistortions in an $n$-dimensional rectangle defined by the\n$\\varepsilon$-insensitivity zone in each dimension of the selected image\nrepresentation domain. Unfortunately, not every image representation domain is\nwell-suited for such a simple, scalar-wise, approach because statistical and/or\nperceptual interactions between the coefficients may exist. These interactions\nimply that scalar approaches may induce distortions that do not follow the\nimage statistics and/or are perceptually annoying. Taking into account these\nrelations would imply using non-rectangular $\\varepsilon$-insensitivity regions\n(allowing coupled distortions in different coefficients), which is beyond the\nconventional SVM formulation.\n  In this paper, we report a condition on the suitable domain for developing\nefficient SVM image coding schemes. We analytically demonstrate that no linear\ndomain fulfills this condition because of the statistical and perceptual\ninter-coefficient relations that exist in these domains. This theoretical\nresult is experimentally confirmed by comparing SVM learning in previously\nreported linear domains and in a recently proposed non-linear perceptual domain\nthat simultaneously reduces the statistical and perceptual relations (so it is\ncloser to fulfilling the proposed condition). These results highlight the\nrelevance of an appropriate choice of the image representation before SVM\nlearning."
  },
  {
    "id": "arxiv-348",
    "title": "Metaheuristics optimized feedforward neural networks for efficient stock\n  price prediction",
    "abstract": "The prediction of stock prices is an important task in economics, investment\nand making financial decisions. This has, for decades, spurred the interest of\nmany researchers to make focused contributions to the design of accurate stock\nprice predictive models; of which some have been utilized to predict the next\nday opening and closing prices of the stock indices. This paper proposes the\ndesign and implementation of a hybrid symbiotic organisms search trained\nfeedforward neural network model for effective and accurate stock price\nprediction. The symbiotic organisms search algorithm is used as an efficient\noptimization technique to train the feedforward neural networks, while the\nresulting training process is used to build a better stock price prediction\nmodel. Furthermore, the study also presents a comparative performance\nevaluation of three different stock price forecasting models; namely, the\nparticle swarm optimization trained feedforward neural network model, the\ngenetic algorithm trained feedforward neural network model and the well-known\nARIMA model. The system developed in support of this study utilizes sixteen\nstock indices as time series datasets for training and testing purpose. Three\nstatistical evaluation measures are used to compare the results of the\nimplemented models, namely the root mean squared error, the mean absolute\npercentage error and the mean absolution deviation. The computational results\nobtained reveal that the symbiotic organisms search trained feedforward neural\nnetwork model exhibits outstanding predictive performance compared to the other\nmodels. However, the performance study shows that the three metaheuristics\ntrained feedforward neural network models have promising predictive competence\nfor solving problems of high dimensional nonlinear time series data, which are\ndifficult to capture by traditional models.",
    "text": "Metaheuristics optimized feedforward neural networks for efficient stock\n  price prediction\n\nThe prediction of stock prices is an important task in economics, investment\nand making financial decisions. This has, for decades, spurred the interest of\nmany researchers to make focused contributions to the design of accurate stock\nprice predictive models; of which some have been utilized to predict the next\nday opening and closing prices of the stock indices. This paper proposes the\ndesign and implementation of a hybrid symbiotic organisms search trained\nfeedforward neural network model for effective and accurate stock price\nprediction. The symbiotic organisms search algorithm is used as an efficient\noptimization technique to train the feedforward neural networks, while the\nresulting training process is used to build a better stock price prediction\nmodel. Furthermore, the study also presents a comparative performance\nevaluation of three different stock price forecasting models; namely, the\nparticle swarm optimization trained feedforward neural network model, the\ngenetic algorithm trained feedforward neural network model and the well-known\nARIMA model. The system developed in support of this study utilizes sixteen\nstock indices as time series datasets for training and testing purpose. Three\nstatistical evaluation measures are used to compare the results of the\nimplemented models, namely the root mean squared error, the mean absolute\npercentage error and the mean absolution deviation. The computational results\nobtained reveal that the symbiotic organisms search trained feedforward neural\nnetwork model exhibits outstanding predictive performance compared to the other\nmodels. However, the performance study shows that the three metaheuristics\ntrained feedforward neural network models have promising predictive competence\nfor solving problems of high dimensional nonlinear time series data, which are\ndifficult to capture by traditional models."
  },
  {
    "id": "arxiv-349",
    "title": "Theoretical Analysis of Sparse Subspace Clustering with Missing Entries",
    "abstract": "Sparse Subspace Clustering (SSC) is a popular unsupervised machine learning\nmethod for clustering data lying close to an unknown union of low-dimensional\nlinear subspaces; a problem with numerous applications in pattern recognition\nand computer vision. Even though the behavior of SSC for complete data is by\nnow well-understood, little is known about its theoretical properties when\napplied to data with missing entries. In this paper we give theoretical\nguarantees for SSC with incomplete data, and analytically establish that\nprojecting the zero-filled data onto the observation pattern of the point being\nexpressed leads to a substantial improvement in performance. The main insight\nthat stems from our analysis is that even though the projection induces\nadditional missing entries, this is counterbalanced by the fact that the\nprojected and zero-filled data are in effect incomplete points associated with\nthe union of the corresponding projected subspaces, with respect to which the\npoint being expressed is complete. The significance of this phenomenon\npotentially extends to the entire class of self-expressive methods.",
    "text": "Theoretical Analysis of Sparse Subspace Clustering with Missing Entries\n\nSparse Subspace Clustering (SSC) is a popular unsupervised machine learning\nmethod for clustering data lying close to an unknown union of low-dimensional\nlinear subspaces; a problem with numerous applications in pattern recognition\nand computer vision. Even though the behavior of SSC for complete data is by\nnow well-understood, little is known about its theoretical properties when\napplied to data with missing entries. In this paper we give theoretical\nguarantees for SSC with incomplete data, and analytically establish that\nprojecting the zero-filled data onto the observation pattern of the point being\nexpressed leads to a substantial improvement in performance. The main insight\nthat stems from our analysis is that even though the projection induces\nadditional missing entries, this is counterbalanced by the fact that the\nprojected and zero-filled data are in effect incomplete points associated with\nthe union of the corresponding projected subspaces, with respect to which the\npoint being expressed is complete. The significance of this phenomenon\npotentially extends to the entire class of self-expressive methods."
  },
  {
    "id": "arxiv-350",
    "title": "Bridging Differential Privacy and Byzantine-Robustness via Model\n  Aggregation",
    "abstract": "This paper aims at jointly addressing two seemly conflicting issues in\nfederated learning: differential privacy (DP) and Byzantine-robustness, which\nare particularly challenging when the distributed data are non-i.i.d.\n(independent and identically distributed). The standard DP mechanisms add noise\nto the transmitted messages, and entangles with robust stochastic gradient\naggregation to defend against Byzantine attacks. In this paper, we decouple the\ntwo issues via robust stochastic model aggregation, in the sense that our\nproposed DP mechanisms and the defense against Byzantine attacks have separated\ninfluence on the learning performance. Leveraging robust stochastic model\naggregation, at each iteration, each worker calculates the difference between\nthe local model and the global one, followed by sending the element-wise signs\nto the master node, which enables robustness to Byzantine attacks. Further, we\ndesign two DP mechanisms to perturb the uploaded signs for the purpose of\nprivacy preservation, and prove that they are $(\\epsilon,0)$-DP by exploiting\nthe properties of noise distributions. With the tools of Moreau envelop and\nproximal point projection, we establish the convergence of the proposed\nalgorithm when the cost function is nonconvex. We analyze the trade-off between\nprivacy preservation and learning performance, and show that the influence of\nour proposed DP mechanisms is decoupled with that of robust stochastic model\naggregation. Numerical experiments demonstrate the effectiveness of the\nproposed algorithm.",
    "text": "Bridging Differential Privacy and Byzantine-Robustness via Model\n  Aggregation\n\nThis paper aims at jointly addressing two seemly conflicting issues in\nfederated learning: differential privacy (DP) and Byzantine-robustness, which\nare particularly challenging when the distributed data are non-i.i.d.\n(independent and identically distributed). The standard DP mechanisms add noise\nto the transmitted messages, and entangles with robust stochastic gradient\naggregation to defend against Byzantine attacks. In this paper, we decouple the\ntwo issues via robust stochastic model aggregation, in the sense that our\nproposed DP mechanisms and the defense against Byzantine attacks have separated\ninfluence on the learning performance. Leveraging robust stochastic model\naggregation, at each iteration, each worker calculates the difference between\nthe local model and the global one, followed by sending the element-wise signs\nto the master node, which enables robustness to Byzantine attacks. Further, we\ndesign two DP mechanisms to perturb the uploaded signs for the purpose of\nprivacy preservation, and prove that they are $(\\epsilon,0)$-DP by exploiting\nthe properties of noise distributions. With the tools of Moreau envelop and\nproximal point projection, we establish the convergence of the proposed\nalgorithm when the cost function is nonconvex. We analyze the trade-off between\nprivacy preservation and learning performance, and show that the influence of\nour proposed DP mechanisms is decoupled with that of robust stochastic model\naggregation. Numerical experiments demonstrate the effectiveness of the\nproposed algorithm."
  },
  {
    "id": "arxiv-351",
    "title": "AFP-SRC: Identification of Antifreeze Proteins Using Sparse\n  Representation Classifier",
    "abstract": "Species living in the extreme cold environment fight against the harsh\nconditions using antifreeze proteins (AFPs), that manipulates the freezing\nmechanism of water in more than one way. This amazing nature of AFP turns out\nto be extremely useful in several industrial and medical applications. The lack\nof similarity in their structure and sequence makes their prediction an arduous\ntask and identifying them experimentally in the wet-lab is time-consuming and\nexpensive. In this research, we propose a computational framework for the\nprediction of AFPs which is essentially based on a sample-specific\nclassification method using the sparse reconstruction. A linear model and an\nover-complete dictionary matrix of known AFPs are used to predict a sparse\nclass-label vector that provides a sample-association score. Delta-rule is\napplied for the reconstruction of two pseudo-samples using lower and upper\nparts of the sample-association vector and based on the minimum recovery score,\nclass labels are assigned. We compare our approach with contemporary methods on\na standard dataset and the proposed method is found to outperform in terms of\nBalanced accuracy and Youden's index. The MATLAB implementation of the proposed\nmethod is available at the author's GitHub page\n(\\{https://github.com/Shujaat123/AFP-SRC}{https://github.com/Shujaat123/AFP-SRC}).",
    "text": "AFP-SRC: Identification of Antifreeze Proteins Using Sparse\n  Representation Classifier\n\nSpecies living in the extreme cold environment fight against the harsh\nconditions using antifreeze proteins (AFPs), that manipulates the freezing\nmechanism of water in more than one way. This amazing nature of AFP turns out\nto be extremely useful in several industrial and medical applications. The lack\nof similarity in their structure and sequence makes their prediction an arduous\ntask and identifying them experimentally in the wet-lab is time-consuming and\nexpensive. In this research, we propose a computational framework for the\nprediction of AFPs which is essentially based on a sample-specific\nclassification method using the sparse reconstruction. A linear model and an\nover-complete dictionary matrix of known AFPs are used to predict a sparse\nclass-label vector that provides a sample-association score. Delta-rule is\napplied for the reconstruction of two pseudo-samples using lower and upper\nparts of the sample-association vector and based on the minimum recovery score,\nclass labels are assigned. We compare our approach with contemporary methods on\na standard dataset and the proposed method is found to outperform in terms of\nBalanced accuracy and Youden's index. The MATLAB implementation of the proposed\nmethod is available at the author's GitHub page\n(\\{https://github.com/Shujaat123/AFP-SRC}{https://github.com/Shujaat123/AFP-SRC})."
  },
  {
    "id": "arxiv-352",
    "title": "When Point Process Meets RNNs: Predicting Fine-Grained User Interests\n  with Mutual Behavioral Infectivity",
    "abstract": "Predicting fine-grained interests of users with temporal behavior is\nimportant to personalization and information filtering applications. However,\nexisting interest prediction methods are incapable of capturing the subtle\ndegreed user interests towards particular items, and the internal time-varying\ndrifting attention of individuals is not studied yet. Moreover, the prediction\nprocess can also be affected by inter-personal influence, known as behavioral\nmutual infectivity. Inspired by point process in modeling temporal point\nprocess, in this paper we present a deep prediction method based on two\nrecurrent neural networks (RNNs) to jointly model each user's continuous\nbrowsing history and asynchronous event sequences in the context of inter-user\nbehavioral mutual infectivity. Our model is able to predict the fine-grained\ninterest from a user regarding a particular item and corresponding timestamps\nwhen an occurrence of event takes place. The proposed approach is more flexible\nto capture the dynamic characteristic of event sequences by using the temporal\npoint process to model event data and timely update its intensity function by\nRNNs. Furthermore, to improve the interpretability of the model, the attention\nmechanism is introduced to emphasize both intra-personal and inter-personal\nbehavior influence over time. Experiments on real datasets demonstrate that our\nmodel outperforms the state-of-the-art methods in fine-grained user interest\nprediction.",
    "text": "When Point Process Meets RNNs: Predicting Fine-Grained User Interests\n  with Mutual Behavioral Infectivity\n\nPredicting fine-grained interests of users with temporal behavior is\nimportant to personalization and information filtering applications. However,\nexisting interest prediction methods are incapable of capturing the subtle\ndegreed user interests towards particular items, and the internal time-varying\ndrifting attention of individuals is not studied yet. Moreover, the prediction\nprocess can also be affected by inter-personal influence, known as behavioral\nmutual infectivity. Inspired by point process in modeling temporal point\nprocess, in this paper we present a deep prediction method based on two\nrecurrent neural networks (RNNs) to jointly model each user's continuous\nbrowsing history and asynchronous event sequences in the context of inter-user\nbehavioral mutual infectivity. Our model is able to predict the fine-grained\ninterest from a user regarding a particular item and corresponding timestamps\nwhen an occurrence of event takes place. The proposed approach is more flexible\nto capture the dynamic characteristic of event sequences by using the temporal\npoint process to model event data and timely update its intensity function by\nRNNs. Furthermore, to improve the interpretability of the model, the attention\nmechanism is introduced to emphasize both intra-personal and inter-personal\nbehavior influence over time. Experiments on real datasets demonstrate that our\nmodel outperforms the state-of-the-art methods in fine-grained user interest\nprediction."
  },
  {
    "id": "arxiv-353",
    "title": "Adversarial Machine Learning Threat Analysis in Open Radio Access\n  Networks",
    "abstract": "The Open Radio Access Network (O-RAN) is a new, open, adaptive, and\nintelligent RAN architecture. Motivated by the success of artificial\nintelligence in other domains, O-RAN strives to leverage machine learning (ML)\nto automatically and efficiently manage network resources in diverse use cases\nsuch as traffic steering, quality of experience prediction, and anomaly\ndetection. Unfortunately, ML-based systems are not free of vulnerabilities;\nspecifically, they suffer from a special type of logical vulnerabilities that\nstem from the inherent limitations of the learning algorithms. To exploit these\nvulnerabilities, an adversary can utilize an attack technique referred to as\nadversarial machine learning (AML). These special type of attacks has already\nbeen demonstrated in recent researches. In this paper, we present a systematic\nAML threat analysis for the O-RAN. We start by reviewing relevant ML use cases\nand analyzing the different ML workflow deployment scenarios in O-RAN. Then, we\ndefine the threat model, identifying potential adversaries, enumerating their\nadversarial capabilities, and analyzing their main goals. Finally, we explore\nthe various AML threats in the O-RAN and review a large number of attacks that\ncan be performed to materialize these threats and demonstrate an AML attack on\na traffic steering model.",
    "text": "Adversarial Machine Learning Threat Analysis in Open Radio Access\n  Networks\n\nThe Open Radio Access Network (O-RAN) is a new, open, adaptive, and\nintelligent RAN architecture. Motivated by the success of artificial\nintelligence in other domains, O-RAN strives to leverage machine learning (ML)\nto automatically and efficiently manage network resources in diverse use cases\nsuch as traffic steering, quality of experience prediction, and anomaly\ndetection. Unfortunately, ML-based systems are not free of vulnerabilities;\nspecifically, they suffer from a special type of logical vulnerabilities that\nstem from the inherent limitations of the learning algorithms. To exploit these\nvulnerabilities, an adversary can utilize an attack technique referred to as\nadversarial machine learning (AML). These special type of attacks has already\nbeen demonstrated in recent researches. In this paper, we present a systematic\nAML threat analysis for the O-RAN. We start by reviewing relevant ML use cases\nand analyzing the different ML workflow deployment scenarios in O-RAN. Then, we\ndefine the threat model, identifying potential adversaries, enumerating their\nadversarial capabilities, and analyzing their main goals. Finally, we explore\nthe various AML threats in the O-RAN and review a large number of attacks that\ncan be performed to materialize these threats and demonstrate an AML attack on\na traffic steering model."
  },
  {
    "id": "arxiv-354",
    "title": "Are We Ready For Learned Cardinality Estimation?",
    "abstract": "Cardinality estimation is a fundamental but long unresolved problem in query\noptimization. Recently, multiple papers from different research groups\nconsistently report that learned models have the potential to replace existing\ncardinality estimators. In this paper, we ask a forward-thinking question: Are\nwe ready to deploy these learned cardinality models in production? Our study\nconsists of three main parts. Firstly, we focus on the static environment\n(i.e., no data updates) and compare five new learned methods with eight\ntraditional methods on four real-world datasets under a unified workload\nsetting. The results show that learned models are indeed more accurate than\ntraditional methods, but they often suffer from high training and inference\ncosts. Secondly, we explore whether these learned models are ready for dynamic\nenvironments (i.e., frequent data updates). We find that they cannot catch up\nwith fast data up-dates and return large errors for different reasons. For less\nfrequent updates, they can perform better but there is no clear winner among\nthemselves. Thirdly, we take a deeper look into learned models and explore when\nthey may go wrong. Our results show that the performance of learned methods can\nbe greatly affected by the changes in correlation, skewness, or domain size.\nMore importantly, their behaviors are much harder to interpret and often\nunpredictable. Based on these findings, we identify two promising research\ndirections (control the cost of learned models and make learned models\ntrustworthy) and suggest a number of research opportunities. We hope that our\nstudy can guide researchers and practitioners to work together to eventually\npush learned cardinality estimators into real database systems.",
    "text": "Are We Ready For Learned Cardinality Estimation?\n\nCardinality estimation is a fundamental but long unresolved problem in query\noptimization. Recently, multiple papers from different research groups\nconsistently report that learned models have the potential to replace existing\ncardinality estimators. In this paper, we ask a forward-thinking question: Are\nwe ready to deploy these learned cardinality models in production? Our study\nconsists of three main parts. Firstly, we focus on the static environment\n(i.e., no data updates) and compare five new learned methods with eight\ntraditional methods on four real-world datasets under a unified workload\nsetting. The results show that learned models are indeed more accurate than\ntraditional methods, but they often suffer from high training and inference\ncosts. Secondly, we explore whether these learned models are ready for dynamic\nenvironments (i.e., frequent data updates). We find that they cannot catch up\nwith fast data up-dates and return large errors for different reasons. For less\nfrequent updates, they can perform better but there is no clear winner among\nthemselves. Thirdly, we take a deeper look into learned models and explore when\nthey may go wrong. Our results show that the performance of learned methods can\nbe greatly affected by the changes in correlation, skewness, or domain size.\nMore importantly, their behaviors are much harder to interpret and often\nunpredictable. Based on these findings, we identify two promising research\ndirections (control the cost of learned models and make learned models\ntrustworthy) and suggest a number of research opportunities. We hope that our\nstudy can guide researchers and practitioners to work together to eventually\npush learned cardinality estimators into real database systems."
  },
  {
    "id": "arxiv-355",
    "title": "Multifidelity Ensemble Kalman Filtering Using Surrogate Models Defined\n  by Physics-Informed Autoencoders",
    "abstract": "Data assimilation is a Bayesian inference process that obtains an enhanced\nunderstanding of a physical system of interest by fusing information from an\ninexact physics-based model, and from noisy sparse observations of reality. The\nmultifidelity ensemble Kalman filter (MFEnKF) recently developed by the authors\ncombines a full-order physical model and a hierarchy of reduced order surrogate\nmodels in order to increase the computational efficiency of data assimilation.\nThe standard MFEnKF uses linear couplings between models, and is statistically\noptimal in case of Gaussian probability densities. This work extends MFEnKF to\nwork with non-linear couplings between the models. Optimal nonlinear projection\nand interpolation operators are obtained by appropriately trained\nphysics-informed autoencoders, and this approach allows to construct reduced\norder surrogate models with less error than conventional linear methods.\nNumerical experiments with the canonical Lorenz '96 model illustrate that\nnonlinear surrogates perform better than linear projection-based ones in the\ncontext of multifidelity filtering.",
    "text": "Multifidelity Ensemble Kalman Filtering Using Surrogate Models Defined\n  by Physics-Informed Autoencoders\n\nData assimilation is a Bayesian inference process that obtains an enhanced\nunderstanding of a physical system of interest by fusing information from an\ninexact physics-based model, and from noisy sparse observations of reality. The\nmultifidelity ensemble Kalman filter (MFEnKF) recently developed by the authors\ncombines a full-order physical model and a hierarchy of reduced order surrogate\nmodels in order to increase the computational efficiency of data assimilation.\nThe standard MFEnKF uses linear couplings between models, and is statistically\noptimal in case of Gaussian probability densities. This work extends MFEnKF to\nwork with non-linear couplings between the models. Optimal nonlinear projection\nand interpolation operators are obtained by appropriately trained\nphysics-informed autoencoders, and this approach allows to construct reduced\norder surrogate models with less error than conventional linear methods.\nNumerical experiments with the canonical Lorenz '96 model illustrate that\nnonlinear surrogates perform better than linear projection-based ones in the\ncontext of multifidelity filtering."
  },
  {
    "id": "arxiv-356",
    "title": "Molecular Energy Learning Using Alternative Blackbox Matrix-Matrix\n  Multiplication Algorithm for Exact Gaussian Process",
    "abstract": "We present an application of the blackbox matrix-matrix multiplication (BBMM)\nalgorithm to scale up the Gaussian Process (GP) training of molecular energies\nin the molecular-orbital based machine learning (MOB-ML) framework. An\nalternative implementation of BBMM (AltBBMM) is also proposed to train more\nefficiently (over four-fold speedup) with the same accuracy and transferability\nas the original BBMM implementation. The training of MOB-ML was limited to 220\nmolecules, and BBMM and AltBBMM scale the training of MOB-ML up by over 30\ntimes to 6500 molecules (more than a million pair energies). The accuracy and\ntransferability of both algorithms are examined on the benchmark datasets of\norganic molecules with 7 and 13 heavy atoms. These lower-scaling\nimplementations of the GP preserve the state-of-the-art learning efficiency in\nthe low-data regime while extending it to the large-data regime with better\naccuracy than other available machine learning works on molecular energies.",
    "text": "Molecular Energy Learning Using Alternative Blackbox Matrix-Matrix\n  Multiplication Algorithm for Exact Gaussian Process\n\nWe present an application of the blackbox matrix-matrix multiplication (BBMM)\nalgorithm to scale up the Gaussian Process (GP) training of molecular energies\nin the molecular-orbital based machine learning (MOB-ML) framework. An\nalternative implementation of BBMM (AltBBMM) is also proposed to train more\nefficiently (over four-fold speedup) with the same accuracy and transferability\nas the original BBMM implementation. The training of MOB-ML was limited to 220\nmolecules, and BBMM and AltBBMM scale the training of MOB-ML up by over 30\ntimes to 6500 molecules (more than a million pair energies). The accuracy and\ntransferability of both algorithms are examined on the benchmark datasets of\norganic molecules with 7 and 13 heavy atoms. These lower-scaling\nimplementations of the GP preserve the state-of-the-art learning efficiency in\nthe low-data regime while extending it to the large-data regime with better\naccuracy than other available machine learning works on molecular energies."
  },
  {
    "id": "arxiv-357",
    "title": "Visual Dialog",
    "abstract": "We introduce the task of Visual Dialog, which requires an AI agent to hold a\nmeaningful dialog with humans in natural, conversational language about visual\ncontent. Specifically, given an image, a dialog history, and a question about\nthe image, the agent has to ground the question in image, infer context from\nhistory, and answer the question accurately. Visual Dialog is disentangled\nenough from a specific downstream task so as to serve as a general test of\nmachine intelligence, while being grounded in vision enough to allow objective\nevaluation of individual responses and benchmark progress. We develop a novel\ntwo-person chat data-collection protocol to curate a large-scale Visual Dialog\ndataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10\nquestion-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog\nquestion-answer pairs.\n  We introduce a family of neural encoder-decoder models for Visual Dialog with\n3 encoders -- Late Fusion, Hierarchical Recurrent Encoder and Memory Network --\nand 2 decoders (generative and discriminative), which outperform a number of\nsophisticated baselines. We propose a retrieval-based evaluation protocol for\nVisual Dialog where the AI agent is asked to sort a set of candidate answers\nand evaluated on metrics such as mean-reciprocal-rank of human response. We\nquantify gap between machine and human performance on the Visual Dialog task\nvia human studies. Putting it all together, we demonstrate the first 'visual\nchatbot'! Our dataset, code, trained models and visual chatbot are available on\nhttps://visualdialog.org",
    "text": "Visual Dialog\n\nWe introduce the task of Visual Dialog, which requires an AI agent to hold a\nmeaningful dialog with humans in natural, conversational language about visual\ncontent. Specifically, given an image, a dialog history, and a question about\nthe image, the agent has to ground the question in image, infer context from\nhistory, and answer the question accurately. Visual Dialog is disentangled\nenough from a specific downstream task so as to serve as a general test of\nmachine intelligence, while being grounded in vision enough to allow objective\nevaluation of individual responses and benchmark progress. We develop a novel\ntwo-person chat data-collection protocol to curate a large-scale Visual Dialog\ndataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10\nquestion-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog\nquestion-answer pairs.\n  We introduce a family of neural encoder-decoder models for Visual Dialog with\n3 encoders -- Late Fusion, Hierarchical Recurrent Encoder and Memory Network --\nand 2 decoders (generative and discriminative), which outperform a number of\nsophisticated baselines. We propose a retrieval-based evaluation protocol for\nVisual Dialog where the AI agent is asked to sort a set of candidate answers\nand evaluated on metrics such as mean-reciprocal-rank of human response. We\nquantify gap between machine and human performance on the Visual Dialog task\nvia human studies. Putting it all together, we demonstrate the first 'visual\nchatbot'! Our dataset, code, trained models and visual chatbot are available on\nhttps://visualdialog.org"
  },
  {
    "id": "arxiv-358",
    "title": "Prediction of Maneuvering Status for Aerial Vehicles using Supervised Learning Methods",
    "abstract": "Aerial Vehicles follow a guided approach based on Latitude, Longitude and\nAltitude. This information can be used for calculating the status of\nmaneuvering for the aerial vehicles along the line of trajectory. This is a\nbinary classification problem and Machine Learning can be leveraged for solving\nsuch problem. In this paper we present a methodology for deriving maneuvering\nstatus and its prediction using Linear, Distance Metric, Discriminant Analysis\nand Boosting Ensemble supervised learning methods. We provide various metrics\nalong the line in the results section that give condensed comparison of the\nappropriate algorithm for prediction of the maneuvering status.",
    "text": "Prediction of Maneuvering Status for Aerial Vehicles using Supervised Learning Methods\n\nAerial Vehicles follow a guided approach based on Latitude, Longitude and\nAltitude. This information can be used for calculating the status of\nmaneuvering for the aerial vehicles along the line of trajectory. This is a\nbinary classification problem and Machine Learning can be leveraged for solving\nsuch problem. In this paper we present a methodology for deriving maneuvering\nstatus and its prediction using Linear, Distance Metric, Discriminant Analysis\nand Boosting Ensemble supervised learning methods. We provide various metrics\nalong the line in the results section that give condensed comparison of the\nappropriate algorithm for prediction of the maneuvering status."
  },
  {
    "id": "arxiv-359",
    "title": "Language Transfer for Early Warning of Epidemics from Social Media",
    "abstract": "Statements on social media can be analysed to identify individuals who are\nexperiencing red flag medical symptoms, allowing early detection of the spread\nof disease such as influenza. Since disease does not respect cultural borders\nand may spread between populations speaking different languages, we would like\nto build multilingual models. However, the data required to train models for\nevery language may be difficult, expensive and time-consuming to obtain,\nparticularly for low-resource languages. Taking Japanese as our target\nlanguage, we explore methods by which data in one language might be used to\nbuild models for a different language. We evaluate strategies of training on\nmachine translated data and of zero-shot transfer through the use of\nmultilingual models. We find that the choice of source language impacts the\nperformance, with Chinese-Japanese being a better language pair than\nEnglish-Japanese. Training on machine translated data shows promise, especially\nwhen used in conjunction with a small amount of target language data.",
    "text": "Language Transfer for Early Warning of Epidemics from Social Media\n\nStatements on social media can be analysed to identify individuals who are\nexperiencing red flag medical symptoms, allowing early detection of the spread\nof disease such as influenza. Since disease does not respect cultural borders\nand may spread between populations speaking different languages, we would like\nto build multilingual models. However, the data required to train models for\nevery language may be difficult, expensive and time-consuming to obtain,\nparticularly for low-resource languages. Taking Japanese as our target\nlanguage, we explore methods by which data in one language might be used to\nbuild models for a different language. We evaluate strategies of training on\nmachine translated data and of zero-shot transfer through the use of\nmultilingual models. We find that the choice of source language impacts the\nperformance, with Chinese-Japanese being a better language pair than\nEnglish-Japanese. Training on machine translated data shows promise, especially\nwhen used in conjunction with a small amount of target language data."
  },
  {
    "id": "arxiv-360",
    "title": "Explainable multiple abnormality classification of chest CT volumes with\n  deep learning",
    "abstract": "Understanding model predictions is critical in healthcare, to facilitate\nrapid verification of model correctness and to guard against use of models that\nexploit confounding variables. We introduce the challenging new task of\nexplainable multiple abnormality classification in volumetric medical images,\nin which a model must indicate the regions used to predict each abnormality. To\nsolve this task, we propose a multiple instance learning convolutional neural\nnetwork, AxialNet, that allows identification of top slices for each\nabnormality. Next we incorporate HiResCAM, an attention mechanism, to identify\nsub-slice regions. We prove that for AxialNet, HiResCAM explanations are\nguaranteed to reflect the locations the model used, unlike Grad-CAM which\nsometimes highlights irrelevant locations. Armed with a model that produces\nfaithful explanations, we then aim to improve the model's learning through a\nnovel mask loss that leverages HiResCAM and 3D allowed regions to encourage the\nmodel to predict abnormalities based only on the organs in which those\nabnormalities appear. The 3D allowed regions are obtained automatically through\na new approach, PARTITION, that combines location information extracted from\nradiology reports with organ segmentation maps obtained through morphological\nimage processing. Overall, we propose the first model for explainable\nmulti-abnormality prediction in volumetric medical images, and then use the\nmask loss to achieve a 33% improvement in organ localization of multiple\nabnormalities in the RAD-ChestCT data set of 36,316 scans, representing the\nstate of the art. This work advances the clinical applicability of multiple\nabnormality modeling in chest CT volumes.",
    "text": "Explainable multiple abnormality classification of chest CT volumes with\n  deep learning\n\nUnderstanding model predictions is critical in healthcare, to facilitate\nrapid verification of model correctness and to guard against use of models that\nexploit confounding variables. We introduce the challenging new task of\nexplainable multiple abnormality classification in volumetric medical images,\nin which a model must indicate the regions used to predict each abnormality. To\nsolve this task, we propose a multiple instance learning convolutional neural\nnetwork, AxialNet, that allows identification of top slices for each\nabnormality. Next we incorporate HiResCAM, an attention mechanism, to identify\nsub-slice regions. We prove that for AxialNet, HiResCAM explanations are\nguaranteed to reflect the locations the model used, unlike Grad-CAM which\nsometimes highlights irrelevant locations. Armed with a model that produces\nfaithful explanations, we then aim to improve the model's learning through a\nnovel mask loss that leverages HiResCAM and 3D allowed regions to encourage the\nmodel to predict abnormalities based only on the organs in which those\nabnormalities appear. The 3D allowed regions are obtained automatically through\na new approach, PARTITION, that combines location information extracted from\nradiology reports with organ segmentation maps obtained through morphological\nimage processing. Overall, we propose the first model for explainable\nmulti-abnormality prediction in volumetric medical images, and then use the\nmask loss to achieve a 33% improvement in organ localization of multiple\nabnormalities in the RAD-ChestCT data set of 36,316 scans, representing the\nstate of the art. This work advances the clinical applicability of multiple\nabnormality modeling in chest CT volumes."
  },
  {
    "id": "arxiv-361",
    "title": "Bias Reduction via Cooperative Bargaining in Synthetic Graph Dataset\n  Generation",
    "abstract": "In general, to draw robust conclusions from a dataset, all the analyzed\npopulation must be represented on said dataset. Having a dataset that does not\nfulfill this condition normally leads to selection bias. Additionally, graphs\nhave been used to model a wide variety of problems. Although synthetic graphs\ncan be used to augment available real graph datasets to overcome selection\nbias, the generation of unbiased synthetic datasets is complex with current\ntools. In this work, we propose a method to find a synthetic graph dataset that\nhas an even representation of graphs with different metrics. The resulting\ndataset can then be used, among others, for benchmarking graph processing\ntechniques as the accuracy of different Graph Neural Network (GNN) models or\nthe speedups obtained by different graph processing acceleration frameworks.",
    "text": "Bias Reduction via Cooperative Bargaining in Synthetic Graph Dataset\n  Generation\n\nIn general, to draw robust conclusions from a dataset, all the analyzed\npopulation must be represented on said dataset. Having a dataset that does not\nfulfill this condition normally leads to selection bias. Additionally, graphs\nhave been used to model a wide variety of problems. Although synthetic graphs\ncan be used to augment available real graph datasets to overcome selection\nbias, the generation of unbiased synthetic datasets is complex with current\ntools. In this work, we propose a method to find a synthetic graph dataset that\nhas an even representation of graphs with different metrics. The resulting\ndataset can then be used, among others, for benchmarking graph processing\ntechniques as the accuracy of different Graph Neural Network (GNN) models or\nthe speedups obtained by different graph processing acceleration frameworks."
  },
  {
    "id": "arxiv-362",
    "title": "Rethinking Graph Neural Networks for Anomaly Detection",
    "abstract": "Graph Neural Networks (GNNs) are widely applied for graph anomaly detection.\nAs one of the key components for GNN design is to select a tailored spectral\nfilter, we take the first step towards analyzing anomalies via the lens of the\ngraph spectrum. Our crucial observation is the existence of anomalies will lead\nto the `right-shift' phenomenon, that is, the spectral energy distribution\nconcentrates less on low frequencies and more on high frequencies. This fact\nmotivates us to propose the Beta Wavelet Graph Neural Network (BWGNN). Indeed,\nBWGNN has spectral and spatial localized band-pass filters to better handle the\n`right-shift' phenomenon in anomalies. We demonstrate the effectiveness of\nBWGNN on four large-scale anomaly detection datasets. Our code and data are\nreleased at https://github.com/squareRoot3/Rethinking-Anomaly-Detection",
    "text": "Rethinking Graph Neural Networks for Anomaly Detection\n\nGraph Neural Networks (GNNs) are widely applied for graph anomaly detection.\nAs one of the key components for GNN design is to select a tailored spectral\nfilter, we take the first step towards analyzing anomalies via the lens of the\ngraph spectrum. Our crucial observation is the existence of anomalies will lead\nto the `right-shift' phenomenon, that is, the spectral energy distribution\nconcentrates less on low frequencies and more on high frequencies. This fact\nmotivates us to propose the Beta Wavelet Graph Neural Network (BWGNN). Indeed,\nBWGNN has spectral and spatial localized band-pass filters to better handle the\n`right-shift' phenomenon in anomalies. We demonstrate the effectiveness of\nBWGNN on four large-scale anomaly detection datasets. Our code and data are\nreleased at https://github.com/squareRoot3/Rethinking-Anomaly-Detection"
  },
  {
    "id": "arxiv-363",
    "title": "Answer Sequence Learning with Neural Networks for Answer Selection in\n  Community Question Answering",
    "abstract": "In this paper, the answer selection problem in community question answering\n(CQA) is regarded as an answer sequence labeling task, and a novel approach is\nproposed based on the recurrent architecture for this problem. Our approach\napplies convolution neural networks (CNNs) to learning the joint representation\nof question-answer pair firstly, and then uses the joint representation as\ninput of the long short-term memory (LSTM) to learn the answer sequence of a\nquestion for labeling the matching quality of each answer. Experiments\nconducted on the SemEval 2015 CQA dataset shows the effectiveness of our\napproach.",
    "text": "Answer Sequence Learning with Neural Networks for Answer Selection in\n  Community Question Answering\n\nIn this paper, the answer selection problem in community question answering\n(CQA) is regarded as an answer sequence labeling task, and a novel approach is\nproposed based on the recurrent architecture for this problem. Our approach\napplies convolution neural networks (CNNs) to learning the joint representation\nof question-answer pair firstly, and then uses the joint representation as\ninput of the long short-term memory (LSTM) to learn the answer sequence of a\nquestion for labeling the matching quality of each answer. Experiments\nconducted on the SemEval 2015 CQA dataset shows the effectiveness of our\napproach."
  },
  {
    "id": "arxiv-364",
    "title": "How Much Position Information Do Convolutional Neural Networks Encode?",
    "abstract": "In contrast to fully connected networks, Convolutional Neural Networks (CNNs)\nachieve efficiency by learning weights associated with local filters with a\nfinite spatial extent. An implication of this is that a filter may know what it\nis looking at, but not where it is positioned in the image. Information\nconcerning absolute position is inherently useful, and it is reasonable to\nassume that deep CNNs may implicitly learn to encode this information if there\nis a means to do so. In this paper, we test this hypothesis revealing the\nsurprising degree of absolute position information that is encoded in commonly\nused neural networks. A comprehensive set of experiments show the validity of\nthis hypothesis and shed light on how and where this information is represented\nwhile offering clues to where positional information is derived from in deep\nCNNs.",
    "text": "How Much Position Information Do Convolutional Neural Networks Encode?\n\nIn contrast to fully connected networks, Convolutional Neural Networks (CNNs)\nachieve efficiency by learning weights associated with local filters with a\nfinite spatial extent. An implication of this is that a filter may know what it\nis looking at, but not where it is positioned in the image. Information\nconcerning absolute position is inherently useful, and it is reasonable to\nassume that deep CNNs may implicitly learn to encode this information if there\nis a means to do so. In this paper, we test this hypothesis revealing the\nsurprising degree of absolute position information that is encoded in commonly\nused neural networks. A comprehensive set of experiments show the validity of\nthis hypothesis and shed light on how and where this information is represented\nwhile offering clues to where positional information is derived from in deep\nCNNs."
  },
  {
    "id": "arxiv-365",
    "title": "Image segmentation via Cellular Automata",
    "abstract": "In this paper, we propose a new approach for building cellular automata to\nsolve real-world segmentation problems. We design and train a cellular\nautomaton that can successfully segment high-resolution images. We consider a\ncolony that densely inhabits the pixel grid, and all cells are governed by a\nrandomized update that uses the current state, the color, and the state of the\n$3\\times 3$ neighborhood. The space of possible rules is defined by a small\nneural network. The update rule is applied repeatedly in parallel to a large\nrandom subset of cells and after convergence is used to produce segmentation\nmasks that are then back-propagated to learn the optimal update rules using\nstandard gradient descent methods. We demonstrate that such models can be\nlearned efficiently with only limited trajectory length and that they show\nremarkable ability to organize the information to produce a globally consistent\nsegmentation result, using only local information exchange. From a practical\nperspective, our approach allows us to build very efficient models -- our\nsmallest automaton uses less than 10,000 parameters to solve complex\nsegmentation tasks.",
    "text": "Image segmentation via Cellular Automata\n\nIn this paper, we propose a new approach for building cellular automata to\nsolve real-world segmentation problems. We design and train a cellular\nautomaton that can successfully segment high-resolution images. We consider a\ncolony that densely inhabits the pixel grid, and all cells are governed by a\nrandomized update that uses the current state, the color, and the state of the\n$3\\times 3$ neighborhood. The space of possible rules is defined by a small\nneural network. The update rule is applied repeatedly in parallel to a large\nrandom subset of cells and after convergence is used to produce segmentation\nmasks that are then back-propagated to learn the optimal update rules using\nstandard gradient descent methods. We demonstrate that such models can be\nlearned efficiently with only limited trajectory length and that they show\nremarkable ability to organize the information to produce a globally consistent\nsegmentation result, using only local information exchange. From a practical\nperspective, our approach allows us to build very efficient models -- our\nsmallest automaton uses less than 10,000 parameters to solve complex\nsegmentation tasks."
  },
  {
    "id": "arxiv-366",
    "title": "Analysis of Software Engineering for Agile Machine Learning Projects",
    "abstract": "The number of machine learning, artificial intelligence or data science\nrelated software engineering projects using Agile methodology is increasing.\nHowever, there are very few studies on how such projects work in practice. In\nthis paper, we analyze project issues tracking data taken from Scrum (a popular\ntool for Agile) for several machine learning projects. We compare this data\nwith corresponding data from non-machine learning projects, in an attempt to\nanalyze how machine learning projects are executed differently from normal\nsoftware engineering projects. On analysis, we find that machine learning\nproject issues use different kinds of words to describe issues, have higher\nnumber of exploratory or research oriented tasks as compared to implementation\ntasks, and have a higher number of issues in the product backlog after each\nsprint, denoting that it is more difficult to estimate the duration of machine\nlearning project related tasks in advance. After analyzing this data, we\npropose a few ways in which Agile machine learning projects can be better\nlogged and executed, given their differences with normal software engineering\nprojects.",
    "text": "Analysis of Software Engineering for Agile Machine Learning Projects\n\nThe number of machine learning, artificial intelligence or data science\nrelated software engineering projects using Agile methodology is increasing.\nHowever, there are very few studies on how such projects work in practice. In\nthis paper, we analyze project issues tracking data taken from Scrum (a popular\ntool for Agile) for several machine learning projects. We compare this data\nwith corresponding data from non-machine learning projects, in an attempt to\nanalyze how machine learning projects are executed differently from normal\nsoftware engineering projects. On analysis, we find that machine learning\nproject issues use different kinds of words to describe issues, have higher\nnumber of exploratory or research oriented tasks as compared to implementation\ntasks, and have a higher number of issues in the product backlog after each\nsprint, denoting that it is more difficult to estimate the duration of machine\nlearning project related tasks in advance. After analyzing this data, we\npropose a few ways in which Agile machine learning projects can be better\nlogged and executed, given their differences with normal software engineering\nprojects."
  },
  {
    "id": "arxiv-367",
    "title": "Balance-Subsampled Stable Prediction",
    "abstract": "In machine learning, it is commonly assumed that training and test data share\nthe same population distribution. However, this assumption is often violated in\npractice because the sample selection bias may induce the distribution shift\nfrom training data to test data. Such a model-agnostic distribution shift\nusually leads to prediction instability across unknown test data. In this\npaper, we propose a novel balance-subsampled stable prediction (BSSP) algorithm\nbased on the theory of fractional factorial design. It isolates the clear\neffect of each predictor from the confounding variables. A design-theoretic\nanalysis shows that the proposed method can reduce the confounding effects\namong predictors induced by the distribution shift, hence improve both the\naccuracy of parameter estimation and prediction stability. Numerical\nexperiments on both synthetic and real-world data sets demonstrate that our\nBSSP algorithm significantly outperforms the baseline methods for stable\nprediction across unknown test data.",
    "text": "Balance-Subsampled Stable Prediction\n\nIn machine learning, it is commonly assumed that training and test data share\nthe same population distribution. However, this assumption is often violated in\npractice because the sample selection bias may induce the distribution shift\nfrom training data to test data. Such a model-agnostic distribution shift\nusually leads to prediction instability across unknown test data. In this\npaper, we propose a novel balance-subsampled stable prediction (BSSP) algorithm\nbased on the theory of fractional factorial design. It isolates the clear\neffect of each predictor from the confounding variables. A design-theoretic\nanalysis shows that the proposed method can reduce the confounding effects\namong predictors induced by the distribution shift, hence improve both the\naccuracy of parameter estimation and prediction stability. Numerical\nexperiments on both synthetic and real-world data sets demonstrate that our\nBSSP algorithm significantly outperforms the baseline methods for stable\nprediction across unknown test data."
  },
  {
    "id": "arxiv-368",
    "title": "Distributed Gradient Descent with Coded Partial Gradient Computations",
    "abstract": "Coded computation techniques provide robustness against straggling servers in\ndistributed computing, with the following limitations: First, they increase\ndecoding complexity. Second, they ignore computations carried out by straggling\nservers; and they are typically designed to recover the full gradient, and\nthus, cannot provide a balance between the accuracy of the gradient and\nper-iteration completion time. Here we introduce a hybrid approach, called\ncoded partial gradient computation (CPGC), that benefits from the advantages of\nboth coded and uncoded computation schemes, and reduces both the computation\ntime and decoding complexity.",
    "text": "Distributed Gradient Descent with Coded Partial Gradient Computations\n\nCoded computation techniques provide robustness against straggling servers in\ndistributed computing, with the following limitations: First, they increase\ndecoding complexity. Second, they ignore computations carried out by straggling\nservers; and they are typically designed to recover the full gradient, and\nthus, cannot provide a balance between the accuracy of the gradient and\nper-iteration completion time. Here we introduce a hybrid approach, called\ncoded partial gradient computation (CPGC), that benefits from the advantages of\nboth coded and uncoded computation schemes, and reduces both the computation\ntime and decoding complexity."
  },
  {
    "id": "arxiv-369",
    "title": "Post-Inference Prior Swapping",
    "abstract": "While Bayesian methods are praised for their ability to incorporate useful\nprior knowledge, in practice, convenient priors that allow for computationally\ncheap or tractable inference are commonly used. In this paper, we investigate\nthe following question: for a given model, is it possible to compute an\ninference result with any convenient false prior, and afterwards, given any\ntarget prior of interest, quickly transform this result into the target\nposterior? A potential solution is to use importance sampling (IS). However, we\ndemonstrate that IS will fail for many choices of the target prior, depending\non its parametric form and similarity to the false prior. Instead, we propose\nprior swapping, a method that leverages the pre-inferred false posterior to\nefficiently generate accurate posterior samples under arbitrary target priors.\nPrior swapping lets us apply less-costly inference algorithms to certain\nmodels, and incorporate new or updated prior information \"post-inference\". We\ngive theoretical guarantees about our method, and demonstrate it empirically on\na number of models and priors.",
    "text": "Post-Inference Prior Swapping\n\nWhile Bayesian methods are praised for their ability to incorporate useful\nprior knowledge, in practice, convenient priors that allow for computationally\ncheap or tractable inference are commonly used. In this paper, we investigate\nthe following question: for a given model, is it possible to compute an\ninference result with any convenient false prior, and afterwards, given any\ntarget prior of interest, quickly transform this result into the target\nposterior? A potential solution is to use importance sampling (IS). However, we\ndemonstrate that IS will fail for many choices of the target prior, depending\non its parametric form and similarity to the false prior. Instead, we propose\nprior swapping, a method that leverages the pre-inferred false posterior to\nefficiently generate accurate posterior samples under arbitrary target priors.\nPrior swapping lets us apply less-costly inference algorithms to certain\nmodels, and incorporate new or updated prior information \"post-inference\". We\ngive theoretical guarantees about our method, and demonstrate it empirically on\na number of models and priors."
  },
  {
    "id": "arxiv-370",
    "title": "Beyond the Chinese Restaurant and Pitman-Yor processes: Statistical\n  Models with Double Power-law Behavior",
    "abstract": "Bayesian nonparametric approaches, in particular the Pitman-Yor process and\nthe associated two-parameter Chinese Restaurant process, have been successfully\nused in applications where the data exhibit a power-law behavior. Examples\ninclude natural language processing, natural images or networks. There is also\ngrowing empirical evidence that some datasets exhibit a two-regime power-law\nbehavior: one regime for small frequencies, and a second regime, with a\ndifferent exponent, for high frequencies. In this paper, we introduce a class\nof completely random measures which are doubly regularly-varying. Contrary to\nthe Pitman-Yor process, we show that when completely random measures in this\nclass are normalized to obtain random probability measures and associated\nrandom partitions, such partitions exhibit a double power-law behavior. We\ndiscuss in particular three models within this class: the beta prime process\n(Broderick et al. (2015, 2018), a novel process called generalized BFRY\nprocess, and a mixture construction. We derive efficient Markov chain Monte\nCarlo algorithms to estimate the parameters of these models. Finally, we show\nthat the proposed models provide a better fit than the Pitman-Yor process on\nvarious datasets.",
    "text": "Beyond the Chinese Restaurant and Pitman-Yor processes: Statistical\n  Models with Double Power-law Behavior\n\nBayesian nonparametric approaches, in particular the Pitman-Yor process and\nthe associated two-parameter Chinese Restaurant process, have been successfully\nused in applications where the data exhibit a power-law behavior. Examples\ninclude natural language processing, natural images or networks. There is also\ngrowing empirical evidence that some datasets exhibit a two-regime power-law\nbehavior: one regime for small frequencies, and a second regime, with a\ndifferent exponent, for high frequencies. In this paper, we introduce a class\nof completely random measures which are doubly regularly-varying. Contrary to\nthe Pitman-Yor process, we show that when completely random measures in this\nclass are normalized to obtain random probability measures and associated\nrandom partitions, such partitions exhibit a double power-law behavior. We\ndiscuss in particular three models within this class: the beta prime process\n(Broderick et al. (2015, 2018), a novel process called generalized BFRY\nprocess, and a mixture construction. We derive efficient Markov chain Monte\nCarlo algorithms to estimate the parameters of these models. Finally, we show\nthat the proposed models provide a better fit than the Pitman-Yor process on\nvarious datasets."
  },
  {
    "id": "arxiv-371",
    "title": "A Unified Probabilistic Model for Learning Latent Factors and Their\n  Connectivities from High-Dimensional Data",
    "abstract": "Connectivity estimation is challenging in the context of high-dimensional\ndata. A useful preprocessing step is to group variables into clusters, however,\nit is not always clear how to do so from the perspective of connectivity\nestimation. Another practical challenge is that we may have data from multiple\nrelated classes (e.g., multiple subjects or conditions) and wish to incorporate\nconstraints on the similarities across classes. We propose a probabilistic\nmodel which simultaneously performs both a grouping of variables (i.e.,\ndetecting community structure) and estimation of connectivities between the\ngroups which correspond to latent variables. The model is essentially a factor\nanalysis model where the factors are allowed to have arbitrary correlations,\nwhile the factor loading matrix is constrained to express a community\nstructure. The model can be applied on multiple classes so that the\nconnectivities can be different between the classes, while the community\nstructure is the same for all classes. We propose an efficient estimation\nalgorithm based on score matching, and prove the identifiability of the model.\nFinally, we present an extension to directed (causal) connectivities over\nlatent variables. Simulations and experiments on fMRI data validate the\npractical utility of the method.",
    "text": "A Unified Probabilistic Model for Learning Latent Factors and Their\n  Connectivities from High-Dimensional Data\n\nConnectivity estimation is challenging in the context of high-dimensional\ndata. A useful preprocessing step is to group variables into clusters, however,\nit is not always clear how to do so from the perspective of connectivity\nestimation. Another practical challenge is that we may have data from multiple\nrelated classes (e.g., multiple subjects or conditions) and wish to incorporate\nconstraints on the similarities across classes. We propose a probabilistic\nmodel which simultaneously performs both a grouping of variables (i.e.,\ndetecting community structure) and estimation of connectivities between the\ngroups which correspond to latent variables. The model is essentially a factor\nanalysis model where the factors are allowed to have arbitrary correlations,\nwhile the factor loading matrix is constrained to express a community\nstructure. The model can be applied on multiple classes so that the\nconnectivities can be different between the classes, while the community\nstructure is the same for all classes. We propose an efficient estimation\nalgorithm based on score matching, and prove the identifiability of the model.\nFinally, we present an extension to directed (causal) connectivities over\nlatent variables. Simulations and experiments on fMRI data validate the\npractical utility of the method."
  },
  {
    "id": "arxiv-372",
    "title": "Recent Developments Combining Ensemble Smoother and Deep Generative\n  Networks for Facies History Matching",
    "abstract": "Ensemble smoothers are among the most successful and efficient techniques\ncurrently available for history matching. However, because these methods rely\non Gaussian assumptions, their performance is severely degraded when the prior\ngeology is described in terms of complex facies distributions. Inspired by the\nimpressive results obtained by deep generative networks in areas such as image\nand video generation, we started an investigation focused on the use of\nautoencoders networks to construct a continuous parameterization for facies\nmodels. In our previous publication, we combined a convolutional variational\nautoencoder (VAE) with the ensemble smoother with multiple data assimilation\n(ES-MDA) for history matching production data in models generated with\nmultiple-point geostatistics. Despite the good results reported in our previous\npublication, a major limitation of the designed parameterization is the fact\nthat it does not allow applying distance-based localization during the ensemble\nsmoother update, which limits its application in large-scale problems.\n  The present work is a continuation of this research project focusing in two\naspects: firstly, we benchmark seven different formulations, including VAE,\ngenerative adversarial network (GAN), Wasserstein GAN, variational\nauto-encoding GAN, principal component analysis (PCA) with cycle GAN, PCA with\ntransfer style network and VAE with style loss. These formulations are tested\nin a synthetic history matching problem with channelized facies. Secondly, we\npropose two strategies to allow the use of distance-based localization with the\ndeep learning parameterizations.",
    "text": "Recent Developments Combining Ensemble Smoother and Deep Generative\n  Networks for Facies History Matching\n\nEnsemble smoothers are among the most successful and efficient techniques\ncurrently available for history matching. However, because these methods rely\non Gaussian assumptions, their performance is severely degraded when the prior\ngeology is described in terms of complex facies distributions. Inspired by the\nimpressive results obtained by deep generative networks in areas such as image\nand video generation, we started an investigation focused on the use of\nautoencoders networks to construct a continuous parameterization for facies\nmodels. In our previous publication, we combined a convolutional variational\nautoencoder (VAE) with the ensemble smoother with multiple data assimilation\n(ES-MDA) for history matching production data in models generated with\nmultiple-point geostatistics. Despite the good results reported in our previous\npublication, a major limitation of the designed parameterization is the fact\nthat it does not allow applying distance-based localization during the ensemble\nsmoother update, which limits its application in large-scale problems.\n  The present work is a continuation of this research project focusing in two\naspects: firstly, we benchmark seven different formulations, including VAE,\ngenerative adversarial network (GAN), Wasserstein GAN, variational\nauto-encoding GAN, principal component analysis (PCA) with cycle GAN, PCA with\ntransfer style network and VAE with style loss. These formulations are tested\nin a synthetic history matching problem with channelized facies. Secondly, we\npropose two strategies to allow the use of distance-based localization with the\ndeep learning parameterizations."
  },
  {
    "id": "arxiv-373",
    "title": "Uncertainty with UAV Search of Multiple Goal-oriented Targets",
    "abstract": "This paper considers the complex problem of a team of UAVs searching targets\nunder uncertainty. The goal of the UAV team is to find all of the moving\ntargets as quickly as possible before they arrive at their selected goal. The\nuncertainty considered is threefold: First, the UAVs do not know the targets'\nlocations and destinations. Second, the sensing capabilities of the UAVs are\nnot perfect. Third, the targets' movement model is unknown. We suggest a\nreal-time algorithmic framework for the UAVs, combining entropy and\nstochastic-temporal belief, that aims at optimizing the probability of a quick\nand successful detection of all of the targets. We have empirically evaluated\nthe algorithmic framework, and have shown its efficiency and significant\nperformance improvement compared to other solutions. Furthermore, we have\nevaluated our framework using Peer Designed Agents (PDAs), which are computer\nagents that simulate targets and show that our algorithmic framework\noutperforms other solutions in this scenario.",
    "text": "Uncertainty with UAV Search of Multiple Goal-oriented Targets\n\nThis paper considers the complex problem of a team of UAVs searching targets\nunder uncertainty. The goal of the UAV team is to find all of the moving\ntargets as quickly as possible before they arrive at their selected goal. The\nuncertainty considered is threefold: First, the UAVs do not know the targets'\nlocations and destinations. Second, the sensing capabilities of the UAVs are\nnot perfect. Third, the targets' movement model is unknown. We suggest a\nreal-time algorithmic framework for the UAVs, combining entropy and\nstochastic-temporal belief, that aims at optimizing the probability of a quick\nand successful detection of all of the targets. We have empirically evaluated\nthe algorithmic framework, and have shown its efficiency and significant\nperformance improvement compared to other solutions. Furthermore, we have\nevaluated our framework using Peer Designed Agents (PDAs), which are computer\nagents that simulate targets and show that our algorithmic framework\noutperforms other solutions in this scenario."
  },
  {
    "id": "arxiv-374",
    "title": "Sequential adaptive elastic net approach for single-snapshot source\n  localization",
    "abstract": "This paper proposes efficient algorithms for accurate recovery of\ndirection-of-arrival (DoA) of sources from single-snapshot measurements using\ncompressed beamforming (CBF). In CBF, the conventional sensor array signal\nmodel is cast as an underdetermined complex-valued linear regression model and\nsparse signal recovery methods are used for solving the DoA finding problem. We\ndevelop a complex-valued pathwise weighted elastic net (c-PW-WEN) algorithm\nthat finds solutions at knots of penalty parameter values over a path (or grid)\nof EN tuning parameter values. c-PW-WEN also computes Lasso or weighted Lasso\nin its path. We then propose a sequential adaptive EN (SAEN) method that is\nbased on c-PW-WEN algorithm with adaptive weights that depend on the previous\nsolution. Extensive simulation studies illustrate that SAEN improves the\nprobability of exact recovery of true support compared to conventional sparse\nsignal recovery approaches such as Lasso, elastic net or orthogonal matching\npursuit in several challenging multiple target scenarios. The effectiveness of\nSAEN is more pronounced in the presence of high mutual coherence.",
    "text": "Sequential adaptive elastic net approach for single-snapshot source\n  localization\n\nThis paper proposes efficient algorithms for accurate recovery of\ndirection-of-arrival (DoA) of sources from single-snapshot measurements using\ncompressed beamforming (CBF). In CBF, the conventional sensor array signal\nmodel is cast as an underdetermined complex-valued linear regression model and\nsparse signal recovery methods are used for solving the DoA finding problem. We\ndevelop a complex-valued pathwise weighted elastic net (c-PW-WEN) algorithm\nthat finds solutions at knots of penalty parameter values over a path (or grid)\nof EN tuning parameter values. c-PW-WEN also computes Lasso or weighted Lasso\nin its path. We then propose a sequential adaptive EN (SAEN) method that is\nbased on c-PW-WEN algorithm with adaptive weights that depend on the previous\nsolution. Extensive simulation studies illustrate that SAEN improves the\nprobability of exact recovery of true support compared to conventional sparse\nsignal recovery approaches such as Lasso, elastic net or orthogonal matching\npursuit in several challenging multiple target scenarios. The effectiveness of\nSAEN is more pronounced in the presence of high mutual coherence."
  },
  {
    "id": "arxiv-375",
    "title": "Detecting Regions of Maximal Divergence for Spatio-Temporal Anomaly\n  Detection",
    "abstract": "Automatic detection of anomalies in space- and time-varying measurements is\nan important tool in several fields, e.g., fraud detection, climate analysis,\nor healthcare monitoring. We present an algorithm for detecting anomalous\nregions in multivariate spatio-temporal time-series, which allows for spotting\nthe interesting parts in large amounts of data, including video and text data.\nIn opposition to existing techniques for detecting isolated anomalous data\npoints, we propose the \"Maximally Divergent Intervals\" (MDI) framework for\nunsupervised detection of coherent spatial regions and time intervals\ncharacterized by a high Kullback-Leibler divergence compared with all other\ndata given. In this regard, we define an unbiased Kullback-Leibler divergence\nthat allows for ranking regions of different size and show how to enable the\nalgorithm to run on large-scale data sets in reasonable time using an interval\nproposal technique. Experiments on both synthetic and real data from various\ndomains, such as climate analysis, video surveillance, and text forensics,\ndemonstrate that our method is widely applicable and a valuable tool for\nfinding interesting events in different types of data.",
    "text": "Detecting Regions of Maximal Divergence for Spatio-Temporal Anomaly\n  Detection\n\nAutomatic detection of anomalies in space- and time-varying measurements is\nan important tool in several fields, e.g., fraud detection, climate analysis,\nor healthcare monitoring. We present an algorithm for detecting anomalous\nregions in multivariate spatio-temporal time-series, which allows for spotting\nthe interesting parts in large amounts of data, including video and text data.\nIn opposition to existing techniques for detecting isolated anomalous data\npoints, we propose the \"Maximally Divergent Intervals\" (MDI) framework for\nunsupervised detection of coherent spatial regions and time intervals\ncharacterized by a high Kullback-Leibler divergence compared with all other\ndata given. In this regard, we define an unbiased Kullback-Leibler divergence\nthat allows for ranking regions of different size and show how to enable the\nalgorithm to run on large-scale data sets in reasonable time using an interval\nproposal technique. Experiments on both synthetic and real data from various\ndomains, such as climate analysis, video surveillance, and text forensics,\ndemonstrate that our method is widely applicable and a valuable tool for\nfinding interesting events in different types of data."
  },
  {
    "id": "arxiv-376",
    "title": "Latent Code-Based Fusion: A Volterra Neural Network Approach",
    "abstract": "We propose a deep structure encoder using the recently introduced Volterra\nNeural Networks (VNNs) to seek a latent representation of multi-modal data\nwhose features are jointly captured by a union of subspaces. The so-called\nself-representation embedding of the latent codes leads to a simplified fusion\nwhich is driven by a similarly constructed decoding. The Volterra Filter\narchitecture achieved reduction in parameter complexity is primarily due to\ncontrolled non-linearities being introduced by the higher-order convolutions in\ncontrast to generalized activation functions. Experimental results on two\ndifferent datasets have shown a significant improvement in the clustering\nperformance for VNNs auto-encoder over conventional Convolutional Neural\nNetworks (CNNs) auto-encoder. In addition, we also show that the proposed\napproach demonstrates a much-improved sample complexity over CNN-based\nauto-encoder with a superb robust classification performance.",
    "text": "Latent Code-Based Fusion: A Volterra Neural Network Approach\n\nWe propose a deep structure encoder using the recently introduced Volterra\nNeural Networks (VNNs) to seek a latent representation of multi-modal data\nwhose features are jointly captured by a union of subspaces. The so-called\nself-representation embedding of the latent codes leads to a simplified fusion\nwhich is driven by a similarly constructed decoding. The Volterra Filter\narchitecture achieved reduction in parameter complexity is primarily due to\ncontrolled non-linearities being introduced by the higher-order convolutions in\ncontrast to generalized activation functions. Experimental results on two\ndifferent datasets have shown a significant improvement in the clustering\nperformance for VNNs auto-encoder over conventional Convolutional Neural\nNetworks (CNNs) auto-encoder. In addition, we also show that the proposed\napproach demonstrates a much-improved sample complexity over CNN-based\nauto-encoder with a superb robust classification performance."
  },
  {
    "id": "arxiv-377",
    "title": "Autonomous CRM Control via CLV Approximation with Deep Reinforcement\n  Learning in Discrete and Continuous Action Space",
    "abstract": "The paper outlines a framework for autonomous control of a CRM (customer\nrelationship management) system. First, it explores how a modified version of\nthe widely accepted Recency-Frequency-Monetary Value system of metrics can be\nused to define the state space of clients or donors. Second, it describes a\nprocedure to determine the optimal direct marketing action in discrete and\ncontinuous action space for the given individual, based on his position in the\nstate space. The procedure involves the use of model-free Q-learning to train a\ndeep neural network that relates a client's position in the state space to\nrewards associated with possible marketing actions. The estimated value\nfunction over the client state space can be interpreted as customer lifetime\nvalue, and thus allows for a quick plug-in estimation of CLV for a given\nclient. Experimental results are presented, based on KDD Cup 1998 mailing\ndataset of donation solicitations.",
    "text": "Autonomous CRM Control via CLV Approximation with Deep Reinforcement\n  Learning in Discrete and Continuous Action Space\n\nThe paper outlines a framework for autonomous control of a CRM (customer\nrelationship management) system. First, it explores how a modified version of\nthe widely accepted Recency-Frequency-Monetary Value system of metrics can be\nused to define the state space of clients or donors. Second, it describes a\nprocedure to determine the optimal direct marketing action in discrete and\ncontinuous action space for the given individual, based on his position in the\nstate space. The procedure involves the use of model-free Q-learning to train a\ndeep neural network that relates a client's position in the state space to\nrewards associated with possible marketing actions. The estimated value\nfunction over the client state space can be interpreted as customer lifetime\nvalue, and thus allows for a quick plug-in estimation of CLV for a given\nclient. Experimental results are presented, based on KDD Cup 1998 mailing\ndataset of donation solicitations."
  },
  {
    "id": "arxiv-378",
    "title": "TLDR: Twin Learning for Dimensionality Reduction",
    "abstract": "Dimensionality reduction methods are unsupervised approaches which learn\nlow-dimensional spaces where some properties of the initial space, typically\nthe notion of \"neighborhood\", are preserved. Such methods usually require\npropagation on large k-NN graphs or complicated optimization solvers. On the\nother hand, self-supervised learning approaches, typically used to learn\nrepresentations from scratch, rely on simple and more scalable frameworks for\nlearning. In this paper, we propose TLDR, a dimensionality reduction method for\ngeneric input spaces that is porting the recent self-supervised learning\nframework of Zbontar et al. (2021) to the specific task of dimensionality\nreduction, over arbitrary representations. We propose to use nearest neighbors\nto build pairs from a training set and a redundancy reduction loss to learn an\nencoder that produces representations invariant across such pairs. TLDR is a\nmethod that is simple, easy to train, and of broad applicability; it consists\nof an offline nearest neighbor computation step that can be highly\napproximated, and a straightforward learning process. Aiming for scalability,\nwe focus on improving linear dimensionality reduction, and show consistent\ngains on image and document retrieval tasks, e.g. gaining +4% mAP over PCA on\nROxford for GeM- AP, improving the performance of DINO on ImageNet or retaining\nit with a 10x compression.",
    "text": "TLDR: Twin Learning for Dimensionality Reduction\n\nDimensionality reduction methods are unsupervised approaches which learn\nlow-dimensional spaces where some properties of the initial space, typically\nthe notion of \"neighborhood\", are preserved. Such methods usually require\npropagation on large k-NN graphs or complicated optimization solvers. On the\nother hand, self-supervised learning approaches, typically used to learn\nrepresentations from scratch, rely on simple and more scalable frameworks for\nlearning. In this paper, we propose TLDR, a dimensionality reduction method for\ngeneric input spaces that is porting the recent self-supervised learning\nframework of Zbontar et al. (2021) to the specific task of dimensionality\nreduction, over arbitrary representations. We propose to use nearest neighbors\nto build pairs from a training set and a redundancy reduction loss to learn an\nencoder that produces representations invariant across such pairs. TLDR is a\nmethod that is simple, easy to train, and of broad applicability; it consists\nof an offline nearest neighbor computation step that can be highly\napproximated, and a straightforward learning process. Aiming for scalability,\nwe focus on improving linear dimensionality reduction, and show consistent\ngains on image and document retrieval tasks, e.g. gaining +4% mAP over PCA on\nROxford for GeM- AP, improving the performance of DINO on ImageNet or retaining\nit with a 10x compression."
  },
  {
    "id": "arxiv-379",
    "title": "A General framework for PAC-Bayes Bounds for Meta-Learning",
    "abstract": "Meta learning automatically infers an inductive bias, that includes the\nhyperparameter of the base-learning algorithm, by observing data from a finite\nnumber of related tasks. This paper studies PAC-Bayes bounds on meta\ngeneralization gap. The meta-generalization gap comprises two sources of\ngeneralization gaps: the environment-level and task-level gaps resulting from\nobservation of a finite number of tasks and data samples per task,\nrespectively. In this paper, by upper bounding arbitrary convex functions,\nwhich link the expected and empirical losses at the environment and also\nper-task levels, we obtain new PAC-Bayes bounds. Using these bounds, we develop\nnew PAC-Bayes meta-learning algorithms. Numerical examples demonstrate the\nmerits of the proposed novel bounds and algorithm in comparison to prior\nPAC-Bayes bounds for meta-learning.",
    "text": "A General framework for PAC-Bayes Bounds for Meta-Learning\n\nMeta learning automatically infers an inductive bias, that includes the\nhyperparameter of the base-learning algorithm, by observing data from a finite\nnumber of related tasks. This paper studies PAC-Bayes bounds on meta\ngeneralization gap. The meta-generalization gap comprises two sources of\ngeneralization gaps: the environment-level and task-level gaps resulting from\nobservation of a finite number of tasks and data samples per task,\nrespectively. In this paper, by upper bounding arbitrary convex functions,\nwhich link the expected and empirical losses at the environment and also\nper-task levels, we obtain new PAC-Bayes bounds. Using these bounds, we develop\nnew PAC-Bayes meta-learning algorithms. Numerical examples demonstrate the\nmerits of the proposed novel bounds and algorithm in comparison to prior\nPAC-Bayes bounds for meta-learning."
  },
  {
    "id": "arxiv-380",
    "title": "Speech enhancement aided end-to-end multi-task learning for voice\n  activity detection",
    "abstract": "Robust voice activity detection (VAD) is a challenging task in low\nsignal-to-noise (SNR) environments. Recent studies show that speech enhancement\nis helpful to VAD, but the performance improvement is limited. To address this\nissue, here we propose a speech enhancement aided end-to-end multi-task model\nfor VAD. The model has two decoders, one for speech enhancement and the other\nfor VAD. The two decoders share the same encoder and speech separation network.\nUnlike the direct thought that takes two separated objectives for VAD and\nspeech enhancement respectively, here we propose a new joint optimization\nobjective -- VAD-masked scale-invariant source-to-distortion ratio (mSI-SDR).\nmSI-SDR uses VAD information to mask the output of the speech enhancement\ndecoder in the training process. It makes the VAD and speech enhancement tasks\njointly optimized not only at the shared encoder and separation network, but\nalso at the objective level. It also satisfies real-time working requirement\ntheoretically. Experimental results show that the multi-task method\nsignificantly outperforms its single-task VAD counterpart. Moreover, mSI-SDR\noutperforms SI-SDR in the same multi-task setting.",
    "text": "Speech enhancement aided end-to-end multi-task learning for voice\n  activity detection\n\nRobust voice activity detection (VAD) is a challenging task in low\nsignal-to-noise (SNR) environments. Recent studies show that speech enhancement\nis helpful to VAD, but the performance improvement is limited. To address this\nissue, here we propose a speech enhancement aided end-to-end multi-task model\nfor VAD. The model has two decoders, one for speech enhancement and the other\nfor VAD. The two decoders share the same encoder and speech separation network.\nUnlike the direct thought that takes two separated objectives for VAD and\nspeech enhancement respectively, here we propose a new joint optimization\nobjective -- VAD-masked scale-invariant source-to-distortion ratio (mSI-SDR).\nmSI-SDR uses VAD information to mask the output of the speech enhancement\ndecoder in the training process. It makes the VAD and speech enhancement tasks\njointly optimized not only at the shared encoder and separation network, but\nalso at the objective level. It also satisfies real-time working requirement\ntheoretically. Experimental results show that the multi-task method\nsignificantly outperforms its single-task VAD counterpart. Moreover, mSI-SDR\noutperforms SI-SDR in the same multi-task setting."
  },
  {
    "id": "arxiv-381",
    "title": "Controlled Caption Generation for Images Through Adversarial Attacks",
    "abstract": "Deep learning is found to be vulnerable to adversarial examples. However, its\nadversarial susceptibility in image caption generation is under-explored. We\nstudy adversarial examples for vision and language models, which typically\nadopt an encoder-decoder framework consisting of two major components: a\nConvolutional Neural Network (i.e., CNN) for image feature extraction and a\nRecurrent Neural Network (RNN) for caption generation. In particular, we\ninvestigate attacks on the visual encoder's hidden layer that is fed to the\nsubsequent recurrent network. The existing methods either attack the\nclassification layer of the visual encoder or they back-propagate the gradients\nfrom the language model. In contrast, we propose a GAN-based algorithm for\ncrafting adversarial examples for neural image captioning that mimics the\ninternal representation of the CNN such that the resulting deep features of the\ninput image enable a controlled incorrect caption generation through the\nrecurrent network. Our contribution provides new insights for understanding\nadversarial attacks on vision systems with language component. The proposed\nmethod employs two strategies for a comprehensive evaluation. The first\nexamines if a neural image captioning system can be misled to output targeted\nimage captions. The second analyzes the possibility of keywords into the\npredicted captions. Experiments show that our algorithm can craft effective\nadversarial images based on the CNN hidden layers to fool captioning framework.\nMoreover, we discover the proposed attack to be highly transferable. Our work\nleads to new robustness implications for neural image captioning.",
    "text": "Controlled Caption Generation for Images Through Adversarial Attacks\n\nDeep learning is found to be vulnerable to adversarial examples. However, its\nadversarial susceptibility in image caption generation is under-explored. We\nstudy adversarial examples for vision and language models, which typically\nadopt an encoder-decoder framework consisting of two major components: a\nConvolutional Neural Network (i.e., CNN) for image feature extraction and a\nRecurrent Neural Network (RNN) for caption generation. In particular, we\ninvestigate attacks on the visual encoder's hidden layer that is fed to the\nsubsequent recurrent network. The existing methods either attack the\nclassification layer of the visual encoder or they back-propagate the gradients\nfrom the language model. In contrast, we propose a GAN-based algorithm for\ncrafting adversarial examples for neural image captioning that mimics the\ninternal representation of the CNN such that the resulting deep features of the\ninput image enable a controlled incorrect caption generation through the\nrecurrent network. Our contribution provides new insights for understanding\nadversarial attacks on vision systems with language component. The proposed\nmethod employs two strategies for a comprehensive evaluation. The first\nexamines if a neural image captioning system can be misled to output targeted\nimage captions. The second analyzes the possibility of keywords into the\npredicted captions. Experiments show that our algorithm can craft effective\nadversarial images based on the CNN hidden layers to fool captioning framework.\nMoreover, we discover the proposed attack to be highly transferable. Our work\nleads to new robustness implications for neural image captioning."
  },
  {
    "id": "arxiv-382",
    "title": "Some Theoretical Insights into Wasserstein GANs",
    "abstract": "Generative Adversarial Networks (GANs) have been successful in producing\noutstanding results in areas as diverse as image, video, and text generation.\nBuilding on these successes, a large number of empirical studies have validated\nthe benefits of the cousin approach called Wasserstein GANs (WGANs), which\nbrings stabilization in the training process. In the present paper, we add a\nnew stone to the edifice by proposing some theoretical advances in the\nproperties of WGANs. First, we properly define the architecture of WGANs in the\ncontext of integral probability metrics parameterized by neural networks and\nhighlight some of their basic mathematical features. We stress in particular\ninteresting optimization properties arising from the use of a parametric\n1-Lipschitz discriminator. Then, in a statistically-driven approach, we study\nthe convergence of empirical WGANs as the sample size tends to infinity, and\nclarify the adversarial effects of the generator and the discriminator by\nunderlining some trade-off properties. These features are finally illustrated\nwith experiments using both synthetic and real-world datasets.",
    "text": "Some Theoretical Insights into Wasserstein GANs\n\nGenerative Adversarial Networks (GANs) have been successful in producing\noutstanding results in areas as diverse as image, video, and text generation.\nBuilding on these successes, a large number of empirical studies have validated\nthe benefits of the cousin approach called Wasserstein GANs (WGANs), which\nbrings stabilization in the training process. In the present paper, we add a\nnew stone to the edifice by proposing some theoretical advances in the\nproperties of WGANs. First, we properly define the architecture of WGANs in the\ncontext of integral probability metrics parameterized by neural networks and\nhighlight some of their basic mathematical features. We stress in particular\ninteresting optimization properties arising from the use of a parametric\n1-Lipschitz discriminator. Then, in a statistically-driven approach, we study\nthe convergence of empirical WGANs as the sample size tends to infinity, and\nclarify the adversarial effects of the generator and the discriminator by\nunderlining some trade-off properties. These features are finally illustrated\nwith experiments using both synthetic and real-world datasets."
  },
  {
    "id": "arxiv-383",
    "title": "Learning Linear Policies for Robust Bipedal Locomotion on Terrains with\n  Varying Slopes",
    "abstract": "In this paper, with a view toward deployment of light-weight control\nframeworks for bipedal walking robots, we realize end-foot trajectories that\nare shaped by a single linear feedback policy. We learn this policy via a\nmodel-free and a gradient-free learning algorithm, Augmented Random Search\n(ARS), in the two robot platforms Rabbit and Digit. Our contributions are\ntwo-fold: a) By using torso and support plane orientation as inputs, we achieve\nrobust walking on slopes of up to 20 degrees in simulation. b) We demonstrate\nadditional behaviors like walking backwards, stepping-in-place, and recovery\nfrom external pushes of up to 120 N. The end result is a robust and a fast\nfeedback control law for bipedal walking on terrains with varying slopes.\nTowards the end, we also provide preliminary results of hardware transfer to\nDigit.",
    "text": "Learning Linear Policies for Robust Bipedal Locomotion on Terrains with\n  Varying Slopes\n\nIn this paper, with a view toward deployment of light-weight control\nframeworks for bipedal walking robots, we realize end-foot trajectories that\nare shaped by a single linear feedback policy. We learn this policy via a\nmodel-free and a gradient-free learning algorithm, Augmented Random Search\n(ARS), in the two robot platforms Rabbit and Digit. Our contributions are\ntwo-fold: a) By using torso and support plane orientation as inputs, we achieve\nrobust walking on slopes of up to 20 degrees in simulation. b) We demonstrate\nadditional behaviors like walking backwards, stepping-in-place, and recovery\nfrom external pushes of up to 120 N. The end result is a robust and a fast\nfeedback control law for bipedal walking on terrains with varying slopes.\nTowards the end, we also provide preliminary results of hardware transfer to\nDigit."
  },
  {
    "id": "arxiv-384",
    "title": "Improving Efficiency in Large-Scale Decentralized Distributed Training",
    "abstract": "Decentralized Parallel SGD (D-PSGD) and its asynchronous variant Asynchronous\nParallel SGD (AD-PSGD) is a family of distributed learning algorithms that have\nbeen demonstrated to perform well for large-scale deep learning tasks. One\ndrawback of (A)D-PSGD is that the spectral gap of the mixing matrix decreases\nwhen the number of learners in the system increases, which hampers convergence.\nIn this paper, we investigate techniques to accelerate (A)D-PSGD based training\nby improving the spectral gap while minimizing the communication cost. We\ndemonstrate the effectiveness of our proposed techniques by running experiments\non the 2000-hour Switchboard speech recognition task and the ImageNet computer\nvision task. On an IBM P9 supercomputer, our system is able to train an LSTM\nacoustic model in 2.28 hours with 7.5% WER on the Hub5-2000 Switchboard (SWB)\ntest set and 13.3% WER on the CallHome (CH) test set using 64 V100 GPUs and in\n1.98 hours with 7.7% WER on SWB and 13.3% WER on CH using 128 V100 GPUs, the\nfastest training time reported to date.",
    "text": "Improving Efficiency in Large-Scale Decentralized Distributed Training\n\nDecentralized Parallel SGD (D-PSGD) and its asynchronous variant Asynchronous\nParallel SGD (AD-PSGD) is a family of distributed learning algorithms that have\nbeen demonstrated to perform well for large-scale deep learning tasks. One\ndrawback of (A)D-PSGD is that the spectral gap of the mixing matrix decreases\nwhen the number of learners in the system increases, which hampers convergence.\nIn this paper, we investigate techniques to accelerate (A)D-PSGD based training\nby improving the spectral gap while minimizing the communication cost. We\ndemonstrate the effectiveness of our proposed techniques by running experiments\non the 2000-hour Switchboard speech recognition task and the ImageNet computer\nvision task. On an IBM P9 supercomputer, our system is able to train an LSTM\nacoustic model in 2.28 hours with 7.5% WER on the Hub5-2000 Switchboard (SWB)\ntest set and 13.3% WER on the CallHome (CH) test set using 64 V100 GPUs and in\n1.98 hours with 7.7% WER on SWB and 13.3% WER on CH using 128 V100 GPUs, the\nfastest training time reported to date."
  },
  {
    "id": "arxiv-385",
    "title": "Introducing Fuzzy Layers for Deep Learning",
    "abstract": "Many state-of-the-art technologies developed in recent years have been\ninfluenced by machine learning to some extent. Most popular at the time of this\nwriting are artificial intelligence methodologies that fall under the umbrella\nof deep learning. Deep learning has been shown across many applications to be\nextremely powerful and capable of handling problems that possess great\ncomplexity and difficulty. In this work, we introduce a new layer to deep\nlearning: the fuzzy layer. Traditionally, the network architecture of neural\nnetworks is composed of an input layer, some combination of hidden layers, and\nan output layer. We propose the introduction of fuzzy layers into the deep\nlearning architecture to exploit the powerful aggregation properties expressed\nthrough fuzzy methodologies, such as the Choquet and Sugueno fuzzy integrals.\nTo date, fuzzy approaches taken to deep learning have been through the\napplication of various fusion strategies at the decision level to aggregate\noutputs from state-of-the-art pre-trained models, e.g., AlexNet, VGG16,\nGoogLeNet, Inception-v3, ResNet-18, etc. While these strategies have been shown\nto improve accuracy performance for image classification tasks, none have\nexplored the use of fuzzified intermediate, or hidden, layers. Herein, we\npresent a new deep learning strategy that incorporates fuzzy strategies into\nthe deep learning architecture focused on the application of semantic\nsegmentation using per-pixel classification. Experiments are conducted on a\nbenchmark data set as well as a data set collected via an unmanned aerial\nsystem at a U.S. Army test site for the task of automatic road segmentation,\nand preliminary results are promising.",
    "text": "Introducing Fuzzy Layers for Deep Learning\n\nMany state-of-the-art technologies developed in recent years have been\ninfluenced by machine learning to some extent. Most popular at the time of this\nwriting are artificial intelligence methodologies that fall under the umbrella\nof deep learning. Deep learning has been shown across many applications to be\nextremely powerful and capable of handling problems that possess great\ncomplexity and difficulty. In this work, we introduce a new layer to deep\nlearning: the fuzzy layer. Traditionally, the network architecture of neural\nnetworks is composed of an input layer, some combination of hidden layers, and\nan output layer. We propose the introduction of fuzzy layers into the deep\nlearning architecture to exploit the powerful aggregation properties expressed\nthrough fuzzy methodologies, such as the Choquet and Sugueno fuzzy integrals.\nTo date, fuzzy approaches taken to deep learning have been through the\napplication of various fusion strategies at the decision level to aggregate\noutputs from state-of-the-art pre-trained models, e.g., AlexNet, VGG16,\nGoogLeNet, Inception-v3, ResNet-18, etc. While these strategies have been shown\nto improve accuracy performance for image classification tasks, none have\nexplored the use of fuzzified intermediate, or hidden, layers. Herein, we\npresent a new deep learning strategy that incorporates fuzzy strategies into\nthe deep learning architecture focused on the application of semantic\nsegmentation using per-pixel classification. Experiments are conducted on a\nbenchmark data set as well as a data set collected via an unmanned aerial\nsystem at a U.S. Army test site for the task of automatic road segmentation,\nand preliminary results are promising."
  },
  {
    "id": "arxiv-386",
    "title": "BCFNet: A Balanced Collaborative Filtering Network with Attention\n  Mechanism",
    "abstract": "Collaborative Filtering (CF) based recommendation methods have been widely\nstudied, which can be generally categorized into two types, i.e.,\nrepresentation learning-based CF methods and matching function learning-based\nCF methods. Representation learning tries to learn a common low dimensional\nspace for the representations of users and items. In this case, a user and item\nmatch better if they have higher similarity in that common space. Matching\nfunction learning tries to directly learn the complex matching function that\nmaps user-item pairs to matching scores. Although both methods are well\ndeveloped, they suffer from two fundamental flaws, i.e., the representation\nlearning resorts to applying a dot product which has limited expressiveness on\nthe latent features of users and items, while the matching function learning\nhas weakness in capturing low-rank relations. To overcome such flaws, we\npropose a novel recommendation model named Balanced Collaborative Filtering\nNetwork (BCFNet), which has the strengths of the two types of methods. In\naddition, an attention mechanism is designed to better capture the hidden\ninformation within implicit feedback and strengthen the learning ability of the\nneural network. Furthermore, a balance module is designed to alleviate the\nover-fitting issue in DNNs. Extensive experiments on eight real-world datasets\ndemonstrate the effectiveness of the proposed model.",
    "text": "BCFNet: A Balanced Collaborative Filtering Network with Attention\n  Mechanism\n\nCollaborative Filtering (CF) based recommendation methods have been widely\nstudied, which can be generally categorized into two types, i.e.,\nrepresentation learning-based CF methods and matching function learning-based\nCF methods. Representation learning tries to learn a common low dimensional\nspace for the representations of users and items. In this case, a user and item\nmatch better if they have higher similarity in that common space. Matching\nfunction learning tries to directly learn the complex matching function that\nmaps user-item pairs to matching scores. Although both methods are well\ndeveloped, they suffer from two fundamental flaws, i.e., the representation\nlearning resorts to applying a dot product which has limited expressiveness on\nthe latent features of users and items, while the matching function learning\nhas weakness in capturing low-rank relations. To overcome such flaws, we\npropose a novel recommendation model named Balanced Collaborative Filtering\nNetwork (BCFNet), which has the strengths of the two types of methods. In\naddition, an attention mechanism is designed to better capture the hidden\ninformation within implicit feedback and strengthen the learning ability of the\nneural network. Furthermore, a balance module is designed to alleviate the\nover-fitting issue in DNNs. Extensive experiments on eight real-world datasets\ndemonstrate the effectiveness of the proposed model."
  },
  {
    "id": "arxiv-387",
    "title": "Trading-Off Static and Dynamic Regret in Online Least-Squares and Beyond",
    "abstract": "Recursive least-squares algorithms often use forgetting factors as a\nheuristic to adapt to non-stationary data streams. The first contribution of\nthis paper rigorously characterizes the effect of forgetting factors for a\nclass of online Newton algorithms. For exp-concave and strongly convex\nobjectives, the algorithms achieve the dynamic regret of $\\max\\{O(\\log\nT),O(\\sqrt{TV})\\}$, where $V$ is a bound on the path length of the comparison\nsequence. In particular, we show how classic recursive least-squares with a\nforgetting factor achieves this dynamic regret bound. By varying $V$, we obtain\na trade-off between static and dynamic regret. In order to obtain more\ncomputationally efficient algorithms, our second contribution is a novel\ngradient descent step size rule for strongly convex functions. Our gradient\ndescent rule recovers the order optimal dynamic regret bounds described above.\nFor smooth problems, we can also obtain static regret of $O(T^{1-\\beta})$ and\ndynamic regret of $O(T^\\beta V^*)$, where $\\beta \\in (0,1)$ and $V^*$ is the\npath length of the sequence of minimizers. By varying $\\beta$, we obtain a\ntrade-off between static and dynamic regret.",
    "text": "Trading-Off Static and Dynamic Regret in Online Least-Squares and Beyond\n\nRecursive least-squares algorithms often use forgetting factors as a\nheuristic to adapt to non-stationary data streams. The first contribution of\nthis paper rigorously characterizes the effect of forgetting factors for a\nclass of online Newton algorithms. For exp-concave and strongly convex\nobjectives, the algorithms achieve the dynamic regret of $\\max\\{O(\\log\nT),O(\\sqrt{TV})\\}$, where $V$ is a bound on the path length of the comparison\nsequence. In particular, we show how classic recursive least-squares with a\nforgetting factor achieves this dynamic regret bound. By varying $V$, we obtain\na trade-off between static and dynamic regret. In order to obtain more\ncomputationally efficient algorithms, our second contribution is a novel\ngradient descent step size rule for strongly convex functions. Our gradient\ndescent rule recovers the order optimal dynamic regret bounds described above.\nFor smooth problems, we can also obtain static regret of $O(T^{1-\\beta})$ and\ndynamic regret of $O(T^\\beta V^*)$, where $\\beta \\in (0,1)$ and $V^*$ is the\npath length of the sequence of minimizers. By varying $\\beta$, we obtain a\ntrade-off between static and dynamic regret."
  },
  {
    "id": "arxiv-388",
    "title": "Personalized Embedding-based e-Commerce Recommendations at eBay",
    "abstract": "Recommender systems are an essential component of e-commerce marketplaces,\nhelping consumers navigate massive amounts of inventory and find what they need\nor love. In this paper, we present an approach for generating personalized item\nrecommendations in an e-commerce marketplace by learning to embed items and\nusers in the same vector space. In order to alleviate the considerable\ncold-start problem present in large marketplaces, item and user embeddings are\ncomputed using content features and multi-modal onsite user activity\nrespectively. Data ablation is incorporated into the offline model training\nprocess to improve the robustness of the production system. In offline\nevaluation using a dataset collected from eBay traffic, our approach was able\nto improve the Recall@k metric over the Recently-Viewed-Item (RVI) method. This\napproach to generating personalized recommendations has been launched to serve\nproduction traffic, and the corresponding scalable engineering architecture is\nalso presented. Initial A/B test results show that compared to the current\npersonalized recommendation module in production, the proposed method increases\nthe surface rate by $\\sim$6\\% to generate recommendations for 90\\% of listing\npage impressions.",
    "text": "Personalized Embedding-based e-Commerce Recommendations at eBay\n\nRecommender systems are an essential component of e-commerce marketplaces,\nhelping consumers navigate massive amounts of inventory and find what they need\nor love. In this paper, we present an approach for generating personalized item\nrecommendations in an e-commerce marketplace by learning to embed items and\nusers in the same vector space. In order to alleviate the considerable\ncold-start problem present in large marketplaces, item and user embeddings are\ncomputed using content features and multi-modal onsite user activity\nrespectively. Data ablation is incorporated into the offline model training\nprocess to improve the robustness of the production system. In offline\nevaluation using a dataset collected from eBay traffic, our approach was able\nto improve the Recall@k metric over the Recently-Viewed-Item (RVI) method. This\napproach to generating personalized recommendations has been launched to serve\nproduction traffic, and the corresponding scalable engineering architecture is\nalso presented. Initial A/B test results show that compared to the current\npersonalized recommendation module in production, the proposed method increases\nthe surface rate by $\\sim$6\\% to generate recommendations for 90\\% of listing\npage impressions."
  },
  {
    "id": "arxiv-389",
    "title": "Location reference identification from tweets during emergencies: A deep\n  learning approach",
    "abstract": "Twitter is recently being used during crises to communicate with officials\nand provide rescue and relief operation in real time. The geographical location\ninformation of the event, as well as users, are vitally important in such\nscenarios. The identification of geographic location is one of the challenging\ntasks as the location information fields, such as user location and place name\nof tweets are not reliable. The extraction of location information from tweet\ntext is difficult as it contains a lot of non-standard English, grammatical\nerrors, spelling mistakes, non-standard abbreviations, and so on. This research\naims to extract location words used in the tweet using a Convolutional Neural\nNetwork (CNN) based model. We achieved the exact matching score of 0.929,\nHamming loss of 0.002, and $F_1$-score of 0.96 for the tweets related to the\nearthquake. Our model was able to extract even three- to four-word long\nlocation references which is also evident from the exact matching score of over\n92\\%. The findings of this paper can help in early event localization,\nemergency situations, real-time road traffic management, localized\nadvertisement, and in various location-based services.",
    "text": "Location reference identification from tweets during emergencies: A deep\n  learning approach\n\nTwitter is recently being used during crises to communicate with officials\nand provide rescue and relief operation in real time. The geographical location\ninformation of the event, as well as users, are vitally important in such\nscenarios. The identification of geographic location is one of the challenging\ntasks as the location information fields, such as user location and place name\nof tweets are not reliable. The extraction of location information from tweet\ntext is difficult as it contains a lot of non-standard English, grammatical\nerrors, spelling mistakes, non-standard abbreviations, and so on. This research\naims to extract location words used in the tweet using a Convolutional Neural\nNetwork (CNN) based model. We achieved the exact matching score of 0.929,\nHamming loss of 0.002, and $F_1$-score of 0.96 for the tweets related to the\nearthquake. Our model was able to extract even three- to four-word long\nlocation references which is also evident from the exact matching score of over\n92\\%. The findings of this paper can help in early event localization,\nemergency situations, real-time road traffic management, localized\nadvertisement, and in various location-based services."
  },
  {
    "id": "arxiv-390",
    "title": "The functional role of cue-driven feature-based feedback in object\n  recognition",
    "abstract": "Visual object recognition is not a trivial task, especially when the objects\nare degraded or surrounded by clutter or presented briefly. External cues (such\nas verbal cues or visual context) can boost recognition performance in such\nconditions. In this work, we build an artificial neural network to model the\ninteraction between the object processing stream (OPS) and the cue. We study\nthe effects of varying neural and representational capacities of the OPS on the\nperformance boost provided by cue-driven feature-based feedback in the OPS. We\nobserve that the feedback provides performance boosts only if the\ncategory-specific features about the objects cannot be fully represented in the\nOPS. This representational limit is more dependent on task demands than neural\ncapacity. We also observe that the feedback scheme trained to maximise\nrecognition performance boost is not the same as tuning-based feedback, and\nactually performs better than tuning-based feedback.",
    "text": "The functional role of cue-driven feature-based feedback in object\n  recognition\n\nVisual object recognition is not a trivial task, especially when the objects\nare degraded or surrounded by clutter or presented briefly. External cues (such\nas verbal cues or visual context) can boost recognition performance in such\nconditions. In this work, we build an artificial neural network to model the\ninteraction between the object processing stream (OPS) and the cue. We study\nthe effects of varying neural and representational capacities of the OPS on the\nperformance boost provided by cue-driven feature-based feedback in the OPS. We\nobserve that the feedback provides performance boosts only if the\ncategory-specific features about the objects cannot be fully represented in the\nOPS. This representational limit is more dependent on task demands than neural\ncapacity. We also observe that the feedback scheme trained to maximise\nrecognition performance boost is not the same as tuning-based feedback, and\nactually performs better than tuning-based feedback."
  },
  {
    "id": "arxiv-391",
    "title": "Approximation Algorithms for Bregman Co-clustering and Tensor Clustering",
    "abstract": "In the past few years powerful generalizations to the Euclidean k-means\nproblem have been made, such as Bregman clustering [7], co-clustering (i.e.,\nsimultaneous clustering of rows and columns of an input matrix) [9,18], and\ntensor clustering [8,34]. Like k-means, these more general problems also suffer\nfrom the NP-hardness of the associated optimization. Researchers have developed\napproximation algorithms of varying degrees of sophistication for k-means,\nk-medians, and more recently also for Bregman clustering [2]. However, there\nseem to be no approximation algorithms for Bregman co- and tensor clustering.\nIn this paper we derive the first (to our knowledge) guaranteed methods for\nthese increasingly important clustering settings. Going beyond Bregman\ndivergences, we also prove an approximation factor for tensor clustering with\narbitrary separable metrics. Through extensive experiments we evaluate the\ncharacteristics of our method, and show that it also has practical impact.",
    "text": "Approximation Algorithms for Bregman Co-clustering and Tensor Clustering\n\nIn the past few years powerful generalizations to the Euclidean k-means\nproblem have been made, such as Bregman clustering [7], co-clustering (i.e.,\nsimultaneous clustering of rows and columns of an input matrix) [9,18], and\ntensor clustering [8,34]. Like k-means, these more general problems also suffer\nfrom the NP-hardness of the associated optimization. Researchers have developed\napproximation algorithms of varying degrees of sophistication for k-means,\nk-medians, and more recently also for Bregman clustering [2]. However, there\nseem to be no approximation algorithms for Bregman co- and tensor clustering.\nIn this paper we derive the first (to our knowledge) guaranteed methods for\nthese increasingly important clustering settings. Going beyond Bregman\ndivergences, we also prove an approximation factor for tensor clustering with\narbitrary separable metrics. Through extensive experiments we evaluate the\ncharacteristics of our method, and show that it also has practical impact."
  },
  {
    "id": "arxiv-392",
    "title": "Revisiting Large Scale Distributed Machine Learning",
    "abstract": "Nowadays, with the widespread of smartphones and other portable gadgets\nequipped with a variety of sensors, data is ubiquitous available and the focus\nof machine learning has shifted from being able to infer from small training\nsamples to dealing with large scale high-dimensional data. In domains such as\npersonal healthcare applications, which motivates this survey, distributed\nmachine learning is a promising line of research, both for scaling up learning\nalgorithms, but mostly for dealing with data which is inherently produced at\ndifferent locations. This report offers a thorough overview of and\nstate-of-the-art algorithms for distributed machine learning, for both\nsupervised and unsupervised learning, ranging from simple linear logistic\nregression to graphical models and clustering. We propose future directions for\nmost categories, specific to the potential personal healthcare applications.\nWith this in mind, the report focuses on how security and low communication\noverhead can be assured in the specific case of a strictly client-server\narchitectural model. As particular directions we provides an exhaustive\npresentation of an empirical clustering algorithm, k-windows, and proposed an\nasynchronous distributed machine learning algorithm that would scale well and\nalso would be computationally cheap and easy to implement.",
    "text": "Revisiting Large Scale Distributed Machine Learning\n\nNowadays, with the widespread of smartphones and other portable gadgets\nequipped with a variety of sensors, data is ubiquitous available and the focus\nof machine learning has shifted from being able to infer from small training\nsamples to dealing with large scale high-dimensional data. In domains such as\npersonal healthcare applications, which motivates this survey, distributed\nmachine learning is a promising line of research, both for scaling up learning\nalgorithms, but mostly for dealing with data which is inherently produced at\ndifferent locations. This report offers a thorough overview of and\nstate-of-the-art algorithms for distributed machine learning, for both\nsupervised and unsupervised learning, ranging from simple linear logistic\nregression to graphical models and clustering. We propose future directions for\nmost categories, specific to the potential personal healthcare applications.\nWith this in mind, the report focuses on how security and low communication\noverhead can be assured in the specific case of a strictly client-server\narchitectural model. As particular directions we provides an exhaustive\npresentation of an empirical clustering algorithm, k-windows, and proposed an\nasynchronous distributed machine learning algorithm that would scale well and\nalso would be computationally cheap and easy to implement."
  },
  {
    "id": "arxiv-393",
    "title": "A Block-wise, Asynchronous and Distributed ADMM Algorithm for General\n  Form Consensus Optimization",
    "abstract": "Many machine learning models, including those with non-smooth regularizers,\ncan be formulated as consensus optimization problems, which can be solved by\nthe alternating direction method of multipliers (ADMM). Many recent efforts\nhave been made to develop asynchronous distributed ADMM to handle large amounts\nof training data. However, all existing asynchronous distributed ADMM methods\nare based on full model updates and require locking all global model parameters\nto handle concurrency, which essentially serializes the updates from different\nworkers. In this paper, we present a novel block-wise, asynchronous and\ndistributed ADMM algorithm, which allows different blocks of model parameters\nto be updated in parallel. The lock-free block-wise algorithm may greatly\nspeedup sparse optimization problems, a common scenario in reality, in which\nmost model updates only modify a subset of all decision variables. We\ntheoretically prove the convergence of our proposed algorithm to stationary\npoints for non-convex general form consensus problems with possibly non-smooth\nregularizers. We implement the proposed ADMM algorithm on the Parameter Server\nframework and demonstrate its convergence and near-linear speedup performance\nas the number of workers increases.",
    "text": "A Block-wise, Asynchronous and Distributed ADMM Algorithm for General\n  Form Consensus Optimization\n\nMany machine learning models, including those with non-smooth regularizers,\ncan be formulated as consensus optimization problems, which can be solved by\nthe alternating direction method of multipliers (ADMM). Many recent efforts\nhave been made to develop asynchronous distributed ADMM to handle large amounts\nof training data. However, all existing asynchronous distributed ADMM methods\nare based on full model updates and require locking all global model parameters\nto handle concurrency, which essentially serializes the updates from different\nworkers. In this paper, we present a novel block-wise, asynchronous and\ndistributed ADMM algorithm, which allows different blocks of model parameters\nto be updated in parallel. The lock-free block-wise algorithm may greatly\nspeedup sparse optimization problems, a common scenario in reality, in which\nmost model updates only modify a subset of all decision variables. We\ntheoretically prove the convergence of our proposed algorithm to stationary\npoints for non-convex general form consensus problems with possibly non-smooth\nregularizers. We implement the proposed ADMM algorithm on the Parameter Server\nframework and demonstrate its convergence and near-linear speedup performance\nas the number of workers increases."
  },
  {
    "id": "arxiv-394",
    "title": "Understanding and Accelerating EM Algorithm's Convergence by Fair\n  Competition Principle and Rate-Verisimilitude Function",
    "abstract": "Why can the Expectation-Maximization (EM) algorithm for mixture models\nconverge? Why can different initial parameters cause various convergence\ndifficulties? The Q-L synchronization theory explains that the observed data\nlog-likelihood L and the complete data log-likelihood Q are positively\ncorrelated; we can achieve maximum L by maximizing Q. According to this theory,\nthe Deterministic Annealing EM (DAEM) algorithm's authors make great efforts to\neliminate locally maximal Q for avoiding L's local convergence. However, this\npaper proves that in some cases, Q may and should decrease for L to increase;\nslow or local convergence exists only because of small samples and unfair\ncompetition. This paper uses marriage competition to explain different\nconvergence difficulties and proposes the Fair Competition Principle (FCP) with\nan initialization map for improving initializations. It uses the\nrate-verisimilitude function, extended from the rate-distortion function, to\nexplain the convergence of the EM and improved EM algorithms. This convergence\nproof adopts variational and iterative methods that Shannon et al. used for\nanalyzing rate-distortion functions. The initialization map can vastly save\nboth algorithms' running times for binary Gaussian mixtures. The FCP and the\ninitialization map are useful for complicated mixtures but not sufficient; we\nneed further studies for specific methods.",
    "text": "Understanding and Accelerating EM Algorithm's Convergence by Fair\n  Competition Principle and Rate-Verisimilitude Function\n\nWhy can the Expectation-Maximization (EM) algorithm for mixture models\nconverge? Why can different initial parameters cause various convergence\ndifficulties? The Q-L synchronization theory explains that the observed data\nlog-likelihood L and the complete data log-likelihood Q are positively\ncorrelated; we can achieve maximum L by maximizing Q. According to this theory,\nthe Deterministic Annealing EM (DAEM) algorithm's authors make great efforts to\neliminate locally maximal Q for avoiding L's local convergence. However, this\npaper proves that in some cases, Q may and should decrease for L to increase;\nslow or local convergence exists only because of small samples and unfair\ncompetition. This paper uses marriage competition to explain different\nconvergence difficulties and proposes the Fair Competition Principle (FCP) with\nan initialization map for improving initializations. It uses the\nrate-verisimilitude function, extended from the rate-distortion function, to\nexplain the convergence of the EM and improved EM algorithms. This convergence\nproof adopts variational and iterative methods that Shannon et al. used for\nanalyzing rate-distortion functions. The initialization map can vastly save\nboth algorithms' running times for binary Gaussian mixtures. The FCP and the\ninitialization map are useful for complicated mixtures but not sufficient; we\nneed further studies for specific methods."
  },
  {
    "id": "arxiv-395",
    "title": "Learning-Based sensitivity analysis and feedback design for drug\n  delivery of mixed therapy of cancer in the presence of high model\n  uncertainties",
    "abstract": "In this paper, a methodology is proposed that enables to analyze the\nsensitivity of the outcome of a therapy to unavoidable high dispersion of the\npatient specific parameters on one hand and to the choice of the parameters\nthat define the drug delivery feedback strategy on the other hand. More\nprecisely, a method is given that enables to extract and rank the most influent\nparameters that determine the probability of success/failure of a given\nfeedback therapy for a given set of initial conditions over a cloud of\nrealizations of uncertainties. Moreover predictors of the expectations of the\namounts of drugs being used can also be derived. This enables to design an\nefficient stochastic optimization framework that guarantees safe contraction of\nthe tumor while minimizing a weighted sum of the quantities of the different\ndrugs being used. The framework is illustrated and validated using the example\nof a mixed therapy of cancer involving three combined drugs namely: a\nchemotherapy drug, an immunology vaccine and an immunotherapy drug. Finally, in\nthis specific case, it is shown that dash-boards can be built in the 2D-space\nof the most influent state components that summarize the outcomes'\nprobabilities and the associated drug usage as iso-values curves in the reduced\nstate space.",
    "text": "Learning-Based sensitivity analysis and feedback design for drug\n  delivery of mixed therapy of cancer in the presence of high model\n  uncertainties\n\nIn this paper, a methodology is proposed that enables to analyze the\nsensitivity of the outcome of a therapy to unavoidable high dispersion of the\npatient specific parameters on one hand and to the choice of the parameters\nthat define the drug delivery feedback strategy on the other hand. More\nprecisely, a method is given that enables to extract and rank the most influent\nparameters that determine the probability of success/failure of a given\nfeedback therapy for a given set of initial conditions over a cloud of\nrealizations of uncertainties. Moreover predictors of the expectations of the\namounts of drugs being used can also be derived. This enables to design an\nefficient stochastic optimization framework that guarantees safe contraction of\nthe tumor while minimizing a weighted sum of the quantities of the different\ndrugs being used. The framework is illustrated and validated using the example\nof a mixed therapy of cancer involving three combined drugs namely: a\nchemotherapy drug, an immunology vaccine and an immunotherapy drug. Finally, in\nthis specific case, it is shown that dash-boards can be built in the 2D-space\nof the most influent state components that summarize the outcomes'\nprobabilities and the associated drug usage as iso-values curves in the reduced\nstate space."
  },
  {
    "id": "arxiv-396",
    "title": "Hierarchical Latent Relation Modeling for Collaborative Metric Learning",
    "abstract": "Collaborative Metric Learning (CML) recently emerged as a powerful paradigm\nfor recommendation based on implicit feedback collaborative filtering. However,\nstandard CML methods learn fixed user and item representations, which fails to\ncapture the complex interests of users. Existing extensions of CML also either\nignore the heterogeneity of user-item relations, i.e. that a user can\nsimultaneously like very different items, or the latent item-item relations,\ni.e. that a user's preference for an item depends, not only on its intrinsic\ncharacteristics, but also on items they previously interacted with. In this\npaper, we present a hierarchical CML model that jointly captures latent\nuser-item and item-item relations from implicit data. Our approach is inspired\nby translation mechanisms from knowledge graph embedding and leverages\nmemory-based attention networks. We empirically show the relevance of this\njoint relational modeling, by outperforming existing CML models on\nrecommendation tasks on several real-world datasets. Our experiments also\nemphasize the limits of current CML relational models on very sparse datasets.",
    "text": "Hierarchical Latent Relation Modeling for Collaborative Metric Learning\n\nCollaborative Metric Learning (CML) recently emerged as a powerful paradigm\nfor recommendation based on implicit feedback collaborative filtering. However,\nstandard CML methods learn fixed user and item representations, which fails to\ncapture the complex interests of users. Existing extensions of CML also either\nignore the heterogeneity of user-item relations, i.e. that a user can\nsimultaneously like very different items, or the latent item-item relations,\ni.e. that a user's preference for an item depends, not only on its intrinsic\ncharacteristics, but also on items they previously interacted with. In this\npaper, we present a hierarchical CML model that jointly captures latent\nuser-item and item-item relations from implicit data. Our approach is inspired\nby translation mechanisms from knowledge graph embedding and leverages\nmemory-based attention networks. We empirically show the relevance of this\njoint relational modeling, by outperforming existing CML models on\nrecommendation tasks on several real-world datasets. Our experiments also\nemphasize the limits of current CML relational models on very sparse datasets."
  },
  {
    "id": "arxiv-397",
    "title": "A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning",
    "abstract": "To achieve general intelligence, agents must learn how to interact with\nothers in a shared environment: this is the challenge of multiagent\nreinforcement learning (MARL). The simplest form is independent reinforcement\nlearning (InRL), where each agent treats its experience as part of its\n(non-stationary) environment. In this paper, we first observe that policies\nlearned using InRL can overfit to the other agents' policies during training,\nfailing to sufficiently generalize during execution. We introduce a new metric,\njoint-policy correlation, to quantify this effect. We describe an algorithm for\ngeneral MARL, based on approximate best responses to mixtures of policies\ngenerated using deep reinforcement learning, and empirical game-theoretic\nanalysis to compute meta-strategies for policy selection. The algorithm\ngeneralizes previous ones such as InRL, iterated best response, double oracle,\nand fictitious play. Then, we present a scalable implementation which reduces\nthe memory requirement using decoupled meta-solvers. Finally, we demonstrate\nthe generality of the resulting policies in two partially observable settings:\ngridworld coordination games and poker.",
    "text": "A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning\n\nTo achieve general intelligence, agents must learn how to interact with\nothers in a shared environment: this is the challenge of multiagent\nreinforcement learning (MARL). The simplest form is independent reinforcement\nlearning (InRL), where each agent treats its experience as part of its\n(non-stationary) environment. In this paper, we first observe that policies\nlearned using InRL can overfit to the other agents' policies during training,\nfailing to sufficiently generalize during execution. We introduce a new metric,\njoint-policy correlation, to quantify this effect. We describe an algorithm for\ngeneral MARL, based on approximate best responses to mixtures of policies\ngenerated using deep reinforcement learning, and empirical game-theoretic\nanalysis to compute meta-strategies for policy selection. The algorithm\ngeneralizes previous ones such as InRL, iterated best response, double oracle,\nand fictitious play. Then, we present a scalable implementation which reduces\nthe memory requirement using decoupled meta-solvers. Finally, we demonstrate\nthe generality of the resulting policies in two partially observable settings:\ngridworld coordination games and poker."
  },
  {
    "id": "arxiv-398",
    "title": "Estimating Node Importance in Knowledge Graphs Using Graph Neural\n  Networks",
    "abstract": "How can we estimate the importance of nodes in a knowledge graph (KG)? A KG\nis a multi-relational graph that has proven valuable for many tasks including\nquestion answering and semantic search. In this paper, we present GENI, a\nmethod for tackling the problem of estimating node importance in KGs, which\nenables several downstream applications such as item recommendation and\nresource allocation. While a number of approaches have been developed to\naddress this problem for general graphs, they do not fully utilize information\navailable in KGs, or lack flexibility needed to model complex relationship\nbetween entities and their importance. To address these limitations, we explore\nsupervised machine learning algorithms. In particular, building upon recent\nadvancement of graph neural networks (GNNs), we develop GENI, a GNN-based\nmethod designed to deal with distinctive challenges involved with predicting\nnode importance in KGs. Our method performs an aggregation of importance scores\ninstead of aggregating node embeddings via predicate-aware attention mechanism\nand flexible centrality adjustment. In our evaluation of GENI and existing\nmethods on predicting node importance in real-world KGs with different\ncharacteristics, GENI achieves 5-17% higher NDCG@100 than the state of the art.",
    "text": "Estimating Node Importance in Knowledge Graphs Using Graph Neural\n  Networks\n\nHow can we estimate the importance of nodes in a knowledge graph (KG)? A KG\nis a multi-relational graph that has proven valuable for many tasks including\nquestion answering and semantic search. In this paper, we present GENI, a\nmethod for tackling the problem of estimating node importance in KGs, which\nenables several downstream applications such as item recommendation and\nresource allocation. While a number of approaches have been developed to\naddress this problem for general graphs, they do not fully utilize information\navailable in KGs, or lack flexibility needed to model complex relationship\nbetween entities and their importance. To address these limitations, we explore\nsupervised machine learning algorithms. In particular, building upon recent\nadvancement of graph neural networks (GNNs), we develop GENI, a GNN-based\nmethod designed to deal with distinctive challenges involved with predicting\nnode importance in KGs. Our method performs an aggregation of importance scores\ninstead of aggregating node embeddings via predicate-aware attention mechanism\nand flexible centrality adjustment. In our evaluation of GENI and existing\nmethods on predicting node importance in real-world KGs with different\ncharacteristics, GENI achieves 5-17% higher NDCG@100 than the state of the art."
  },
  {
    "id": "arxiv-399",
    "title": "Learning to Auto Weight: Entirely Data-driven and Highly Efficient\n  Weighting Framework",
    "abstract": "Example weighting algorithm is an effective solution to the training bias\nproblem, however, most previous typical methods are usually limited to human\nknowledge and require laborious tuning of hyperparameters. In this paper, we\npropose a novel example weighting framework called Learning to Auto Weight\n(LAW). The proposed framework finds step-dependent weighting policies\nadaptively, and can be jointly trained with target networks without any\nassumptions or prior knowledge about the dataset. It consists of three key\ncomponents: Stage-based Searching Strategy (3SM) is adopted to shrink the huge\nsearching space in a complete training process; Duplicate Network Reward (DNR)\ngives more accurate supervision by removing randomness during the searching\nprocess; Full Data Update (FDU) further improves the updating efficiency.\nExperimental results demonstrate the superiority of weighting policy explored\nby LAW over standard training pipeline. Compared with baselines, LAW can find a\nbetter weighting schedule which achieves much more superior accuracy on both\nbiased CIFAR and ImageNet.",
    "text": "Learning to Auto Weight: Entirely Data-driven and Highly Efficient\n  Weighting Framework\n\nExample weighting algorithm is an effective solution to the training bias\nproblem, however, most previous typical methods are usually limited to human\nknowledge and require laborious tuning of hyperparameters. In this paper, we\npropose a novel example weighting framework called Learning to Auto Weight\n(LAW). The proposed framework finds step-dependent weighting policies\nadaptively, and can be jointly trained with target networks without any\nassumptions or prior knowledge about the dataset. It consists of three key\ncomponents: Stage-based Searching Strategy (3SM) is adopted to shrink the huge\nsearching space in a complete training process; Duplicate Network Reward (DNR)\ngives more accurate supervision by removing randomness during the searching\nprocess; Full Data Update (FDU) further improves the updating efficiency.\nExperimental results demonstrate the superiority of weighting policy explored\nby LAW over standard training pipeline. Compared with baselines, LAW can find a\nbetter weighting schedule which achieves much more superior accuracy on both\nbiased CIFAR and ImageNet."
  },
  {
    "id": "arxiv-400",
    "title": "On Lipschitz Regularization of Convolutional Layers using Toeplitz\n  Matrix Theory",
    "abstract": "This paper tackles the problem of Lipschitz regularization of Convolutional\nNeural Networks. Lipschitz regularity is now established as a key property of\nmodern deep learning with implications in training stability, generalization,\nrobustness against adversarial examples, etc. However, computing the exact\nvalue of the Lipschitz constant of a neural network is known to be NP-hard.\nRecent attempts from the literature introduce upper bounds to approximate this\nconstant that are either efficient but loose or accurate but computationally\nexpensive. In this work, by leveraging the theory of Toeplitz matrices, we\nintroduce a new upper bound for convolutional layers that is both tight and\neasy to compute. Based on this result we devise an algorithm to train Lipschitz\nregularized Convolutional Neural Networks.",
    "text": "On Lipschitz Regularization of Convolutional Layers using Toeplitz\n  Matrix Theory\n\nThis paper tackles the problem of Lipschitz regularization of Convolutional\nNeural Networks. Lipschitz regularity is now established as a key property of\nmodern deep learning with implications in training stability, generalization,\nrobustness against adversarial examples, etc. However, computing the exact\nvalue of the Lipschitz constant of a neural network is known to be NP-hard.\nRecent attempts from the literature introduce upper bounds to approximate this\nconstant that are either efficient but loose or accurate but computationally\nexpensive. In this work, by leveraging the theory of Toeplitz matrices, we\nintroduce a new upper bound for convolutional layers that is both tight and\neasy to compute. Based on this result we devise an algorithm to train Lipschitz\nregularized Convolutional Neural Networks."
  },
  {
    "id": "arxiv-401",
    "title": "Generalised Gaussian Process Latent Variable Models (GPLVM) with\n  Stochastic Variational Inference",
    "abstract": "Gaussian process latent variable models (GPLVM) are a flexible and non-linear\napproach to dimensionality reduction, extending classical Gaussian processes to\nan unsupervised learning context. The Bayesian incarnation of the GPLVM Titsias\nand Lawrence, 2010] uses a variational framework, where the posterior over\nlatent variables is approximated by a well-behaved variational family, a\nfactorized Gaussian yielding a tractable lower bound. However, the\nnon-factories ability of the lower bound prevents truly scalable inference. In\nthis work, we study the doubly stochastic formulation of the Bayesian GPLVM\nmodel amenable with minibatch training. We show how this framework is\ncompatible with different latent variable formulations and perform experiments\nto compare a suite of models. Further, we demonstrate how we can train in the\npresence of massively missing data and obtain high-fidelity reconstructions. We\ndemonstrate the model's performance by benchmarking against the canonical\nsparse GPLVM for high-dimensional data examples.",
    "text": "Generalised Gaussian Process Latent Variable Models (GPLVM) with\n  Stochastic Variational Inference\n\nGaussian process latent variable models (GPLVM) are a flexible and non-linear\napproach to dimensionality reduction, extending classical Gaussian processes to\nan unsupervised learning context. The Bayesian incarnation of the GPLVM Titsias\nand Lawrence, 2010] uses a variational framework, where the posterior over\nlatent variables is approximated by a well-behaved variational family, a\nfactorized Gaussian yielding a tractable lower bound. However, the\nnon-factories ability of the lower bound prevents truly scalable inference. In\nthis work, we study the doubly stochastic formulation of the Bayesian GPLVM\nmodel amenable with minibatch training. We show how this framework is\ncompatible with different latent variable formulations and perform experiments\nto compare a suite of models. Further, we demonstrate how we can train in the\npresence of massively missing data and obtain high-fidelity reconstructions. We\ndemonstrate the model's performance by benchmarking against the canonical\nsparse GPLVM for high-dimensional data examples."
  },
  {
    "id": "arxiv-402",
    "title": "GANterfactual - Counterfactual Explanations for Medical Non-Experts\n  using Generative Adversarial Learning",
    "abstract": "With the ongoing rise of machine learning, the need for methods for\nexplaining decisions made by artificial intelligence systems is becoming a more\nand more important topic. Especially for image classification tasks, many\nstate-of-the-art tools to explain such classifiers rely on visual highlighting\nof important areas of the input data. Contrary, counterfactual explanation\nsystems try to enable a counterfactual reasoning by modifying the input image\nin a way such that the classifier would have made a different prediction. By\ndoing so, the users of counterfactual explanation systems are equipped with a\ncompletely different kind of explanatory information. However, methods for\ngenerating realistic counterfactual explanations for image classifiers are\nstill rare. Especially in medical contexts, where relevant information often\nconsists of textural and structural information, high-quality counterfactual\nimages have the potential to give meaningful insights into decision processes.\nIn this work, we present GANterfactual, an approach to generate such\ncounterfactual image explanations based on adversarial image-to-image\ntranslation techniques. Additionally, we conduct a user study to evaluate our\napproach in an exemplary medical use case. Our results show that, in the chosen\nmedical use-case, counterfactual explanations lead to significantly better\nresults regarding mental models, explanation satisfaction, trust, emotions, and\nself-efficacy than two state-of-the-art systems that work with saliency maps,\nnamely LIME and LRP.",
    "text": "GANterfactual - Counterfactual Explanations for Medical Non-Experts\n  using Generative Adversarial Learning\n\nWith the ongoing rise of machine learning, the need for methods for\nexplaining decisions made by artificial intelligence systems is becoming a more\nand more important topic. Especially for image classification tasks, many\nstate-of-the-art tools to explain such classifiers rely on visual highlighting\nof important areas of the input data. Contrary, counterfactual explanation\nsystems try to enable a counterfactual reasoning by modifying the input image\nin a way such that the classifier would have made a different prediction. By\ndoing so, the users of counterfactual explanation systems are equipped with a\ncompletely different kind of explanatory information. However, methods for\ngenerating realistic counterfactual explanations for image classifiers are\nstill rare. Especially in medical contexts, where relevant information often\nconsists of textural and structural information, high-quality counterfactual\nimages have the potential to give meaningful insights into decision processes.\nIn this work, we present GANterfactual, an approach to generate such\ncounterfactual image explanations based on adversarial image-to-image\ntranslation techniques. Additionally, we conduct a user study to evaluate our\napproach in an exemplary medical use case. Our results show that, in the chosen\nmedical use-case, counterfactual explanations lead to significantly better\nresults regarding mental models, explanation satisfaction, trust, emotions, and\nself-efficacy than two state-of-the-art systems that work with saliency maps,\nnamely LIME and LRP."
  },
  {
    "id": "arxiv-403",
    "title": "Learning Video Representations from Correspondence Proposals",
    "abstract": "Correspondences between frames encode rich information about dynamic content\nin videos. However, it is challenging to effectively capture and learn those\ndue to their irregular structure and complex dynamics. In this paper, we\npropose a novel neural network that learns video representations by aggregating\ninformation from potential correspondences. This network, named $CPNet$, can\nlearn evolving 2D fields with temporal consistency. In particular, it can\neffectively learn representations for videos by mixing appearance and\nlong-range motion with an RGB-only input. We provide extensive ablation\nexperiments to validate our model. CPNet shows stronger performance than\nexisting methods on Kinetics and achieves the state-of-the-art performance on\nSomething-Something and Jester. We provide analysis towards the behavior of our\nmodel and show its robustness to errors in proposals.",
    "text": "Learning Video Representations from Correspondence Proposals\n\nCorrespondences between frames encode rich information about dynamic content\nin videos. However, it is challenging to effectively capture and learn those\ndue to their irregular structure and complex dynamics. In this paper, we\npropose a novel neural network that learns video representations by aggregating\ninformation from potential correspondences. This network, named $CPNet$, can\nlearn evolving 2D fields with temporal consistency. In particular, it can\neffectively learn representations for videos by mixing appearance and\nlong-range motion with an RGB-only input. We provide extensive ablation\nexperiments to validate our model. CPNet shows stronger performance than\nexisting methods on Kinetics and achieves the state-of-the-art performance on\nSomething-Something and Jester. We provide analysis towards the behavior of our\nmodel and show its robustness to errors in proposals."
  },
  {
    "id": "arxiv-404",
    "title": "New-Onset Diabetes Assessment Using Artificial Intelligence-Enhanced Electrocardiography",
    "abstract": "Undiagnosed diabetes is present in 21.4% of adults with diabetes. Diabetes\ncan remain asymptomatic and undetected due to limitations in screening rates.\nTo address this issue, questionnaires, such as the American Diabetes\nAssociation (ADA) Risk test, have been recommended for use by physicians and\nthe public. Based on evidence that blood glucose concentration can affect\ncardiac electrophysiology, we hypothesized that an artificial intelligence\n(AI)-enhanced electrocardiogram (ECG) could identify adults with new-onset\ndiabetes. We trained a neural network to estimate HbA1c using a 12-lead ECG and\nreadily available demographics. We retrospectively assembled a dataset\ncomprised of patients with paired ECG and HbA1c data. The population of\npatients who receive both an ECG and HbA1c may a biased sample of the complete\noutpatient population, so we adjusted the importance placed on each patient to\ngenerate a more representative pseudo-population. We found ECG-based assessment\noutperforms the ADA Risk test, achieving a higher area under the curve (0.80\nvs. 0.68) and positive predictive value (14% vs. 9%) -- 2.6 times the\nprevalence of diabetes in the cohort. The AI-enhanced ECG significantly\noutperforms electrophysiologist interpretation of the ECG, suggesting that the\ntask is beyond current clinical capabilities. Given the prevalence of ECGs in\nclinics and via wearable devices, such a tool would make precise, automated\ndiabetes assessment widely accessible.",
    "text": "New-Onset Diabetes Assessment Using Artificial Intelligence-Enhanced Electrocardiography\n\nUndiagnosed diabetes is present in 21.4% of adults with diabetes. Diabetes\ncan remain asymptomatic and undetected due to limitations in screening rates.\nTo address this issue, questionnaires, such as the American Diabetes\nAssociation (ADA) Risk test, have been recommended for use by physicians and\nthe public. Based on evidence that blood glucose concentration can affect\ncardiac electrophysiology, we hypothesized that an artificial intelligence\n(AI)-enhanced electrocardiogram (ECG) could identify adults with new-onset\ndiabetes. We trained a neural network to estimate HbA1c using a 12-lead ECG and\nreadily available demographics. We retrospectively assembled a dataset\ncomprised of patients with paired ECG and HbA1c data. The population of\npatients who receive both an ECG and HbA1c may a biased sample of the complete\noutpatient population, so we adjusted the importance placed on each patient to\ngenerate a more representative pseudo-population. We found ECG-based assessment\noutperforms the ADA Risk test, achieving a higher area under the curve (0.80\nvs. 0.68) and positive predictive value (14% vs. 9%) -- 2.6 times the\nprevalence of diabetes in the cohort. The AI-enhanced ECG significantly\noutperforms electrophysiologist interpretation of the ECG, suggesting that the\ntask is beyond current clinical capabilities. Given the prevalence of ECGs in\nclinics and via wearable devices, such a tool would make precise, automated\ndiabetes assessment widely accessible."
  },
  {
    "id": "arxiv-405",
    "title": "Better Depth-Width Trade-offs for Neural Networks through the lens of\n  Dynamical Systems",
    "abstract": "The expressivity of neural networks as a function of their depth, width and\ntype of activation units has been an important question in deep learning\ntheory. Recently, depth separation results for ReLU networks were obtained via\na new connection with dynamical systems, using a generalized notion of fixed\npoints of a continuous map $f$, called periodic points. In this work, we\nstrengthen the connection with dynamical systems and we improve the existing\nwidth lower bounds along several aspects. Our first main result is\nperiod-specific width lower bounds that hold under the stronger notion of\n$L^1$-approximation error, instead of the weaker classification error. Our\nsecond contribution is that we provide sharper width lower bounds, still\nyielding meaningful exponential depth-width separations, in regimes where\nprevious results wouldn't apply. A byproduct of our results is that there\nexists a universal constant characterizing the depth-width trade-offs, as long\nas $f$ has odd periods. Technically, our results follow by unveiling a tighter\nconnection between the following three quantities of a given function: its\nperiod, its Lipschitz constant and the growth rate of the number of\noscillations arising under compositions of the function $f$ with itself.",
    "text": "Better Depth-Width Trade-offs for Neural Networks through the lens of\n  Dynamical Systems\n\nThe expressivity of neural networks as a function of their depth, width and\ntype of activation units has been an important question in deep learning\ntheory. Recently, depth separation results for ReLU networks were obtained via\na new connection with dynamical systems, using a generalized notion of fixed\npoints of a continuous map $f$, called periodic points. In this work, we\nstrengthen the connection with dynamical systems and we improve the existing\nwidth lower bounds along several aspects. Our first main result is\nperiod-specific width lower bounds that hold under the stronger notion of\n$L^1$-approximation error, instead of the weaker classification error. Our\nsecond contribution is that we provide sharper width lower bounds, still\nyielding meaningful exponential depth-width separations, in regimes where\nprevious results wouldn't apply. A byproduct of our results is that there\nexists a universal constant characterizing the depth-width trade-offs, as long\nas $f$ has odd periods. Technically, our results follow by unveiling a tighter\nconnection between the following three quantities of a given function: its\nperiod, its Lipschitz constant and the growth rate of the number of\noscillations arising under compositions of the function $f$ with itself."
  },
  {
    "id": "arxiv-406",
    "title": "A Hierarchical Approach to Multi-Energy Demand Response: From\n  Electricity to Multi-Energy Applications",
    "abstract": "Due to proliferation of energy efficiency measures and availability of the\nrenewable energy resources, traditional energy infrastructure systems\n(electricity, heat, gas) can no longer be operated in a centralized manner\nunder the assumption that consumer behavior is inflexible, i.e. cannot be\nadjusted in return for an adequate incentive. To allow for a less centralized\noperating paradigm, consumer-end perspective and abilities should be integrated\nin current dispatch practices and accounted for in switching between different\nenergy sources not only at the system but also at the individual consumer\nlevel. Since consumers are confined within different built environments, this\npaper looks into an opportunity to control energy consumption of an aggregation\nof many residential, commercial and industrial consumers, into an ensemble.\nThis ensemble control becomes a modern demand response contributor to the set\nof modeling tools for multi-energy infrastructure systems.",
    "text": "A Hierarchical Approach to Multi-Energy Demand Response: From\n  Electricity to Multi-Energy Applications\n\nDue to proliferation of energy efficiency measures and availability of the\nrenewable energy resources, traditional energy infrastructure systems\n(electricity, heat, gas) can no longer be operated in a centralized manner\nunder the assumption that consumer behavior is inflexible, i.e. cannot be\nadjusted in return for an adequate incentive. To allow for a less centralized\noperating paradigm, consumer-end perspective and abilities should be integrated\nin current dispatch practices and accounted for in switching between different\nenergy sources not only at the system but also at the individual consumer\nlevel. Since consumers are confined within different built environments, this\npaper looks into an opportunity to control energy consumption of an aggregation\nof many residential, commercial and industrial consumers, into an ensemble.\nThis ensemble control becomes a modern demand response contributor to the set\nof modeling tools for multi-energy infrastructure systems."
  },
  {
    "id": "arxiv-407",
    "title": "Correlated signal inference by free energy exploration",
    "abstract": "The inference of correlated signal fields with unknown correlation structures\nis of high scientific and technological relevance, but poses significant\nconceptual and numerical challenges. To address these, we develop the\ncorrelated signal inference (CSI) algorithm within information field theory\n(IFT) and discuss its numerical implementation. To this end, we introduce the\nfree energy exploration (FrEE) strategy for numerical information field theory\n(NIFTy) applications. The FrEE strategy is to let the mathematical structure of\nthe inference problem determine the dynamics of the numerical solver. FrEE uses\nthe Gibbs free energy formalism for all involved unknown fields and correlation\nstructures without marginalization of nuisance quantities. It thereby avoids\nthe complexity marginalization often impose to IFT equations. FrEE\nsimultaneously solves for the mean and the uncertainties of signal, nuisance,\nand auxiliary fields, while exploiting any analytically calculable quantity.\nFinally, FrEE uses a problem specific and self-tuning exploration strategy to\nswiftly identify the optimal field estimates as well as their uncertainty maps.\nFor all estimated fields, properly weighted posterior samples drawn from their\nexact, fully non-Gaussian distributions can be generated. Here, we develop the\nFrEE strategies for the CSI of a normal, a log-normal, and a Poisson log-normal\nIFT signal inference problem and demonstrate their performances via their NIFTy\nimplementations.",
    "text": "Correlated signal inference by free energy exploration\n\nThe inference of correlated signal fields with unknown correlation structures\nis of high scientific and technological relevance, but poses significant\nconceptual and numerical challenges. To address these, we develop the\ncorrelated signal inference (CSI) algorithm within information field theory\n(IFT) and discuss its numerical implementation. To this end, we introduce the\nfree energy exploration (FrEE) strategy for numerical information field theory\n(NIFTy) applications. The FrEE strategy is to let the mathematical structure of\nthe inference problem determine the dynamics of the numerical solver. FrEE uses\nthe Gibbs free energy formalism for all involved unknown fields and correlation\nstructures without marginalization of nuisance quantities. It thereby avoids\nthe complexity marginalization often impose to IFT equations. FrEE\nsimultaneously solves for the mean and the uncertainties of signal, nuisance,\nand auxiliary fields, while exploiting any analytically calculable quantity.\nFinally, FrEE uses a problem specific and self-tuning exploration strategy to\nswiftly identify the optimal field estimates as well as their uncertainty maps.\nFor all estimated fields, properly weighted posterior samples drawn from their\nexact, fully non-Gaussian distributions can be generated. Here, we develop the\nFrEE strategies for the CSI of a normal, a log-normal, and a Poisson log-normal\nIFT signal inference problem and demonstrate their performances via their NIFTy\nimplementations."
  },
  {
    "id": "arxiv-408",
    "title": "FlowNet: Learning Optical Flow with Convolutional Networks",
    "abstract": "Convolutional neural networks (CNNs) have recently been very successful in a\nvariety of computer vision tasks, especially on those linked to recognition.\nOptical flow estimation has not been among the tasks where CNNs were\nsuccessful. In this paper we construct appropriate CNNs which are capable of\nsolving the optical flow estimation problem as a supervised learning task. We\npropose and compare two architectures: a generic architecture and another one\nincluding a layer that correlates feature vectors at different image locations.\n  Since existing ground truth data sets are not sufficiently large to train a\nCNN, we generate a synthetic Flying Chairs dataset. We show that networks\ntrained on this unrealistic data still generalize very well to existing\ndatasets such as Sintel and KITTI, achieving competitive accuracy at frame\nrates of 5 to 10 fps.",
    "text": "FlowNet: Learning Optical Flow with Convolutional Networks\n\nConvolutional neural networks (CNNs) have recently been very successful in a\nvariety of computer vision tasks, especially on those linked to recognition.\nOptical flow estimation has not been among the tasks where CNNs were\nsuccessful. In this paper we construct appropriate CNNs which are capable of\nsolving the optical flow estimation problem as a supervised learning task. We\npropose and compare two architectures: a generic architecture and another one\nincluding a layer that correlates feature vectors at different image locations.\n  Since existing ground truth data sets are not sufficiently large to train a\nCNN, we generate a synthetic Flying Chairs dataset. We show that networks\ntrained on this unrealistic data still generalize very well to existing\ndatasets such as Sintel and KITTI, achieving competitive accuracy at frame\nrates of 5 to 10 fps."
  },
  {
    "id": "arxiv-409",
    "title": "MS-nowcasting: Operational Precipitation Nowcasting with Convolutional\n  LSTMs at Microsoft Weather",
    "abstract": "We present the encoder-forecaster convolutional long short-term memory (LSTM)\ndeep-learning model that powers Microsoft Weather's operational precipitation\nnowcasting product. This model takes as input a sequence of weather radar\nmosaics and deterministically predicts future radar reflectivity at lead times\nup to 6 hours. By stacking a large input receptive field along the feature\ndimension and conditioning the model's forecaster with predictions from the\nphysics-based High Resolution Rapid Refresh (HRRR) model, we are able to\noutperform optical flow and HRRR baselines by 20-25% on multiple metrics\naveraged over all lead times.",
    "text": "MS-nowcasting: Operational Precipitation Nowcasting with Convolutional\n  LSTMs at Microsoft Weather\n\nWe present the encoder-forecaster convolutional long short-term memory (LSTM)\ndeep-learning model that powers Microsoft Weather's operational precipitation\nnowcasting product. This model takes as input a sequence of weather radar\nmosaics and deterministically predicts future radar reflectivity at lead times\nup to 6 hours. By stacking a large input receptive field along the feature\ndimension and conditioning the model's forecaster with predictions from the\nphysics-based High Resolution Rapid Refresh (HRRR) model, we are able to\noutperform optical flow and HRRR baselines by 20-25% on multiple metrics\naveraged over all lead times."
  },
  {
    "id": "arxiv-410",
    "title": "A multi-instance deep neural network classifier: application to Higgs\n  boson CP measurement",
    "abstract": "We investigate properties of a classifier applied to the measurements of the\nCP state of the Higgs boson in $H\\rightarrow\\tau\\tau$ decays. The problem is\nframed as binary classifier applied to individual instances. Then the prior\nknowledge that the instances belong to the same class is used to define the\nmulti-instance classifier. Its final score is calculated as multiplication of\nsingle instance scores for a given series of instances. In the paper we discuss\nproperties of such classifier, notably its dependence on the number of\ninstances in the series. This classifier exhibits very strong random dependence\non the number of epochs used for training and requires careful tuning of the\nclassification threshold. We derive formula for this optimal threshold.",
    "text": "A multi-instance deep neural network classifier: application to Higgs\n  boson CP measurement\n\nWe investigate properties of a classifier applied to the measurements of the\nCP state of the Higgs boson in $H\\rightarrow\\tau\\tau$ decays. The problem is\nframed as binary classifier applied to individual instances. Then the prior\nknowledge that the instances belong to the same class is used to define the\nmulti-instance classifier. Its final score is calculated as multiplication of\nsingle instance scores for a given series of instances. In the paper we discuss\nproperties of such classifier, notably its dependence on the number of\ninstances in the series. This classifier exhibits very strong random dependence\non the number of epochs used for training and requires careful tuning of the\nclassification threshold. We derive formula for this optimal threshold."
  },
  {
    "id": "arxiv-411",
    "title": "Overlapping Community Detection with Graph Neural Networks",
    "abstract": "Community detection is a fundamental problem in machine learning. While deep\nlearning has shown great promise in many graphrelated tasks, developing neural\nmodels for community detection has received surprisingly little attention. The\nfew existing approaches focus on detecting disjoint communities, even though\ncommunities in real graphs are well known to be overlapping. We address this\nshortcoming and propose a graph neural network (GNN) based model for\noverlapping community detection. Despite its simplicity, our model outperforms\nthe existing baselines by a large margin in the task of community recovery. We\nestablish through an extensive experimental evaluation that the proposed model\nis effective, scalable and robust to hyperparameter settings. We also perform\nan ablation study that confirms that GNN is the key ingredient to the power of\nthe proposed model.",
    "text": "Overlapping Community Detection with Graph Neural Networks\n\nCommunity detection is a fundamental problem in machine learning. While deep\nlearning has shown great promise in many graphrelated tasks, developing neural\nmodels for community detection has received surprisingly little attention. The\nfew existing approaches focus on detecting disjoint communities, even though\ncommunities in real graphs are well known to be overlapping. We address this\nshortcoming and propose a graph neural network (GNN) based model for\noverlapping community detection. Despite its simplicity, our model outperforms\nthe existing baselines by a large margin in the task of community recovery. We\nestablish through an extensive experimental evaluation that the proposed model\nis effective, scalable and robust to hyperparameter settings. We also perform\nan ablation study that confirms that GNN is the key ingredient to the power of\nthe proposed model."
  },
  {
    "id": "arxiv-412",
    "title": "Deep Structured Prediction with Nonlinear Output Transformations",
    "abstract": "Deep structured models are widely used for tasks like semantic segmentation,\nwhere explicit correlations between variables provide important prior\ninformation which generally helps to reduce the data needs of deep nets.\nHowever, current deep structured models are restricted by oftentimes very local\nneighborhood structure, which cannot be increased for computational complexity\nreasons, and by the fact that the output configuration, or a representation\nthereof, cannot be transformed further. Very recent approaches which address\nthose issues include graphical model inference inside deep nets so as to permit\nsubsequent non-linear output space transformations. However, optimization of\nthose formulations is challenging and not well understood. Here, we develop a\nnovel model which generalizes existing approaches, such as structured\nprediction energy networks, and discuss a formulation which maintains\napplicability of existing inference techniques.",
    "text": "Deep Structured Prediction with Nonlinear Output Transformations\n\nDeep structured models are widely used for tasks like semantic segmentation,\nwhere explicit correlations between variables provide important prior\ninformation which generally helps to reduce the data needs of deep nets.\nHowever, current deep structured models are restricted by oftentimes very local\nneighborhood structure, which cannot be increased for computational complexity\nreasons, and by the fact that the output configuration, or a representation\nthereof, cannot be transformed further. Very recent approaches which address\nthose issues include graphical model inference inside deep nets so as to permit\nsubsequent non-linear output space transformations. However, optimization of\nthose formulations is challenging and not well understood. Here, we develop a\nnovel model which generalizes existing approaches, such as structured\nprediction energy networks, and discuss a formulation which maintains\napplicability of existing inference techniques."
  },
  {
    "id": "arxiv-413",
    "title": "Structure learning for CTBN's via penalized maximum likelihood methods",
    "abstract": "The continuous-time Bayesian networks (CTBNs) represent a class of stochastic\nprocesses, which can be used to model complex phenomena, for instance, they can\ndescribe interactions occurring in living processes, in social science models\nor in medicine. The literature on this topic is usually focused on the case\nwhen the dependence structure of a system is known and we are to determine\nconditional transition intensities (parameters of the network). In the paper,\nwe study the structure learning problem, which is a more challenging task and\nthe existing research on this topic is limited. The approach, which we propose,\nis based on a penalized likelihood method. We prove that our algorithm, under\nmild regularity conditions, recognizes the dependence structure of the graph\nwith high probability. We also investigate the properties of the procedure in\nnumerical studies to demonstrate its effectiveness.",
    "text": "Structure learning for CTBN's via penalized maximum likelihood methods\n\nThe continuous-time Bayesian networks (CTBNs) represent a class of stochastic\nprocesses, which can be used to model complex phenomena, for instance, they can\ndescribe interactions occurring in living processes, in social science models\nor in medicine. The literature on this topic is usually focused on the case\nwhen the dependence structure of a system is known and we are to determine\nconditional transition intensities (parameters of the network). In the paper,\nwe study the structure learning problem, which is a more challenging task and\nthe existing research on this topic is limited. The approach, which we propose,\nis based on a penalized likelihood method. We prove that our algorithm, under\nmild regularity conditions, recognizes the dependence structure of the graph\nwith high probability. We also investigate the properties of the procedure in\nnumerical studies to demonstrate its effectiveness."
  },
  {
    "id": "arxiv-414",
    "title": "Regularized Kernel Recursive Least Square Algoirthm",
    "abstract": "In most adaptive signal processing applications, system linearity is assumed\nand adaptive linear filters are thus used. The traditional class of supervised\nadaptive filters rely on error-correction learning for their adaptive\ncapability. The kernel method is a powerful nonparametric modeling tool for\npattern analysis and statistical signal processing. Through a nonlinear\nmapping, kernel methods transform the data into a set of points in a\nReproducing Kernel Hilbert Space. KRLS achieves high accuracy and has fast\nconvergence rate in stationary scenario. However the good performance is\nobtained at a cost of high computation complexity. Sparsification in kernel\nmethods is know to related to less computational complexity and memory\nconsumption.",
    "text": "Regularized Kernel Recursive Least Square Algoirthm\n\nIn most adaptive signal processing applications, system linearity is assumed\nand adaptive linear filters are thus used. The traditional class of supervised\nadaptive filters rely on error-correction learning for their adaptive\ncapability. The kernel method is a powerful nonparametric modeling tool for\npattern analysis and statistical signal processing. Through a nonlinear\nmapping, kernel methods transform the data into a set of points in a\nReproducing Kernel Hilbert Space. KRLS achieves high accuracy and has fast\nconvergence rate in stationary scenario. However the good performance is\nobtained at a cost of high computation complexity. Sparsification in kernel\nmethods is know to related to less computational complexity and memory\nconsumption."
  },
  {
    "id": "arxiv-415",
    "title": "A Generative Deep Learning Approach to Stochastic Downscaling of\n  Precipitation Forecasts",
    "abstract": "Despite continuous improvements, precipitation forecasts are still not as\naccurate and reliable as those of other meteorological variables. A major\ncontributing factor to this is that several key processes affecting\nprecipitation distribution and intensity occur below the resolved scale of\nglobal weather models. Generative adversarial networks (GANs) have been\ndemonstrated by the computer vision community to be successful at\nsuper-resolution problems, i.e., learning to add fine-scale structure to coarse\nimages. Leinonen et al. (2020) previously applied a GAN to produce ensembles of\nreconstructed high-resolution atmospheric fields, given coarsened input data.\nIn this paper, we demonstrate this approach can be extended to the more\nchallenging problem of increasing the accuracy and resolution of comparatively\nlow-resolution input from a weather forecasting model, using high-resolution\nradar measurements as a \"ground truth\". The neural network must learn to add\nresolution and structure whilst accounting for non-negligible forecast error.\nWe show that GANs and VAE-GANs can match the statistical properties of\nstate-of-the-art pointwise post-processing methods whilst creating\nhigh-resolution, spatially coherent precipitation maps. Our model compares\nfavourably to the best existing downscaling methods in both pixel-wise and\npooled CRPS scores, power spectrum information and rank histograms (used to\nassess calibration). We test our models and show that they perform in a range\nof scenarios, including heavy rainfall.",
    "text": "A Generative Deep Learning Approach to Stochastic Downscaling of\n  Precipitation Forecasts\n\nDespite continuous improvements, precipitation forecasts are still not as\naccurate and reliable as those of other meteorological variables. A major\ncontributing factor to this is that several key processes affecting\nprecipitation distribution and intensity occur below the resolved scale of\nglobal weather models. Generative adversarial networks (GANs) have been\ndemonstrated by the computer vision community to be successful at\nsuper-resolution problems, i.e., learning to add fine-scale structure to coarse\nimages. Leinonen et al. (2020) previously applied a GAN to produce ensembles of\nreconstructed high-resolution atmospheric fields, given coarsened input data.\nIn this paper, we demonstrate this approach can be extended to the more\nchallenging problem of increasing the accuracy and resolution of comparatively\nlow-resolution input from a weather forecasting model, using high-resolution\nradar measurements as a \"ground truth\". The neural network must learn to add\nresolution and structure whilst accounting for non-negligible forecast error.\nWe show that GANs and VAE-GANs can match the statistical properties of\nstate-of-the-art pointwise post-processing methods whilst creating\nhigh-resolution, spatially coherent precipitation maps. Our model compares\nfavourably to the best existing downscaling methods in both pixel-wise and\npooled CRPS scores, power spectrum information and rank histograms (used to\nassess calibration). We test our models and show that they perform in a range\nof scenarios, including heavy rainfall."
  },
  {
    "id": "arxiv-416",
    "title": "A Supervised Feature Selection Method For Mixed-Type Data using\n  Density-based Feature Clustering",
    "abstract": "Feature selection methods are widely used to address the high computational\noverheads and curse of dimensionality in classifying high-dimensional data.\nMost conventional feature selection methods focus on handling homogeneous\nfeatures, while real-world datasets usually have a mixture of continuous and\ndiscrete features. Some recent mixed-type feature selection studies only select\nfeatures with high relevance to class labels and ignore the redundancy among\nfeatures. The determination of an appropriate feature subset is also a\nchallenge. In this paper, a supervised feature selection method using\ndensity-based feature clustering (SFSDFC) is proposed to obtain an appropriate\nfinal feature subset for mixed-type data. SFSDFC decomposes the feature space\ninto a set of disjoint feature clusters using a novel density-based clustering\nmethod. Then, an effective feature selection strategy is employed to obtain a\nsubset of important features with minimal redundancy from those feature\nclusters. Extensive experiments as well as comparison studies with five\nstate-of-the-art methods are conducted on SFSDFC using thirteen real-world\nbenchmark datasets and results justify the efficacy of the SFSDFC method.",
    "text": "A Supervised Feature Selection Method For Mixed-Type Data using\n  Density-based Feature Clustering\n\nFeature selection methods are widely used to address the high computational\noverheads and curse of dimensionality in classifying high-dimensional data.\nMost conventional feature selection methods focus on handling homogeneous\nfeatures, while real-world datasets usually have a mixture of continuous and\ndiscrete features. Some recent mixed-type feature selection studies only select\nfeatures with high relevance to class labels and ignore the redundancy among\nfeatures. The determination of an appropriate feature subset is also a\nchallenge. In this paper, a supervised feature selection method using\ndensity-based feature clustering (SFSDFC) is proposed to obtain an appropriate\nfinal feature subset for mixed-type data. SFSDFC decomposes the feature space\ninto a set of disjoint feature clusters using a novel density-based clustering\nmethod. Then, an effective feature selection strategy is employed to obtain a\nsubset of important features with minimal redundancy from those feature\nclusters. Extensive experiments as well as comparison studies with five\nstate-of-the-art methods are conducted on SFSDFC using thirteen real-world\nbenchmark datasets and results justify the efficacy of the SFSDFC method."
  },
  {
    "id": "arxiv-417",
    "title": "Robust Text-to-SQL Generation with Execution-Guided Decoding",
    "abstract": "We consider the problem of neural semantic parsing, which translates natural\nlanguage questions into executable SQL queries. We introduce a new mechanism,\nexecution guidance, to leverage the semantics of SQL. It detects and excludes\nfaulty programs during the decoding procedure by conditioning on the execution\nof partially generated program. The mechanism can be used with any\nautoregressive generative model, which we demonstrate on four state-of-the-art\nrecurrent or template-based semantic parsing models. We demonstrate that\nexecution guidance universally improves model performance on various\ntext-to-SQL datasets with different scales and query complexity: WikiSQL, ATIS,\nand GeoQuery. As a result, we achieve new state-of-the-art execution accuracy\nof 83.8% on WikiSQL.",
    "text": "Robust Text-to-SQL Generation with Execution-Guided Decoding\n\nWe consider the problem of neural semantic parsing, which translates natural\nlanguage questions into executable SQL queries. We introduce a new mechanism,\nexecution guidance, to leverage the semantics of SQL. It detects and excludes\nfaulty programs during the decoding procedure by conditioning on the execution\nof partially generated program. The mechanism can be used with any\nautoregressive generative model, which we demonstrate on four state-of-the-art\nrecurrent or template-based semantic parsing models. We demonstrate that\nexecution guidance universally improves model performance on various\ntext-to-SQL datasets with different scales and query complexity: WikiSQL, ATIS,\nand GeoQuery. As a result, we achieve new state-of-the-art execution accuracy\nof 83.8% on WikiSQL."
  },
  {
    "id": "arxiv-418",
    "title": "Self-Supervised Neural Architecture Search for Imbalanced Datasets",
    "abstract": "Neural Architecture Search (NAS) provides state-of-the-art results when\ntrained on well-curated datasets with annotated labels. However, annotating\ndata or even having balanced number of samples can be a luxury for\npractitioners from different scientific fields, e.g., in the medical domain. To\nthat end, we propose a NAS-based framework that bears the threefold\ncontributions: (a) we focus on the self-supervised scenario, i.e., where no\nlabels are required to determine the architecture, and (b) we assume the\ndatasets are imbalanced, (c) we design each component to be able to run on a\nresource constrained setup, i.e., on a single GPU (e.g. Google Colab). Our\ncomponents build on top of recent developments in self-supervised\nlearning~\\citep{zbontar2021barlow}, self-supervised NAS~\\citep{kaplan2020self}\nand extend them for the case of imbalanced datasets. We conduct experiments on\nan (artificially) imbalanced version of CIFAR-10 and we demonstrate our\nproposed method outperforms standard neural networks, while using $27\\times$\nless parameters. To validate our assumption on a naturally imbalanced dataset,\nwe also conduct experiments on ChestMNIST and COVID-19 X-ray. The results\ndemonstrate how the proposed method can be used in imbalanced datasets, while\nit can be fully run on a single GPU. Code is available\n\\href{https://github.com/TimofeevAlex/ssnas_imbalanced}{here}.",
    "text": "Self-Supervised Neural Architecture Search for Imbalanced Datasets\n\nNeural Architecture Search (NAS) provides state-of-the-art results when\ntrained on well-curated datasets with annotated labels. However, annotating\ndata or even having balanced number of samples can be a luxury for\npractitioners from different scientific fields, e.g., in the medical domain. To\nthat end, we propose a NAS-based framework that bears the threefold\ncontributions: (a) we focus on the self-supervised scenario, i.e., where no\nlabels are required to determine the architecture, and (b) we assume the\ndatasets are imbalanced, (c) we design each component to be able to run on a\nresource constrained setup, i.e., on a single GPU (e.g. Google Colab). Our\ncomponents build on top of recent developments in self-supervised\nlearning~\\citep{zbontar2021barlow}, self-supervised NAS~\\citep{kaplan2020self}\nand extend them for the case of imbalanced datasets. We conduct experiments on\nan (artificially) imbalanced version of CIFAR-10 and we demonstrate our\nproposed method outperforms standard neural networks, while using $27\\times$\nless parameters. To validate our assumption on a naturally imbalanced dataset,\nwe also conduct experiments on ChestMNIST and COVID-19 X-ray. The results\ndemonstrate how the proposed method can be used in imbalanced datasets, while\nit can be fully run on a single GPU. Code is available\n\\href{https://github.com/TimofeevAlex/ssnas_imbalanced}{here}."
  },
  {
    "id": "arxiv-419",
    "title": "Dueling RL: Reinforcement Learning with Trajectory Preferences",
    "abstract": "We consider the problem of preference based reinforcement learning (PbRL),\nwhere, unlike traditional reinforcement learning, an agent receives feedback\nonly in terms of a 1 bit (0/1) preference over a trajectory pair instead of\nabsolute rewards for them. The success of the traditional RL framework\ncrucially relies on the underlying agent-reward model, which, however, depends\non how accurately a system designer can express an appropriate reward function\nand often a non-trivial task. The main novelty of our framework is the ability\nto learn from preference-based trajectory feedback that eliminates the need to\nhand-craft numeric reward models. This paper sets up a formal framework for the\nPbRL problem with non-markovian rewards, where the trajectory preferences are\nencoded by a generalized linear model of dimension $d$. Assuming the transition\nmodel is known, we then propose an algorithm with almost optimal regret\nguarantee of $\\tilde {\\mathcal{O}}\\left( SH d \\log (T / \\delta) \\sqrt{T}\n\\right)$. We further, extend the above algorithm to the case of unknown\ntransition dynamics, and provide an algorithm with near optimal regret\nguarantee $\\widetilde{\\mathcal{O}}((\\sqrt{d} + H^2 + |\\mathcal{S}|)\\sqrt{dT}\n+\\sqrt{|\\mathcal{S}||\\mathcal{A}|TH} )$. To the best of our knowledge, our work\nis one of the first to give tight regret guarantees for preference based RL\nproblems with trajectory preferences.",
    "text": "Dueling RL: Reinforcement Learning with Trajectory Preferences\n\nWe consider the problem of preference based reinforcement learning (PbRL),\nwhere, unlike traditional reinforcement learning, an agent receives feedback\nonly in terms of a 1 bit (0/1) preference over a trajectory pair instead of\nabsolute rewards for them. The success of the traditional RL framework\ncrucially relies on the underlying agent-reward model, which, however, depends\non how accurately a system designer can express an appropriate reward function\nand often a non-trivial task. The main novelty of our framework is the ability\nto learn from preference-based trajectory feedback that eliminates the need to\nhand-craft numeric reward models. This paper sets up a formal framework for the\nPbRL problem with non-markovian rewards, where the trajectory preferences are\nencoded by a generalized linear model of dimension $d$. Assuming the transition\nmodel is known, we then propose an algorithm with almost optimal regret\nguarantee of $\\tilde {\\mathcal{O}}\\left( SH d \\log (T / \\delta) \\sqrt{T}\n\\right)$. We further, extend the above algorithm to the case of unknown\ntransition dynamics, and provide an algorithm with near optimal regret\nguarantee $\\widetilde{\\mathcal{O}}((\\sqrt{d} + H^2 + |\\mathcal{S}|)\\sqrt{dT}\n+\\sqrt{|\\mathcal{S}||\\mathcal{A}|TH} )$. To the best of our knowledge, our work\nis one of the first to give tight regret guarantees for preference based RL\nproblems with trajectory preferences."
  },
  {
    "id": "arxiv-420",
    "title": "learn2learn: A Library for Meta-Learning Research",
    "abstract": "Meta-learning researchers face two fundamental issues in their empirical\nwork: prototyping and reproducibility. Researchers are prone to make mistakes\nwhen prototyping new algorithms and tasks because modern meta-learning methods\nrely on unconventional functionalities of machine learning frameworks. In turn,\nreproducing existing results becomes a tedious endeavour -- a situation\nexacerbated by the lack of standardized implementations and benchmarks. As a\nresult, researchers spend inordinate amounts of time on implementing software\nrather than understanding and developing new ideas.\n  This manuscript introduces learn2learn, a library for meta-learning research\nfocused on solving those prototyping and reproducibility issues. learn2learn\nprovides low-level routines common across a wide-range of meta-learning\ntechniques (e.g. meta-descent, meta-reinforcement learning, few-shot learning),\nand builds standardized interfaces to algorithms and benchmarks on top of them.\nIn releasing learn2learn under a free and open source license, we hope to\nfoster a community around standardized software for meta-learning research.",
    "text": "learn2learn: A Library for Meta-Learning Research\n\nMeta-learning researchers face two fundamental issues in their empirical\nwork: prototyping and reproducibility. Researchers are prone to make mistakes\nwhen prototyping new algorithms and tasks because modern meta-learning methods\nrely on unconventional functionalities of machine learning frameworks. In turn,\nreproducing existing results becomes a tedious endeavour -- a situation\nexacerbated by the lack of standardized implementations and benchmarks. As a\nresult, researchers spend inordinate amounts of time on implementing software\nrather than understanding and developing new ideas.\n  This manuscript introduces learn2learn, a library for meta-learning research\nfocused on solving those prototyping and reproducibility issues. learn2learn\nprovides low-level routines common across a wide-range of meta-learning\ntechniques (e.g. meta-descent, meta-reinforcement learning, few-shot learning),\nand builds standardized interfaces to algorithms and benchmarks on top of them.\nIn releasing learn2learn under a free and open source license, we hope to\nfoster a community around standardized software for meta-learning research."
  },
  {
    "id": "arxiv-421",
    "title": "A 14uJ/Decision Keyword Spotting Accelerator with In-SRAM-Computing and\n  On Chip Learning for Customization",
    "abstract": "Keyword spotting has gained popularity as a natural way to interact with\nconsumer devices in recent years. However, because of its always-on nature and\nthe variety of speech, it necessitates a low-power design as well as user\ncustomization. This paper describes a low-power, energy-efficient keyword\nspotting accelerator with SRAM based in-memory computing (IMC) and on-chip\nlearning for user customization. However, IMC is constrained by macro size,\nlimited precision, and non-ideal effects. To address the issues mentioned\nabove, this paper proposes bias compensation and fine-tuning using an IMC-aware\nmodel design. Furthermore, because learning with low-precision edge devices\nresults in zero error and gradient values due to quantization, this paper\nproposes error scaling and small gradient accumulation to achieve the same\naccuracy as ideal model training. The simulation results show that with user\ncustomization, we can recover the accuracy loss from 51.08\\% to 89.76\\% with\ncompensation and fine-tuning and further improve to 96.71\\% with customization.\nThe chip implementation can successfully run the model with only 14$uJ$ per\ndecision. When compared to the state-of-the-art works, the presented design has\nhigher energy efficiency with additional on-chip model customization\ncapabilities for higher accuracy.",
    "text": "A 14uJ/Decision Keyword Spotting Accelerator with In-SRAM-Computing and\n  On Chip Learning for Customization\n\nKeyword spotting has gained popularity as a natural way to interact with\nconsumer devices in recent years. However, because of its always-on nature and\nthe variety of speech, it necessitates a low-power design as well as user\ncustomization. This paper describes a low-power, energy-efficient keyword\nspotting accelerator with SRAM based in-memory computing (IMC) and on-chip\nlearning for user customization. However, IMC is constrained by macro size,\nlimited precision, and non-ideal effects. To address the issues mentioned\nabove, this paper proposes bias compensation and fine-tuning using an IMC-aware\nmodel design. Furthermore, because learning with low-precision edge devices\nresults in zero error and gradient values due to quantization, this paper\nproposes error scaling and small gradient accumulation to achieve the same\naccuracy as ideal model training. The simulation results show that with user\ncustomization, we can recover the accuracy loss from 51.08\\% to 89.76\\% with\ncompensation and fine-tuning and further improve to 96.71\\% with customization.\nThe chip implementation can successfully run the model with only 14$uJ$ per\ndecision. When compared to the state-of-the-art works, the presented design has\nhigher energy efficiency with additional on-chip model customization\ncapabilities for higher accuracy."
  },
  {
    "id": "arxiv-422",
    "title": "Certified Patch Robustness via Smoothed Vision Transformers",
    "abstract": "Certified patch defenses can guarantee robustness of an image classifier to\narbitrary changes within a bounded contiguous region. But, currently, this\nrobustness comes at a cost of degraded standard accuracies and slower inference\ntimes. We demonstrate how using vision transformers enables significantly\nbetter certified patch robustness that is also more computationally efficient\nand does not incur a substantial drop in standard accuracy. These improvements\nstem from the inherent ability of the vision transformer to gracefully handle\nlargely masked images. Our code is available at\nhttps://github.com/MadryLab/smoothed-vit.",
    "text": "Certified Patch Robustness via Smoothed Vision Transformers\n\nCertified patch defenses can guarantee robustness of an image classifier to\narbitrary changes within a bounded contiguous region. But, currently, this\nrobustness comes at a cost of degraded standard accuracies and slower inference\ntimes. We demonstrate how using vision transformers enables significantly\nbetter certified patch robustness that is also more computationally efficient\nand does not incur a substantial drop in standard accuracy. These improvements\nstem from the inherent ability of the vision transformer to gracefully handle\nlargely masked images. Our code is available at\nhttps://github.com/MadryLab/smoothed-vit."
  },
  {
    "id": "arxiv-423",
    "title": "Design Considerations for High Impact, Automated Echocardiogram Analysis",
    "abstract": "Deep learning has the potential to automate echocardiogram analysis for early\ndetection of heart disease. Based on a qualitative analysis of design concerns,\nthis study suggests that predicting normal heart function instead of disease\naccounts for data quality bias and significantly increases efficiency in\ncardiologists' workflows.",
    "text": "Design Considerations for High Impact, Automated Echocardiogram Analysis\n\nDeep learning has the potential to automate echocardiogram analysis for early\ndetection of heart disease. Based on a qualitative analysis of design concerns,\nthis study suggests that predicting normal heart function instead of disease\naccounts for data quality bias and significantly increases efficiency in\ncardiologists' workflows."
  },
  {
    "id": "arxiv-424",
    "title": "DwNet: Dense warp-based network for pose-guided human video generation",
    "abstract": "Generation of realistic high-resolution videos of human subjects is a\nchallenging and important task in computer vision. In this paper, we focus on\nhuman motion transfer - generation of a video depicting a particular subject,\nobserved in a single image, performing a series of motions exemplified by an\nauxiliary (driving) video. Our GAN-based architecture, DwNet, leverages dense\nintermediate pose-guided representation and refinement process to warp the\nrequired subject appearance, in the form of the texture, from a source image\ninto a desired pose. Temporal consistency is maintained by further conditioning\nthe decoding process within a GAN on the previously generated frame. In this\nway a video is generated in an iterative and recurrent fashion. We illustrate\nthe efficacy of our approach by showing state-of-the-art quantitative and\nqualitative performance on two benchmark datasets: TaiChi and Fashion Modeling.\nThe latter is collected by us and will be made publicly available to the\ncommunity.",
    "text": "DwNet: Dense warp-based network for pose-guided human video generation\n\nGeneration of realistic high-resolution videos of human subjects is a\nchallenging and important task in computer vision. In this paper, we focus on\nhuman motion transfer - generation of a video depicting a particular subject,\nobserved in a single image, performing a series of motions exemplified by an\nauxiliary (driving) video. Our GAN-based architecture, DwNet, leverages dense\nintermediate pose-guided representation and refinement process to warp the\nrequired subject appearance, in the form of the texture, from a source image\ninto a desired pose. Temporal consistency is maintained by further conditioning\nthe decoding process within a GAN on the previously generated frame. In this\nway a video is generated in an iterative and recurrent fashion. We illustrate\nthe efficacy of our approach by showing state-of-the-art quantitative and\nqualitative performance on two benchmark datasets: TaiChi and Fashion Modeling.\nThe latter is collected by us and will be made publicly available to the\ncommunity."
  },
  {
    "id": "arxiv-425",
    "title": "Decentralized Differentially Private Without-Replacement Stochastic\n  Gradient Descent",
    "abstract": "While machine learning has achieved remarkable results in a wide variety of\ndomains, the training of models often requires large datasets that may need to\nbe collected from different individuals. As sensitive information may be\ncontained in the individual's dataset, sharing training data may lead to severe\nprivacy concerns. Therefore, there is a compelling need to develop\nprivacy-aware machine learning methods, for which one effective approach is to\nleverage the generic framework of differential privacy. Considering that\nstochastic gradient descent (SGD) is one of the mostly adopted methods for\nlarge-scale machine learning problems, two decentralized differentially private\nSGD algorithms are proposed in this work. Particularly, we focus on SGD without\nreplacement due to its favorable structure for practical implementation. In\naddition, both privacy and convergence analysis are provided for the proposed\nalgorithms. Finally, extensive experiments are performed to verify the\ntheoretical results and demonstrate the effectiveness of the proposed\nalgorithms.",
    "text": "Decentralized Differentially Private Without-Replacement Stochastic\n  Gradient Descent\n\nWhile machine learning has achieved remarkable results in a wide variety of\ndomains, the training of models often requires large datasets that may need to\nbe collected from different individuals. As sensitive information may be\ncontained in the individual's dataset, sharing training data may lead to severe\nprivacy concerns. Therefore, there is a compelling need to develop\nprivacy-aware machine learning methods, for which one effective approach is to\nleverage the generic framework of differential privacy. Considering that\nstochastic gradient descent (SGD) is one of the mostly adopted methods for\nlarge-scale machine learning problems, two decentralized differentially private\nSGD algorithms are proposed in this work. Particularly, we focus on SGD without\nreplacement due to its favorable structure for practical implementation. In\naddition, both privacy and convergence analysis are provided for the proposed\nalgorithms. Finally, extensive experiments are performed to verify the\ntheoretical results and demonstrate the effectiveness of the proposed\nalgorithms."
  },
  {
    "id": "arxiv-426",
    "title": "Meta-Reinforcement Learning for Heuristic Planning",
    "abstract": "In Meta-Reinforcement Learning (meta-RL) an agent is trained on a set of\ntasks to prepare for and learn faster in new, unseen, but related tasks. The\ntraining tasks are usually hand-crafted to be representative of the expected\ndistribution of test tasks and hence all used in training. We show that given a\nset of training tasks, learning can be both faster and more effective (leading\nto better performance in the test tasks), if the training tasks are\nappropriately selected. We propose a task selection algorithm,\nInformation-Theoretic Task Selection (ITTS), based on information theory, which\noptimizes the set of tasks used for training in meta-RL, irrespectively of how\nthey are generated. The algorithm establishes which training tasks are both\nsufficiently relevant for the test tasks, and different enough from one\nanother. We reproduce different meta-RL experiments from the literature and\nshow that ITTS improves the final performance in all of them.",
    "text": "Meta-Reinforcement Learning for Heuristic Planning\n\nIn Meta-Reinforcement Learning (meta-RL) an agent is trained on a set of\ntasks to prepare for and learn faster in new, unseen, but related tasks. The\ntraining tasks are usually hand-crafted to be representative of the expected\ndistribution of test tasks and hence all used in training. We show that given a\nset of training tasks, learning can be both faster and more effective (leading\nto better performance in the test tasks), if the training tasks are\nappropriately selected. We propose a task selection algorithm,\nInformation-Theoretic Task Selection (ITTS), based on information theory, which\noptimizes the set of tasks used for training in meta-RL, irrespectively of how\nthey are generated. The algorithm establishes which training tasks are both\nsufficiently relevant for the test tasks, and different enough from one\nanother. We reproduce different meta-RL experiments from the literature and\nshow that ITTS improves the final performance in all of them."
  },
  {
    "id": "arxiv-427",
    "title": "The Theoretical Expressiveness of Maxpooling",
    "abstract": "Over the decade since deep neural networks became state of the art image\nclassifiers there has been a tendency towards less use of max pooling: the\nfunction that takes the largest of nearby pixels in an image. Since max pooling\nfeatured prominently in earlier generations of image classifiers, we wish to\nunderstand this trend, and whether it is justified. We develop a theoretical\nframework analyzing ReLU based approximations to max pooling, and prove a sense\nin which max pooling cannot be efficiently replicated using ReLU activations.\nWe analyze the error of a class of optimal approximations, and find that whilst\nthe error can be made exponentially small in the kernel size, doing so requires\nan exponentially complex approximation.\n  Our work gives a theoretical basis for understanding the trend away from max\npooling in newer architectures. We conclude that the main cause of a difference\nbetween max pooling and an optimal approximation, a prevalent large difference\nbetween the max and other values within pools, can be overcome with other\narchitectural decisions, or is not prevalent in natural images.",
    "text": "The Theoretical Expressiveness of Maxpooling\n\nOver the decade since deep neural networks became state of the art image\nclassifiers there has been a tendency towards less use of max pooling: the\nfunction that takes the largest of nearby pixels in an image. Since max pooling\nfeatured prominently in earlier generations of image classifiers, we wish to\nunderstand this trend, and whether it is justified. We develop a theoretical\nframework analyzing ReLU based approximations to max pooling, and prove a sense\nin which max pooling cannot be efficiently replicated using ReLU activations.\nWe analyze the error of a class of optimal approximations, and find that whilst\nthe error can be made exponentially small in the kernel size, doing so requires\nan exponentially complex approximation.\n  Our work gives a theoretical basis for understanding the trend away from max\npooling in newer architectures. We conclude that the main cause of a difference\nbetween max pooling and an optimal approximation, a prevalent large difference\nbetween the max and other values within pools, can be overcome with other\narchitectural decisions, or is not prevalent in natural images."
  },
  {
    "id": "arxiv-428",
    "title": "Gradient Boosted Normalizing Flows",
    "abstract": "By chaining a sequence of differentiable invertible transformations,\nnormalizing flows (NF) provide an expressive method of posterior approximation,\nexact density evaluation, and sampling. The trend in normalizing flow\nliterature has been to devise deeper, more complex transformations to achieve\ngreater flexibility. We propose an alternative: Gradient Boosted Normalizing\nFlows (GBNF) model a density by successively adding new NF components with\ngradient boosting. Under the boosting framework, each new NF component\noptimizes a sample weighted likelihood objective, resulting in new components\nthat are fit to the residuals of the previously trained components. The GBNF\nformulation results in a mixture model structure, whose flexibility increases\nas more components are added. Moreover, GBNFs offer a wider, as opposed to\nstrictly deeper, approach that improves existing NFs at the cost of additional\ntraining---not more complex transformations. We demonstrate the effectiveness\nof this technique for density estimation and, by coupling GBNF with a\nvariational autoencoder, generative modeling of images. Our results show that\nGBNFs outperform their non-boosted analog, and, in some cases, produce better\nresults with smaller, simpler flows.",
    "text": "Gradient Boosted Normalizing Flows\n\nBy chaining a sequence of differentiable invertible transformations,\nnormalizing flows (NF) provide an expressive method of posterior approximation,\nexact density evaluation, and sampling. The trend in normalizing flow\nliterature has been to devise deeper, more complex transformations to achieve\ngreater flexibility. We propose an alternative: Gradient Boosted Normalizing\nFlows (GBNF) model a density by successively adding new NF components with\ngradient boosting. Under the boosting framework, each new NF component\noptimizes a sample weighted likelihood objective, resulting in new components\nthat are fit to the residuals of the previously trained components. The GBNF\nformulation results in a mixture model structure, whose flexibility increases\nas more components are added. Moreover, GBNFs offer a wider, as opposed to\nstrictly deeper, approach that improves existing NFs at the cost of additional\ntraining---not more complex transformations. We demonstrate the effectiveness\nof this technique for density estimation and, by coupling GBNF with a\nvariational autoencoder, generative modeling of images. Our results show that\nGBNFs outperform their non-boosted analog, and, in some cases, produce better\nresults with smaller, simpler flows."
  },
  {
    "id": "arxiv-429",
    "title": "Human irrationality: both bad and good for reward inference",
    "abstract": "Assuming humans are (approximately) rational enables robots to infer reward\nfunctions by observing human behavior. But people exhibit a wide array of\nirrationalities, and our goal with this work is to better understand the effect\nthey can have on reward inference. The challenge with studying this effect is\nthat there are many types of irrationality, with varying degrees of\nmathematical formalization. We thus operationalize irrationality in the\nlanguage of MDPs, by altering the Bellman optimality equation, and use this\nframework to study how these alterations would affect inference.\n  We find that wrongly modeling a systematically irrational human as\nnoisy-rational performs a lot worse than correctly capturing these biases -- so\nmuch so that it can be better to skip inference altogether and stick to the\nprior! More importantly, we show that an irrational human, when correctly\nmodelled, can communicate more information about the reward than a perfectly\nrational human can. That is, if a robot has the correct model of a human's\nirrationality, it can make an even stronger inference than it ever could if the\nhuman were rational. Irrationality fundamentally helps rather than hinder\nreward inference, but it needs to be correctly accounted for.",
    "text": "Human irrationality: both bad and good for reward inference\n\nAssuming humans are (approximately) rational enables robots to infer reward\nfunctions by observing human behavior. But people exhibit a wide array of\nirrationalities, and our goal with this work is to better understand the effect\nthey can have on reward inference. The challenge with studying this effect is\nthat there are many types of irrationality, with varying degrees of\nmathematical formalization. We thus operationalize irrationality in the\nlanguage of MDPs, by altering the Bellman optimality equation, and use this\nframework to study how these alterations would affect inference.\n  We find that wrongly modeling a systematically irrational human as\nnoisy-rational performs a lot worse than correctly capturing these biases -- so\nmuch so that it can be better to skip inference altogether and stick to the\nprior! More importantly, we show that an irrational human, when correctly\nmodelled, can communicate more information about the reward than a perfectly\nrational human can. That is, if a robot has the correct model of a human's\nirrationality, it can make an even stronger inference than it ever could if the\nhuman were rational. Irrationality fundamentally helps rather than hinder\nreward inference, but it needs to be correctly accounted for."
  },
  {
    "id": "arxiv-430",
    "title": "AAG: Self-Supervised Representation Learning by Auxiliary Augmentation\n  with GNT-Xent Loss",
    "abstract": "Self-supervised representation learning is an emerging research topic for its\npowerful capacity in learning with unlabeled data. As a mainstream\nself-supervised learning method, augmentation-based contrastive learning has\nachieved great success in various computer vision tasks that lack manual\nannotations. Despite current progress, the existing methods are often limited\nby extra cost on memory or storage, and their performance still has large room\nfor improvement. Here we present a self-supervised representation learning\nmethod, namely AAG, which is featured by an auxiliary augmentation strategy and\nGNT-Xent loss. The auxiliary augmentation is able to promote the performance of\ncontrastive learning by increasing the diversity of images. The proposed\nGNT-Xent loss enables a steady and fast training process and yields competitive\naccuracy. Experiment results demonstrate the superiority of AAG to previous\nstate-of-the-art methods on CIFAR10, CIFAR100, and SVHN. Especially, AAG\nachieves 94.5% top-1 accuracy on CIFAR10 with batch size 64, which is 0.5%\nhigher than the best result of SimCLR with batch size 1024.",
    "text": "AAG: Self-Supervised Representation Learning by Auxiliary Augmentation\n  with GNT-Xent Loss\n\nSelf-supervised representation learning is an emerging research topic for its\npowerful capacity in learning with unlabeled data. As a mainstream\nself-supervised learning method, augmentation-based contrastive learning has\nachieved great success in various computer vision tasks that lack manual\nannotations. Despite current progress, the existing methods are often limited\nby extra cost on memory or storage, and their performance still has large room\nfor improvement. Here we present a self-supervised representation learning\nmethod, namely AAG, which is featured by an auxiliary augmentation strategy and\nGNT-Xent loss. The auxiliary augmentation is able to promote the performance of\ncontrastive learning by increasing the diversity of images. The proposed\nGNT-Xent loss enables a steady and fast training process and yields competitive\naccuracy. Experiment results demonstrate the superiority of AAG to previous\nstate-of-the-art methods on CIFAR10, CIFAR100, and SVHN. Especially, AAG\nachieves 94.5% top-1 accuracy on CIFAR10 with batch size 64, which is 0.5%\nhigher than the best result of SimCLR with batch size 1024."
  },
  {
    "id": "arxiv-431",
    "title": "Shedding some light on Light Up with Artificial Intelligence",
    "abstract": "The Light-Up puzzle, also known as the AKARI puzzle, has never been solved\nusing modern artificial intelligence (AI) methods. Currently, the most widely\nused computational technique to autonomously develop solutions involve\nevolution theory algorithms. This project is an effort to apply new AI\ntechniques for solving the Light-up puzzle faster and more computationally\nefficient. The algorithms explored for producing optimal solutions include hill\nclimbing, simulated annealing, feed-forward neural network (FNN), and\nconvolutional neural network (CNN). Two algorithms were developed for hill\nclimbing and simulated annealing using 2 actions (add and remove light bulb)\nversus 3 actions(add, remove, or move light-bulb to a different cell). Both\nhill climbing and simulated annealing algorithms showed a higher accuracy for\nthe case of 3 actions. The simulated annealing showed to significantly\noutperform hill climbing, FNN, CNN, and an evolutionary theory algorithm\nachieving 100% accuracy in 30 unique board configurations. Lastly, while FNN\nand CNN algorithms showed low accuracies, computational times were\nsignificantly faster compared to the remaining algorithms. The GitHub\nrepository for this project can be found at\nhttps://github.com/rperera12/AKARI-LightUp-GameSolver-with-DeepNeuralNetworks-and-HillClimb-or-SimulatedAnnealing.",
    "text": "Shedding some light on Light Up with Artificial Intelligence\n\nThe Light-Up puzzle, also known as the AKARI puzzle, has never been solved\nusing modern artificial intelligence (AI) methods. Currently, the most widely\nused computational technique to autonomously develop solutions involve\nevolution theory algorithms. This project is an effort to apply new AI\ntechniques for solving the Light-up puzzle faster and more computationally\nefficient. The algorithms explored for producing optimal solutions include hill\nclimbing, simulated annealing, feed-forward neural network (FNN), and\nconvolutional neural network (CNN). Two algorithms were developed for hill\nclimbing and simulated annealing using 2 actions (add and remove light bulb)\nversus 3 actions(add, remove, or move light-bulb to a different cell). Both\nhill climbing and simulated annealing algorithms showed a higher accuracy for\nthe case of 3 actions. The simulated annealing showed to significantly\noutperform hill climbing, FNN, CNN, and an evolutionary theory algorithm\nachieving 100% accuracy in 30 unique board configurations. Lastly, while FNN\nand CNN algorithms showed low accuracies, computational times were\nsignificantly faster compared to the remaining algorithms. The GitHub\nrepository for this project can be found at\nhttps://github.com/rperera12/AKARI-LightUp-GameSolver-with-DeepNeuralNetworks-and-HillClimb-or-SimulatedAnnealing."
  },
  {
    "id": "arxiv-432",
    "title": "Process, Bias and Temperature Scalable CMOS Analog Computing Circuits for Machine Learning",
    "abstract": "Analog computing is attractive compared to digital computing due to its\npotential for achieving higher computational density and higher energy\nefficiency. However, unlike digital circuits, conventional analog computing\ncircuits cannot be easily mapped across different process nodes due to\ndifferences in transistor biasing regimes, temperature variations and limited\ndynamic range. In this work, we generalize the previously reported\nmargin-propagation-based analog computing framework for designing novel\n\\textit{shape-based analog computing} (S-AC) circuits that can be easily\ncross-mapped across different process nodes. Similar to digital designs S-AC\ndesigns can also be scaled for precision, speed, and power. As a\nproof-of-concept, we show several examples of S-AC circuits implementing\nmathematical functions that are commonly used in machine learning (ML)\narchitectures. Using circuit simulations we demonstrate that the circuit\ninput/output characteristics remain robust when mapped from a planar CMOS 180nm\nprocess to a FinFET 7nm process. Also, using benchmark datasets we demonstrate\nthat the classification accuracy of a S-AC based neural network remains robust\nwhen mapped across the two processes and to changes in temperature.",
    "text": "Process, Bias and Temperature Scalable CMOS Analog Computing Circuits for Machine Learning\n\nAnalog computing is attractive compared to digital computing due to its\npotential for achieving higher computational density and higher energy\nefficiency. However, unlike digital circuits, conventional analog computing\ncircuits cannot be easily mapped across different process nodes due to\ndifferences in transistor biasing regimes, temperature variations and limited\ndynamic range. In this work, we generalize the previously reported\nmargin-propagation-based analog computing framework for designing novel\n\\textit{shape-based analog computing} (S-AC) circuits that can be easily\ncross-mapped across different process nodes. Similar to digital designs S-AC\ndesigns can also be scaled for precision, speed, and power. As a\nproof-of-concept, we show several examples of S-AC circuits implementing\nmathematical functions that are commonly used in machine learning (ML)\narchitectures. Using circuit simulations we demonstrate that the circuit\ninput/output characteristics remain robust when mapped from a planar CMOS 180nm\nprocess to a FinFET 7nm process. Also, using benchmark datasets we demonstrate\nthat the classification accuracy of a S-AC based neural network remains robust\nwhen mapped across the two processes and to changes in temperature."
  },
  {
    "id": "arxiv-433",
    "title": "Open Set Domain Adaptation using Optimal Transport",
    "abstract": "We present a 2-step optimal transport approach that performs a mapping from a\nsource distribution to a target distribution. Here, the target has the\nparticularity to present new classes not present in the source domain. The\nfirst step of the approach aims at rejecting the samples issued from these new\nclasses using an optimal transport plan. The second step solves the target\n(class ratio) shift still as an optimal transport problem. We develop a dual\napproach to solve the optimization problem involved at each step and we prove\nthat our results outperform recent state-of-the-art performances. We further\napply the approach to the setting where the source and target distributions\npresent both a label-shift and an increasing covariate (features) shift to show\nits robustness.",
    "text": "Open Set Domain Adaptation using Optimal Transport\n\nWe present a 2-step optimal transport approach that performs a mapping from a\nsource distribution to a target distribution. Here, the target has the\nparticularity to present new classes not present in the source domain. The\nfirst step of the approach aims at rejecting the samples issued from these new\nclasses using an optimal transport plan. The second step solves the target\n(class ratio) shift still as an optimal transport problem. We develop a dual\napproach to solve the optimization problem involved at each step and we prove\nthat our results outperform recent state-of-the-art performances. We further\napply the approach to the setting where the source and target distributions\npresent both a label-shift and an increasing covariate (features) shift to show\nits robustness."
  },
  {
    "id": "arxiv-434",
    "title": "XEM: An Explainable-by-Design Ensemble Method for Multivariate Time\n  Series Classification",
    "abstract": "We present XEM, an eXplainable-by-design Ensemble method for Multivariate\ntime series classification. XEM relies on a new hybrid ensemble method that\ncombines an explicit boosting-bagging approach to handle the bias-variance\ntrade-off faced by machine learning models and an implicit divide-and-conquer\napproach to individualize classifier errors on different parts of the training\ndata. Our evaluation shows that XEM outperforms the state-of-the-art MTS\nclassifiers on the public UEA datasets. Furthermore, XEM provides faithful\nexplainability-by-design and manifests robust performance when faced with\nchallenges arising from continuous data collection (different MTS length,\nmissing data and noise).",
    "text": "XEM: An Explainable-by-Design Ensemble Method for Multivariate Time\n  Series Classification\n\nWe present XEM, an eXplainable-by-design Ensemble method for Multivariate\ntime series classification. XEM relies on a new hybrid ensemble method that\ncombines an explicit boosting-bagging approach to handle the bias-variance\ntrade-off faced by machine learning models and an implicit divide-and-conquer\napproach to individualize classifier errors on different parts of the training\ndata. Our evaluation shows that XEM outperforms the state-of-the-art MTS\nclassifiers on the public UEA datasets. Furthermore, XEM provides faithful\nexplainability-by-design and manifests robust performance when faced with\nchallenges arising from continuous data collection (different MTS length,\nmissing data and noise)."
  },
  {
    "id": "arxiv-435",
    "title": "Defending Observation Attacks in Deep Reinforcement Learning via Detection and Denoising",
    "abstract": "Neural network policies trained using Deep Reinforcement Learning (DRL) are\nwell-known to be susceptible to adversarial attacks. In this paper, we consider\nattacks manifesting as perturbations in the observation space managed by the\nexternal environment. These attacks have been shown to downgrade policy\nperformance significantly. We focus our attention on well-trained deterministic\nand stochastic neural network policies in the context of continuous control\nbenchmarks subject to four well-studied observation space adversarial attacks.\nTo defend against these attacks, we propose a novel defense strategy using a\ndetect-and-denoise schema. Unlike previous adversarial training approaches that\nsample data in adversarial scenarios, our solution does not require sampling\ndata in an environment under attack, thereby greatly reducing risk during\ntraining. Detailed experimental results show that our technique is comparable\nwith state-of-the-art adversarial training approaches.",
    "text": "Defending Observation Attacks in Deep Reinforcement Learning via Detection and Denoising\n\nNeural network policies trained using Deep Reinforcement Learning (DRL) are\nwell-known to be susceptible to adversarial attacks. In this paper, we consider\nattacks manifesting as perturbations in the observation space managed by the\nexternal environment. These attacks have been shown to downgrade policy\nperformance significantly. We focus our attention on well-trained deterministic\nand stochastic neural network policies in the context of continuous control\nbenchmarks subject to four well-studied observation space adversarial attacks.\nTo defend against these attacks, we propose a novel defense strategy using a\ndetect-and-denoise schema. Unlike previous adversarial training approaches that\nsample data in adversarial scenarios, our solution does not require sampling\ndata in an environment under attack, thereby greatly reducing risk during\ntraining. Detailed experimental results show that our technique is comparable\nwith state-of-the-art adversarial training approaches."
  },
  {
    "id": "arxiv-436",
    "title": "A New Approach for Interpretability and Reliability in Clinical Risk\n  Prediction: Acute Coronary Syndrome Scenario",
    "abstract": "We intend to create a new risk assessment methodology that combines the best\ncharacteristics of both risk score and machine learning models. More\nspecifically, we aim to develop a method that, besides having a good\nperformance, offers a personalized model and outcome for each patient, presents\nhigh interpretability, and incorporates an estimation of the prediction\nreliability which is not usually available. By combining these features in the\nsame approach we expect that it can boost the confidence of physicians to use\nsuch a tool in their daily activity. In order to achieve the mentioned goals, a\nthree-step methodology was developed: several rules were created by\ndichotomizing risk factors; such rules were trained with a machine learning\nclassifier to predict the acceptance degree of each rule (the probability that\nthe rule is correct) for each patient; that information was combined and used\nto compute the risk of mortality and the reliability of such prediction. The\nmethodology was applied to a dataset of patients admitted with any type of\nacute coronary syndromes (ACS), to assess the 30-days all-cause mortality risk.\nThe performance was compared with state-of-the-art approaches: logistic\nregression (LR), artificial neural network (ANN), and clinical risk score model\n(Global Registry of Acute Coronary Events - GRACE). The proposed approach\nachieved testing results identical to the standard LR, but offers superior\ninterpretability and personalization; it also significantly outperforms the\nGRACE risk model and the standard ANN model. The calibration curve also\nsuggests a very good generalization ability of the obtained model as it\napproaches the ideal curve. Finally, the reliability estimation of individual\npredictions presented a great correlation with the misclassifications rate.\nThose properties may have a beneficial application in other clinical scenarios\nas well. [abridged]",
    "text": "A New Approach for Interpretability and Reliability in Clinical Risk\n  Prediction: Acute Coronary Syndrome Scenario\n\nWe intend to create a new risk assessment methodology that combines the best\ncharacteristics of both risk score and machine learning models. More\nspecifically, we aim to develop a method that, besides having a good\nperformance, offers a personalized model and outcome for each patient, presents\nhigh interpretability, and incorporates an estimation of the prediction\nreliability which is not usually available. By combining these features in the\nsame approach we expect that it can boost the confidence of physicians to use\nsuch a tool in their daily activity. In order to achieve the mentioned goals, a\nthree-step methodology was developed: several rules were created by\ndichotomizing risk factors; such rules were trained with a machine learning\nclassifier to predict the acceptance degree of each rule (the probability that\nthe rule is correct) for each patient; that information was combined and used\nto compute the risk of mortality and the reliability of such prediction. The\nmethodology was applied to a dataset of patients admitted with any type of\nacute coronary syndromes (ACS), to assess the 30-days all-cause mortality risk.\nThe performance was compared with state-of-the-art approaches: logistic\nregression (LR), artificial neural network (ANN), and clinical risk score model\n(Global Registry of Acute Coronary Events - GRACE). The proposed approach\nachieved testing results identical to the standard LR, but offers superior\ninterpretability and personalization; it also significantly outperforms the\nGRACE risk model and the standard ANN model. The calibration curve also\nsuggests a very good generalization ability of the obtained model as it\napproaches the ideal curve. Finally, the reliability estimation of individual\npredictions presented a great correlation with the misclassifications rate.\nThose properties may have a beneficial application in other clinical scenarios\nas well. [abridged]"
  },
  {
    "id": "arxiv-437",
    "title": "Fast Projection onto the Capped Simplex with Applications to Sparse\n  Regression in Bioinformatics",
    "abstract": "We consider the problem of projecting a vector onto the so-called k-capped\nsimplex, which is a hyper-cube cut by a hyperplane. For an n-dimensional input\nvector with bounded elements, we found that a simple algorithm based on\nNewton's method is able to solve the projection problem to high precision with\na complexity roughly about O(n), which has a much lower computational cost\ncompared with the existing sorting-based methods proposed in the literature. We\nprovide a theory for partial explanation and justification of the method.\n  We demonstrate that the proposed algorithm can produce a solution of the\nprojection problem with high precision on large scale datasets, and the\nalgorithm is able to significantly outperform the state-of-the-art methods in\nterms of runtime (about 6-8 times faster than a commercial software with\nrespect to CPU time for input vector with 1 million variables or more).\n  We further illustrate the effectiveness of the proposed algorithm on solving\nsparse regression in a bioinformatics problem. Empirical results on the GWAS\ndataset (with 1,500,000 single-nucleotide polymorphisms) show that, when using\nthe proposed method to accelerate the Projected Quasi-Newton (PQN) method, the\naccelerated PQN algorithm is able to handle huge-scale regression problem and\nit is more efficient (about 3-6 times faster) than the current state-of-the-art\nmethods.",
    "text": "Fast Projection onto the Capped Simplex with Applications to Sparse\n  Regression in Bioinformatics\n\nWe consider the problem of projecting a vector onto the so-called k-capped\nsimplex, which is a hyper-cube cut by a hyperplane. For an n-dimensional input\nvector with bounded elements, we found that a simple algorithm based on\nNewton's method is able to solve the projection problem to high precision with\na complexity roughly about O(n), which has a much lower computational cost\ncompared with the existing sorting-based methods proposed in the literature. We\nprovide a theory for partial explanation and justification of the method.\n  We demonstrate that the proposed algorithm can produce a solution of the\nprojection problem with high precision on large scale datasets, and the\nalgorithm is able to significantly outperform the state-of-the-art methods in\nterms of runtime (about 6-8 times faster than a commercial software with\nrespect to CPU time for input vector with 1 million variables or more).\n  We further illustrate the effectiveness of the proposed algorithm on solving\nsparse regression in a bioinformatics problem. Empirical results on the GWAS\ndataset (with 1,500,000 single-nucleotide polymorphisms) show that, when using\nthe proposed method to accelerate the Projected Quasi-Newton (PQN) method, the\naccelerated PQN algorithm is able to handle huge-scale regression problem and\nit is more efficient (about 3-6 times faster) than the current state-of-the-art\nmethods."
  },
  {
    "id": "arxiv-438",
    "title": "Towards Expressive Graph Representation",
    "abstract": "Graph Neural Network (GNN) aggregates the neighborhood of each node into the\nnode embedding and shows its powerful capability for graph representation\nlearning. However, most existing GNN variants aggregate the neighborhood\ninformation in a fixed non-injective fashion, which may map different graphs or\nnodes to the same embedding, reducing the model expressiveness. We present a\ntheoretical framework to design a continuous injective set function for\nneighborhood aggregation in GNN. Using the framework, we propose expressive GNN\nthat aggregates the neighborhood of each node with a continuous injective set\nfunction, so that a GNN layer maps similar nodes with similar neighborhoods to\nsimilar embeddings, different nodes to different embeddings and the equivalent\nnodes or isomorphic graphs to the same embeddings. Moreover, the proposed\nexpressive GNN can naturally learn expressive representations for graphs with\ncontinuous node attributes. We validate the proposed expressive GNN (ExpGNN)\nfor graph classification on multiple benchmark datasets including simple graphs\nand attributed graphs. The experimental results demonstrate that our model\nachieves state-of-the-art performances on most of the benchmarks.",
    "text": "Towards Expressive Graph Representation\n\nGraph Neural Network (GNN) aggregates the neighborhood of each node into the\nnode embedding and shows its powerful capability for graph representation\nlearning. However, most existing GNN variants aggregate the neighborhood\ninformation in a fixed non-injective fashion, which may map different graphs or\nnodes to the same embedding, reducing the model expressiveness. We present a\ntheoretical framework to design a continuous injective set function for\nneighborhood aggregation in GNN. Using the framework, we propose expressive GNN\nthat aggregates the neighborhood of each node with a continuous injective set\nfunction, so that a GNN layer maps similar nodes with similar neighborhoods to\nsimilar embeddings, different nodes to different embeddings and the equivalent\nnodes or isomorphic graphs to the same embeddings. Moreover, the proposed\nexpressive GNN can naturally learn expressive representations for graphs with\ncontinuous node attributes. We validate the proposed expressive GNN (ExpGNN)\nfor graph classification on multiple benchmark datasets including simple graphs\nand attributed graphs. The experimental results demonstrate that our model\nachieves state-of-the-art performances on most of the benchmarks."
  },
  {
    "id": "arxiv-439",
    "title": "Transformer Memory as a Differentiable Search Index",
    "abstract": "In this paper, we demonstrate that information retrieval can be accomplished\nwith a single Transformer, in which all information about the corpus is encoded\nin the parameters of the model. To this end, we introduce the Differentiable\nSearch Index (DSI), a new paradigm that learns a text-to-text model that maps\nstring queries directly to relevant docids; in other words, a DSI model answers\nqueries directly using only its parameters, dramatically simplifying the whole\nretrieval process. We study variations in how documents and their identifiers\nare represented, variations in training procedures, and the interplay between\nmodels and corpus sizes. Experiments demonstrate that given appropriate design\nchoices, DSI significantly outperforms strong baselines such as dual encoder\nmodels. Moreover, DSI demonstrates strong generalization capabilities,\noutperforming a BM25 baseline in a zero-shot setup.",
    "text": "Transformer Memory as a Differentiable Search Index\n\nIn this paper, we demonstrate that information retrieval can be accomplished\nwith a single Transformer, in which all information about the corpus is encoded\nin the parameters of the model. To this end, we introduce the Differentiable\nSearch Index (DSI), a new paradigm that learns a text-to-text model that maps\nstring queries directly to relevant docids; in other words, a DSI model answers\nqueries directly using only its parameters, dramatically simplifying the whole\nretrieval process. We study variations in how documents and their identifiers\nare represented, variations in training procedures, and the interplay between\nmodels and corpus sizes. Experiments demonstrate that given appropriate design\nchoices, DSI significantly outperforms strong baselines such as dual encoder\nmodels. Moreover, DSI demonstrates strong generalization capabilities,\noutperforming a BM25 baseline in a zero-shot setup."
  },
  {
    "id": "arxiv-440",
    "title": "Robust Optimization as Data Augmentation for Large-scale Graphs",
    "abstract": "Data augmentation helps neural networks generalize better by enlarging the\ntraining set, but it remains an open question how to effectively augment graph\ndata to enhance the performance of GNNs (Graph Neural Networks). While most\nexisting graph regularizers focus on manipulating graph topological structures\nby adding/removing edges, we offer a method to augment node features for better\nperformance. We propose FLAG (Free Large-scale Adversarial Augmentation on\nGraphs), which iteratively augments node features with gradient-based\nadversarial perturbations during training. By making the model invariant to\nsmall fluctuations in input data, our method helps models generalize to\nout-of-distribution samples and boosts model performance at test time. FLAG is\na general-purpose approach for graph data, which universally works in node\nclassification, link prediction, and graph classification tasks. FLAG is also\nhighly flexible and scalable, and is deployable with arbitrary GNN backbones\nand large-scale datasets. We demonstrate the efficacy and stability of our\nmethod through extensive experiments and ablation studies. We also provide\nintuitive observations for a deeper understanding of our method.",
    "text": "Robust Optimization as Data Augmentation for Large-scale Graphs\n\nData augmentation helps neural networks generalize better by enlarging the\ntraining set, but it remains an open question how to effectively augment graph\ndata to enhance the performance of GNNs (Graph Neural Networks). While most\nexisting graph regularizers focus on manipulating graph topological structures\nby adding/removing edges, we offer a method to augment node features for better\nperformance. We propose FLAG (Free Large-scale Adversarial Augmentation on\nGraphs), which iteratively augments node features with gradient-based\nadversarial perturbations during training. By making the model invariant to\nsmall fluctuations in input data, our method helps models generalize to\nout-of-distribution samples and boosts model performance at test time. FLAG is\na general-purpose approach for graph data, which universally works in node\nclassification, link prediction, and graph classification tasks. FLAG is also\nhighly flexible and scalable, and is deployable with arbitrary GNN backbones\nand large-scale datasets. We demonstrate the efficacy and stability of our\nmethod through extensive experiments and ablation studies. We also provide\nintuitive observations for a deeper understanding of our method."
  },
  {
    "id": "arxiv-441",
    "title": "FairGAN: Fairness-aware Generative Adversarial Networks",
    "abstract": "Fairness-aware learning is increasingly important in data mining.\nDiscrimination prevention aims to prevent discrimination in the training data\nbefore it is used to conduct predictive analysis. In this paper, we focus on\nfair data generation that ensures the generated data is discrimination free.\nInspired by generative adversarial networks (GAN), we present fairness-aware\ngenerative adversarial networks, called FairGAN, which are able to learn a\ngenerator producing fair data and also preserving good data utility. Compared\nwith the naive fair data generation models, FairGAN further ensures the\nclassifiers which are trained on generated data can achieve fair classification\non real data. Experiments on a real dataset show the effectiveness of FairGAN.",
    "text": "FairGAN: Fairness-aware Generative Adversarial Networks\n\nFairness-aware learning is increasingly important in data mining.\nDiscrimination prevention aims to prevent discrimination in the training data\nbefore it is used to conduct predictive analysis. In this paper, we focus on\nfair data generation that ensures the generated data is discrimination free.\nInspired by generative adversarial networks (GAN), we present fairness-aware\ngenerative adversarial networks, called FairGAN, which are able to learn a\ngenerator producing fair data and also preserving good data utility. Compared\nwith the naive fair data generation models, FairGAN further ensures the\nclassifiers which are trained on generated data can achieve fair classification\non real data. Experiments on a real dataset show the effectiveness of FairGAN."
  },
  {
    "id": "arxiv-442",
    "title": "Multimodal Abstractive Summarization for How2 Videos",
    "abstract": "In this paper, we study abstractive summarization for open-domain videos.\nUnlike the traditional text news summarization, the goal is less to \"compress\"\ntext information but rather to provide a fluent textual summary of information\nthat has been collected and fused from different source modalities, in our case\nvideo and audio transcripts (or text). We show how a multi-source\nsequence-to-sequence model with hierarchical attention can integrate\ninformation from different modalities into a coherent output, compare various\nmodels trained with different modalities and present pilot experiments on the\nHow2 corpus of instructional videos. We also propose a new evaluation metric\n(Content F1) for abstractive summarization task that measures semantic adequacy\nrather than fluency of the summaries, which is covered by metrics like ROUGE\nand BLEU.",
    "text": "Multimodal Abstractive Summarization for How2 Videos\n\nIn this paper, we study abstractive summarization for open-domain videos.\nUnlike the traditional text news summarization, the goal is less to \"compress\"\ntext information but rather to provide a fluent textual summary of information\nthat has been collected and fused from different source modalities, in our case\nvideo and audio transcripts (or text). We show how a multi-source\nsequence-to-sequence model with hierarchical attention can integrate\ninformation from different modalities into a coherent output, compare various\nmodels trained with different modalities and present pilot experiments on the\nHow2 corpus of instructional videos. We also propose a new evaluation metric\n(Content F1) for abstractive summarization task that measures semantic adequacy\nrather than fluency of the summaries, which is covered by metrics like ROUGE\nand BLEU."
  },
  {
    "id": "arxiv-443",
    "title": "Provably Efficient Model-Free Constrained RL with Linear Function Approximation",
    "abstract": "We study the constrained reinforcement learning problem, in which an agent\naims to maximize the expected cumulative reward subject to a constraint on the\nexpected total value of a utility function. In contrast to existing model-based\napproaches or model-free methods accompanied with a `simulator', we aim to\ndevelop the first model-free, simulator-free algorithm that achieves a\nsublinear regret and a sublinear constraint violation even in large-scale\nsystems. To this end, we consider the episodic constrained Markov decision\nprocesses with linear function approximation, where the transition dynamics and\nthe reward function can be represented as a linear function of some known\nfeature mapping. We show that $\\tilde{\\mathcal{O}}(\\sqrt{d^3H^3T})$ regret and\n$\\tilde{\\mathcal{O}}(\\sqrt{d^3H^3T})$ constraint violation bounds can be\nachieved, where $d$ is the dimension of the feature mapping, $H$ is the length\nof the episode, and $T$ is the total number of steps. Our bounds are attained\nwithout explicitly estimating the unknown transition model or requiring a\nsimulator, and they depend on the state space only through the dimension of the\nfeature mapping. Hence our bounds hold even when the number of states goes to\ninfinity. Our main results are achieved via novel adaptations of the standard\nLSVI-UCB algorithms. In particular, we first introduce primal-dual optimization\ninto the LSVI-UCB algorithm to balance between regret and constraint violation.\nMore importantly, we replace the standard greedy selection with respect to the\nstate-action function in LSVI-UCB with a soft-max policy. This turns out to be\nkey in establishing uniform concentration for the constrained case via its\napproximation-smoothness trade-off. We also show that one can achieve an even\nzero constraint violation while still maintaining the same order with respect\nto $T$.",
    "text": "Provably Efficient Model-Free Constrained RL with Linear Function Approximation\n\nWe study the constrained reinforcement learning problem, in which an agent\naims to maximize the expected cumulative reward subject to a constraint on the\nexpected total value of a utility function. In contrast to existing model-based\napproaches or model-free methods accompanied with a `simulator', we aim to\ndevelop the first model-free, simulator-free algorithm that achieves a\nsublinear regret and a sublinear constraint violation even in large-scale\nsystems. To this end, we consider the episodic constrained Markov decision\nprocesses with linear function approximation, where the transition dynamics and\nthe reward function can be represented as a linear function of some known\nfeature mapping. We show that $\\tilde{\\mathcal{O}}(\\sqrt{d^3H^3T})$ regret and\n$\\tilde{\\mathcal{O}}(\\sqrt{d^3H^3T})$ constraint violation bounds can be\nachieved, where $d$ is the dimension of the feature mapping, $H$ is the length\nof the episode, and $T$ is the total number of steps. Our bounds are attained\nwithout explicitly estimating the unknown transition model or requiring a\nsimulator, and they depend on the state space only through the dimension of the\nfeature mapping. Hence our bounds hold even when the number of states goes to\ninfinity. Our main results are achieved via novel adaptations of the standard\nLSVI-UCB algorithms. In particular, we first introduce primal-dual optimization\ninto the LSVI-UCB algorithm to balance between regret and constraint violation.\nMore importantly, we replace the standard greedy selection with respect to the\nstate-action function in LSVI-UCB with a soft-max policy. This turns out to be\nkey in establishing uniform concentration for the constrained case via its\napproximation-smoothness trade-off. We also show that one can achieve an even\nzero constraint violation while still maintaining the same order with respect\nto $T$."
  },
  {
    "id": "arxiv-444",
    "title": "Neural Bayes: A Generic Parameterization Method for Unsupervised\n  Representation Learning",
    "abstract": "We introduce a parameterization method called Neural Bayes which allows\ncomputing statistical quantities that are in general difficult to compute and\nopens avenues for formulating new objectives for unsupervised representation\nlearning. Specifically, given an observed random variable $\\mathbf{x}$ and a\nlatent discrete variable $z$, we can express $p(\\mathbf{x}|z)$,\n$p(z|\\mathbf{x})$ and $p(z)$ in closed form in terms of a sufficiently\nexpressive function (Eg. neural network) using our parameterization without\nrestricting the class of these distributions. To demonstrate its usefulness, we\ndevelop two independent use cases for this parameterization:\n  1. Mutual Information Maximization (MIM): MIM has become a popular means for\nself-supervised representation learning. Neural Bayes allows us to compute\nmutual information between observed random variables $\\mathbf{x}$ and latent\ndiscrete random variables $z$ in closed form. We use this for learning image\nrepresentations and show its usefulness on downstream classification tasks.\n  2. Disjoint Manifold Labeling: Neural Bayes allows us to formulate an\nobjective which can optimally label samples from disjoint manifolds present in\nthe support of a continuous distribution. This can be seen as a specific form\nof clustering where each disjoint manifold in the support is a separate\ncluster. We design clustering tasks that obey this formulation and empirically\nshow that the model optimally labels the disjoint manifolds. Our code is\navailable at \\url{https://github.com/salesforce/NeuralBayes}",
    "text": "Neural Bayes: A Generic Parameterization Method for Unsupervised\n  Representation Learning\n\nWe introduce a parameterization method called Neural Bayes which allows\ncomputing statistical quantities that are in general difficult to compute and\nopens avenues for formulating new objectives for unsupervised representation\nlearning. Specifically, given an observed random variable $\\mathbf{x}$ and a\nlatent discrete variable $z$, we can express $p(\\mathbf{x}|z)$,\n$p(z|\\mathbf{x})$ and $p(z)$ in closed form in terms of a sufficiently\nexpressive function (Eg. neural network) using our parameterization without\nrestricting the class of these distributions. To demonstrate its usefulness, we\ndevelop two independent use cases for this parameterization:\n  1. Mutual Information Maximization (MIM): MIM has become a popular means for\nself-supervised representation learning. Neural Bayes allows us to compute\nmutual information between observed random variables $\\mathbf{x}$ and latent\ndiscrete random variables $z$ in closed form. We use this for learning image\nrepresentations and show its usefulness on downstream classification tasks.\n  2. Disjoint Manifold Labeling: Neural Bayes allows us to formulate an\nobjective which can optimally label samples from disjoint manifolds present in\nthe support of a continuous distribution. This can be seen as a specific form\nof clustering where each disjoint manifold in the support is a separate\ncluster. We design clustering tasks that obey this formulation and empirically\nshow that the model optimally labels the disjoint manifolds. Our code is\navailable at \\url{https://github.com/salesforce/NeuralBayes}"
  },
  {
    "id": "arxiv-445",
    "title": "Ideological Sublations: Resolution of Dialectic in Population-based\n  Optimization",
    "abstract": "A population-based optimization algorithm was designed, inspired by two main\nthinking modes in philosophy, both based on dialectic concept and\nthesis-antithesis paradigm. They impose two different kinds of dialectics.\nIdealistic and materialistic antitheses are formulated as optimization models.\nBased on the models, the population is coordinated for dialectical\ninteractions. At the population-based context, the formulated optimization\nmodels are reduced to a simple detection problem for each thinker (particle).\nAccording to the assigned thinking mode to each thinker and her/his\nmeasurements of corresponding dialectic with other candidate particles, they\ndeterministically decide to interact with a thinker in maximum dialectic with\ntheir theses. The position of a thinker at maximum dialectic is known as an\navailable antithesis among the existing solutions. The dialectical interactions\nat each ideological community are distinguished by meaningful distributions of\nstep-sizes for each thinking mode. In fact, the thinking modes are regarded as\nexploration and exploitation elements of the proposed algorithm. The result is\na delicate balance without any requirement for adjustment of step-size\ncoefficients. Main parameter of the proposed algorithm is the number of\nparticles appointed to each thinking modes, or equivalently for each kind of\nmotions. An additional integer parameter is defined to boost the stability of\nthe final algorithm in some particular problems. The proposed algorithm is\nevaluated by a testbed of 12 single-objective continuous benchmark functions.\nMoreover, its performance and speed were highlighted in sparse reconstruction\nand antenna selection problems, at the context of compressed sensing and\nmassive MIMO, respectively. The results indicate fast and efficient performance\nin comparison with well-known evolutionary algorithms and dedicated\nstate-of-the-art algorithms.",
    "text": "Ideological Sublations: Resolution of Dialectic in Population-based\n  Optimization\n\nA population-based optimization algorithm was designed, inspired by two main\nthinking modes in philosophy, both based on dialectic concept and\nthesis-antithesis paradigm. They impose two different kinds of dialectics.\nIdealistic and materialistic antitheses are formulated as optimization models.\nBased on the models, the population is coordinated for dialectical\ninteractions. At the population-based context, the formulated optimization\nmodels are reduced to a simple detection problem for each thinker (particle).\nAccording to the assigned thinking mode to each thinker and her/his\nmeasurements of corresponding dialectic with other candidate particles, they\ndeterministically decide to interact with a thinker in maximum dialectic with\ntheir theses. The position of a thinker at maximum dialectic is known as an\navailable antithesis among the existing solutions. The dialectical interactions\nat each ideological community are distinguished by meaningful distributions of\nstep-sizes for each thinking mode. In fact, the thinking modes are regarded as\nexploration and exploitation elements of the proposed algorithm. The result is\na delicate balance without any requirement for adjustment of step-size\ncoefficients. Main parameter of the proposed algorithm is the number of\nparticles appointed to each thinking modes, or equivalently for each kind of\nmotions. An additional integer parameter is defined to boost the stability of\nthe final algorithm in some particular problems. The proposed algorithm is\nevaluated by a testbed of 12 single-objective continuous benchmark functions.\nMoreover, its performance and speed were highlighted in sparse reconstruction\nand antenna selection problems, at the context of compressed sensing and\nmassive MIMO, respectively. The results indicate fast and efficient performance\nin comparison with well-known evolutionary algorithms and dedicated\nstate-of-the-art algorithms."
  },
  {
    "id": "arxiv-446",
    "title": "Generalization Bounds of SGLD for Non-convex Learning: Two Theoretical\n  Viewpoints",
    "abstract": "Algorithm-dependent generalization error bounds are central to statistical\nlearning theory. A learning algorithm may use a large hypothesis space, but the\nlimited number of iterations controls its model capacity and generalization\nerror. The impacts of stochastic gradient methods on generalization error for\nnon-convex learning problems not only have important theoretical consequences,\nbut are also critical to generalization errors of deep learning.\n  In this paper, we study the generalization errors of Stochastic Gradient\nLangevin Dynamics (SGLD) with non-convex objectives. Two theories are proposed\nwith non-asymptotic discrete-time analysis, using Stability and PAC-Bayesian\nresults respectively. The stability-based theory obtains a bound of\n$O\\left(\\frac{1}{n}L\\sqrt{\\beta T_k}\\right)$, where $L$ is uniform Lipschitz\nparameter, $\\beta$ is inverse temperature, and $T_k$ is aggregated step sizes.\nFor PAC-Bayesian theory, though the bound has a slower $O(1/\\sqrt{n})$ rate,\nthe contribution of each step is shown with an exponentially decaying factor by\nimposing $\\ell^2$ regularization, and the uniform Lipschitz constant is also\nreplaced by actual norms of gradients along trajectory. Our bounds have no\nimplicit dependence on dimensions, norms or other capacity measures of\nparameter, which elegantly characterizes the phenomenon of \"Fast Training\nGuarantees Generalization\" in non-convex settings. This is the first\nalgorithm-dependent result with reasonable dependence on aggregated step sizes\nfor non-convex learning, and has important implications to statistical learning\naspects of stochastic gradient methods in complicated models such as deep\nlearning.",
    "text": "Generalization Bounds of SGLD for Non-convex Learning: Two Theoretical\n  Viewpoints\n\nAlgorithm-dependent generalization error bounds are central to statistical\nlearning theory. A learning algorithm may use a large hypothesis space, but the\nlimited number of iterations controls its model capacity and generalization\nerror. The impacts of stochastic gradient methods on generalization error for\nnon-convex learning problems not only have important theoretical consequences,\nbut are also critical to generalization errors of deep learning.\n  In this paper, we study the generalization errors of Stochastic Gradient\nLangevin Dynamics (SGLD) with non-convex objectives. Two theories are proposed\nwith non-asymptotic discrete-time analysis, using Stability and PAC-Bayesian\nresults respectively. The stability-based theory obtains a bound of\n$O\\left(\\frac{1}{n}L\\sqrt{\\beta T_k}\\right)$, where $L$ is uniform Lipschitz\nparameter, $\\beta$ is inverse temperature, and $T_k$ is aggregated step sizes.\nFor PAC-Bayesian theory, though the bound has a slower $O(1/\\sqrt{n})$ rate,\nthe contribution of each step is shown with an exponentially decaying factor by\nimposing $\\ell^2$ regularization, and the uniform Lipschitz constant is also\nreplaced by actual norms of gradients along trajectory. Our bounds have no\nimplicit dependence on dimensions, norms or other capacity measures of\nparameter, which elegantly characterizes the phenomenon of \"Fast Training\nGuarantees Generalization\" in non-convex settings. This is the first\nalgorithm-dependent result with reasonable dependence on aggregated step sizes\nfor non-convex learning, and has important implications to statistical learning\naspects of stochastic gradient methods in complicated models such as deep\nlearning."
  },
  {
    "id": "arxiv-447",
    "title": "PixColor: Pixel Recursive Colorization",
    "abstract": "We propose a novel approach to automatically produce multiple colorized\nversions of a grayscale image. Our method results from the observation that the\ntask of automated colorization is relatively easy given a low-resolution\nversion of the color image. We first train a conditional PixelCNN to generate a\nlow resolution color for a given grayscale image. Then, given the generated\nlow-resolution color image and the original grayscale image as inputs, we train\na second CNN to generate a high-resolution colorization of an image. We\ndemonstrate that our approach produces more diverse and plausible colorizations\nthan existing methods, as judged by human raters in a \"Visual Turing Test\".",
    "text": "PixColor: Pixel Recursive Colorization\n\nWe propose a novel approach to automatically produce multiple colorized\nversions of a grayscale image. Our method results from the observation that the\ntask of automated colorization is relatively easy given a low-resolution\nversion of the color image. We first train a conditional PixelCNN to generate a\nlow resolution color for a given grayscale image. Then, given the generated\nlow-resolution color image and the original grayscale image as inputs, we train\na second CNN to generate a high-resolution colorization of an image. We\ndemonstrate that our approach produces more diverse and plausible colorizations\nthan existing methods, as judged by human raters in a \"Visual Turing Test\"."
  },
  {
    "id": "arxiv-448",
    "title": "Neural Mesh Refiner for 6-DoF Pose Estimation",
    "abstract": "How can we effectively utilise the 2D monocular image information for\nrecovering the 6D pose (6-DoF) of the visual objects? Deep learning has shown\nto be effective for robust and real-time monocular pose estimation. Oftentimes,\nthe network learns to regress the 6-DoF pose using a naive loss function.\nHowever, due to a lack of geometrical scene understanding from the directly\nregressed pose estimation, there are misalignments between the rendered mesh\nfrom the 3D object and the 2D instance segmentation result, e.g., bounding\nboxes and masks prediction. This paper bridges the gap between 2D mask\ngeneration and 3D location prediction via a differentiable neural mesh\nrenderer. We utilise the overlay between the accurate mask prediction and less\naccurate mesh prediction to iteratively optimise the direct regressed 6D pose\ninformation with a focus on translation estimation. By leveraging geometry, we\ndemonstrate that our technique significantly improves direct regression\nperformance on the difficult task of translation estimation and achieve the\nstate of the art results on Peking University/Baidu - Autonomous Driving\ndataset and the ApolloScape 3D Car Instance dataset. The code can be found at\n\\url{https://bit.ly/2IRihfU}.",
    "text": "Neural Mesh Refiner for 6-DoF Pose Estimation\n\nHow can we effectively utilise the 2D monocular image information for\nrecovering the 6D pose (6-DoF) of the visual objects? Deep learning has shown\nto be effective for robust and real-time monocular pose estimation. Oftentimes,\nthe network learns to regress the 6-DoF pose using a naive loss function.\nHowever, due to a lack of geometrical scene understanding from the directly\nregressed pose estimation, there are misalignments between the rendered mesh\nfrom the 3D object and the 2D instance segmentation result, e.g., bounding\nboxes and masks prediction. This paper bridges the gap between 2D mask\ngeneration and 3D location prediction via a differentiable neural mesh\nrenderer. We utilise the overlay between the accurate mask prediction and less\naccurate mesh prediction to iteratively optimise the direct regressed 6D pose\ninformation with a focus on translation estimation. By leveraging geometry, we\ndemonstrate that our technique significantly improves direct regression\nperformance on the difficult task of translation estimation and achieve the\nstate of the art results on Peking University/Baidu - Autonomous Driving\ndataset and the ApolloScape 3D Car Instance dataset. The code can be found at\n\\url{https://bit.ly/2IRihfU}."
  },
  {
    "id": "arxiv-449",
    "title": "SpeedMachines: Anytime Structured Prediction",
    "abstract": "Structured prediction plays a central role in machine learning applications\nfrom computational biology to computer vision. These models require\nsignificantly more computation than unstructured models, and, in many\napplications, algorithms may need to make predictions within a computational\nbudget or in an anytime fashion. In this work we propose an anytime technique\nfor learning structured prediction that, at training time, incorporates both\nstructural elements and feature computation trade-offs that affect test-time\ninference. We apply our technique to the challenging problem of scene\nunderstanding in computer vision and demonstrate efficient and anytime\npredictions that gradually improve towards state-of-the-art classification\nperformance as the allotted time increases.",
    "text": "SpeedMachines: Anytime Structured Prediction\n\nStructured prediction plays a central role in machine learning applications\nfrom computational biology to computer vision. These models require\nsignificantly more computation than unstructured models, and, in many\napplications, algorithms may need to make predictions within a computational\nbudget or in an anytime fashion. In this work we propose an anytime technique\nfor learning structured prediction that, at training time, incorporates both\nstructural elements and feature computation trade-offs that affect test-time\ninference. We apply our technique to the challenging problem of scene\nunderstanding in computer vision and demonstrate efficient and anytime\npredictions that gradually improve towards state-of-the-art classification\nperformance as the allotted time increases."
  },
  {
    "id": "arxiv-450",
    "title": "NADE: A Benchmark for Robust Adverse Drug Events Extraction in Face of\n  Negations",
    "abstract": "Adverse Drug Event (ADE) extraction models can rapidly examine large\ncollections of social media texts, detecting mentions of drug-related adverse\nreactions and trigger medical investigations. However, despite the recent\nadvances in NLP, it is currently unknown if such models are robust in face of\nnegation, which is pervasive across language varieties.\n  In this paper we evaluate three state-of-the-art systems, showing their\nfragility against negation, and then we introduce two possible strategies to\nincrease the robustness of these models: a pipeline approach, relying on a\nspecific component for negation detection; an augmentation of an ADE extraction\ndataset to artificially create negated samples and further train the models.\n  We show that both strategies bring significant increases in performance,\nlowering the number of spurious entities predicted by the models. Our dataset\nand code will be publicly released to encourage research on the topic.",
    "text": "NADE: A Benchmark for Robust Adverse Drug Events Extraction in Face of\n  Negations\n\nAdverse Drug Event (ADE) extraction models can rapidly examine large\ncollections of social media texts, detecting mentions of drug-related adverse\nreactions and trigger medical investigations. However, despite the recent\nadvances in NLP, it is currently unknown if such models are robust in face of\nnegation, which is pervasive across language varieties.\n  In this paper we evaluate three state-of-the-art systems, showing their\nfragility against negation, and then we introduce two possible strategies to\nincrease the robustness of these models: a pipeline approach, relying on a\nspecific component for negation detection; an augmentation of an ADE extraction\ndataset to artificially create negated samples and further train the models.\n  We show that both strategies bring significant increases in performance,\nlowering the number of spurious entities predicted by the models. Our dataset\nand code will be publicly released to encourage research on the topic."
  },
  {
    "id": "arxiv-451",
    "title": "CoSimGNN: Towards Large-scale Graph Similarity Computation",
    "abstract": "The ability to compute similarity scores between graphs based on metrics such\nas Graph Edit Distance (GED) is important in many real-world applications, such\nas 3D action recognition and biological molecular identification. Computing\nexact GED values is typically an NP-hard problem and traditional algorithms\nusually achieve an unsatisfactory trade-off between accuracy and efficiency.\nRecently, Graph Neural Networks (GNNs) provide a data-driven solution for this\ntask, which is more efficient while maintaining prediction accuracy in small\ngraph (around 10 nodes per graph) similarity computation. Existing GNN-based\nmethods, which either respectively embed two graphs (lack of low-level\ncross-graph interactions) or deploy cross-graph interactions for whole graph\npairs (redundant and time-consuming), are still not able to achieve competitive\nresults when the number of nodes in graphs increases. In this paper, we focus\non similarity computation for large-scale graphs and propose the\n\"embedding-coarsening-matching\" framework, which first embeds and coarsens\nlarge graphs to coarsened graphs with denser local topology and then deploys\nfine-grained interactions on the coarsened graphs for the final similarity\nscores.",
    "text": "CoSimGNN: Towards Large-scale Graph Similarity Computation\n\nThe ability to compute similarity scores between graphs based on metrics such\nas Graph Edit Distance (GED) is important in many real-world applications, such\nas 3D action recognition and biological molecular identification. Computing\nexact GED values is typically an NP-hard problem and traditional algorithms\nusually achieve an unsatisfactory trade-off between accuracy and efficiency.\nRecently, Graph Neural Networks (GNNs) provide a data-driven solution for this\ntask, which is more efficient while maintaining prediction accuracy in small\ngraph (around 10 nodes per graph) similarity computation. Existing GNN-based\nmethods, which either respectively embed two graphs (lack of low-level\ncross-graph interactions) or deploy cross-graph interactions for whole graph\npairs (redundant and time-consuming), are still not able to achieve competitive\nresults when the number of nodes in graphs increases. In this paper, we focus\non similarity computation for large-scale graphs and propose the\n\"embedding-coarsening-matching\" framework, which first embeds and coarsens\nlarge graphs to coarsened graphs with denser local topology and then deploys\nfine-grained interactions on the coarsened graphs for the final similarity\nscores."
  },
  {
    "id": "arxiv-452",
    "title": "Learning to Herd Agents Amongst Obstacles: Training Robust Shepherding\n  Behaviors using Deep Reinforcement Learning",
    "abstract": "Robotic shepherding problem considers the control and navigation of a group\nof coherent agents (e.g., a flock of bird or a fleet of drones) through the\nmotion of an external robot, called shepherd. Machine learning based methods\nhave successfully solved this problem in an empty environment with no\nobstacles. Rule-based methods, on the other hand, can handle more complex\nscenarios in which environments are cluttered with obstacles and allow multiple\nshepherds to work collaboratively. However, these rule-based methods are\nfragile due to the difficulty in defining a comprehensive set of rules that can\nhandle all possible cases. To overcome these limitations, we propose the first\nknown learning-based method that can herd agents amongst obstacles. By using\ndeep reinforcement learning techniques combined with the probabilistic\nroadmaps, we train a shepherding model using noisy but controlled environmental\nand behavioral parameters. Our experimental results show that the proposed\nmethod is robust, namely, it is insensitive to the uncertainties originated\nfrom both environmental and behavioral models. Consequently, the proposed\nmethod has a higher success rate, shorter completion time and path length than\nthe rule-based behavioral methods have. These advantages are particularly\nprominent in more challenging scenarios involving more difficult groups and\nstrenuous passages.",
    "text": "Learning to Herd Agents Amongst Obstacles: Training Robust Shepherding\n  Behaviors using Deep Reinforcement Learning\n\nRobotic shepherding problem considers the control and navigation of a group\nof coherent agents (e.g., a flock of bird or a fleet of drones) through the\nmotion of an external robot, called shepherd. Machine learning based methods\nhave successfully solved this problem in an empty environment with no\nobstacles. Rule-based methods, on the other hand, can handle more complex\nscenarios in which environments are cluttered with obstacles and allow multiple\nshepherds to work collaboratively. However, these rule-based methods are\nfragile due to the difficulty in defining a comprehensive set of rules that can\nhandle all possible cases. To overcome these limitations, we propose the first\nknown learning-based method that can herd agents amongst obstacles. By using\ndeep reinforcement learning techniques combined with the probabilistic\nroadmaps, we train a shepherding model using noisy but controlled environmental\nand behavioral parameters. Our experimental results show that the proposed\nmethod is robust, namely, it is insensitive to the uncertainties originated\nfrom both environmental and behavioral models. Consequently, the proposed\nmethod has a higher success rate, shorter completion time and path length than\nthe rule-based behavioral methods have. These advantages are particularly\nprominent in more challenging scenarios involving more difficult groups and\nstrenuous passages."
  },
  {
    "id": "arxiv-453",
    "title": "Relational Reasoning Network (RRN) for Anatomical Landmarking",
    "abstract": "Accurately identifying anatomical landmarks is a crucial step in deformation\nanalysis and surgical planning for craniomaxillofacial (CMF) bones. Available\nmethods require segmentation of the object of interest for precise landmarking.\nUnlike those, our purpose in this study is to perform anatomical landmarking\nusing the inherent relation of CMF bones without explicitly segmenting them. We\npropose a new deep network architecture, called relational reasoning network\n(RRN), to accurately learn the local and the global relations of the landmarks.\nSpecifically, we are interested in learning landmarks in CMF region: mandible,\nmaxilla, and nasal bones. The proposed RRN works in an end-to-end manner,\nutilizing learned relations of the landmarks based on dense-block units and\nwithout the need for segmentation. For a given a few landmarks as input, the\nproposed system accurately and efficiently localizes the remaining landmarks on\nthe aforementioned bones. For a comprehensive evaluation of RRN, we used\ncone-beam computed tomography (CBCT) scans of 250 patients. The proposed system\nidentifies the landmark locations very accurately even when there are severe\npathologies or deformations in the bones. The proposed RRN has also revealed\nunique relationships among the landmarks that help us infer several reasoning\nabout informativeness of the landmark points. RRN is invariant to order of\nlandmarks and it allowed us to discover the optimal configurations (number and\nlocation) for landmarks to be localized within the object of interest\n(mandible) or nearby objects (maxilla and nasal). To the best of our knowledge,\nthis is the first of its kind algorithm finding anatomical relations of the\nobjects using deep learning.",
    "text": "Relational Reasoning Network (RRN) for Anatomical Landmarking\n\nAccurately identifying anatomical landmarks is a crucial step in deformation\nanalysis and surgical planning for craniomaxillofacial (CMF) bones. Available\nmethods require segmentation of the object of interest for precise landmarking.\nUnlike those, our purpose in this study is to perform anatomical landmarking\nusing the inherent relation of CMF bones without explicitly segmenting them. We\npropose a new deep network architecture, called relational reasoning network\n(RRN), to accurately learn the local and the global relations of the landmarks.\nSpecifically, we are interested in learning landmarks in CMF region: mandible,\nmaxilla, and nasal bones. The proposed RRN works in an end-to-end manner,\nutilizing learned relations of the landmarks based on dense-block units and\nwithout the need for segmentation. For a given a few landmarks as input, the\nproposed system accurately and efficiently localizes the remaining landmarks on\nthe aforementioned bones. For a comprehensive evaluation of RRN, we used\ncone-beam computed tomography (CBCT) scans of 250 patients. The proposed system\nidentifies the landmark locations very accurately even when there are severe\npathologies or deformations in the bones. The proposed RRN has also revealed\nunique relationships among the landmarks that help us infer several reasoning\nabout informativeness of the landmark points. RRN is invariant to order of\nlandmarks and it allowed us to discover the optimal configurations (number and\nlocation) for landmarks to be localized within the object of interest\n(mandible) or nearby objects (maxilla and nasal). To the best of our knowledge,\nthis is the first of its kind algorithm finding anatomical relations of the\nobjects using deep learning."
  },
  {
    "id": "arxiv-454",
    "title": "Video Diffusion Models",
    "abstract": "Generating temporally coherent high fidelity video is an important milestone\nin generative modeling research. We make progress towards this milestone by\nproposing a diffusion model for video generation that shows very promising\ninitial results. Our model is a natural extension of the standard image\ndiffusion architecture, and it enables jointly training from image and video\ndata, which we find to reduce the variance of minibatch gradients and speed up\noptimization. To generate long and higher resolution videos we introduce a new\nconditional sampling technique for spatial and temporal video extension that\nperforms better than previously proposed methods. We present the first results\non a large text-conditioned video generation task, as well as state-of-the-art\nresults on an established unconditional video generation benchmark.\nSupplementary material is available at https://video-diffusion.github.io/",
    "text": "Video Diffusion Models\n\nGenerating temporally coherent high fidelity video is an important milestone\nin generative modeling research. We make progress towards this milestone by\nproposing a diffusion model for video generation that shows very promising\ninitial results. Our model is a natural extension of the standard image\ndiffusion architecture, and it enables jointly training from image and video\ndata, which we find to reduce the variance of minibatch gradients and speed up\noptimization. To generate long and higher resolution videos we introduce a new\nconditional sampling technique for spatial and temporal video extension that\nperforms better than previously proposed methods. We present the first results\non a large text-conditioned video generation task, as well as state-of-the-art\nresults on an established unconditional video generation benchmark.\nSupplementary material is available at https://video-diffusion.github.io/"
  },
  {
    "id": "arxiv-455",
    "title": "An Artificial Neural Network Architecture Based on Context\n  Transformations in Cortical Minicolumns",
    "abstract": "Cortical minicolumns are considered a model of cortical organization. Their\nfunction is still a source of research and not reflected properly in modern\narchitecture of nets in algorithms of Artificial Intelligence. We assume its\nfunction and describe it in this article. Furthermore, we show how this\nproposal allows to construct a new architecture, that is not based on\nconvolutional neural networks, test it on MNIST data and receive close to\nConvolutional Neural Network accuracy. We also show that the proposed\narchitecture possesses an ability to train on a small quantity of samples. To\nachieve these results, we enable the minicolumns to remember context\ntransformations.",
    "text": "An Artificial Neural Network Architecture Based on Context\n  Transformations in Cortical Minicolumns\n\nCortical minicolumns are considered a model of cortical organization. Their\nfunction is still a source of research and not reflected properly in modern\narchitecture of nets in algorithms of Artificial Intelligence. We assume its\nfunction and describe it in this article. Furthermore, we show how this\nproposal allows to construct a new architecture, that is not based on\nconvolutional neural networks, test it on MNIST data and receive close to\nConvolutional Neural Network accuracy. We also show that the proposed\narchitecture possesses an ability to train on a small quantity of samples. To\nachieve these results, we enable the minicolumns to remember context\ntransformations."
  },
  {
    "id": "arxiv-456",
    "title": "A deep learning approach for inverse design of the metasurface for\n  dual-polarized waves",
    "abstract": "Compared to the conventional metasurface design, machine learning-based\nmethods have recently created an inspiring platform for an inverse realization\nof the metasurfaces. Here, we have used the Deep Neural Network (DNN) for the\ngeneration of desired output unit cell structures in an ultra-wide working\nfrequency band for both TE and TM polarized waves. To automatically generate\nmetasurfaces in a wide range of working frequencies from 4 to 45 GHz, we\ndeliberately design an 8 ring-shaped pattern in such a way that the unit-cells\ngenerated in the dataset can produce single or multiple notches in the desired\nworking frequency band. Compared to the general approach, whereby the final\nmetasurface structure may be formed by any randomly distributed \"0\" and \"1\", we\npropose here a restricted output structure. By restricting the output, the\nnumber of calculations will be reduced and the learning speed will be\nincreased. Moreover, we have shown that the accuracy of the network reaches\n91\\%. Obtaining the final unit cell directly without any time-consuming\noptimization algorithms for both TE and TM polarized waves, and high average\naccuracy, promises an effective strategy for the metasurface design; thus, the\ndesigner is required only to focus on the design goal.",
    "text": "A deep learning approach for inverse design of the metasurface for\n  dual-polarized waves\n\nCompared to the conventional metasurface design, machine learning-based\nmethods have recently created an inspiring platform for an inverse realization\nof the metasurfaces. Here, we have used the Deep Neural Network (DNN) for the\ngeneration of desired output unit cell structures in an ultra-wide working\nfrequency band for both TE and TM polarized waves. To automatically generate\nmetasurfaces in a wide range of working frequencies from 4 to 45 GHz, we\ndeliberately design an 8 ring-shaped pattern in such a way that the unit-cells\ngenerated in the dataset can produce single or multiple notches in the desired\nworking frequency band. Compared to the general approach, whereby the final\nmetasurface structure may be formed by any randomly distributed \"0\" and \"1\", we\npropose here a restricted output structure. By restricting the output, the\nnumber of calculations will be reduced and the learning speed will be\nincreased. Moreover, we have shown that the accuracy of the network reaches\n91\\%. Obtaining the final unit cell directly without any time-consuming\noptimization algorithms for both TE and TM polarized waves, and high average\naccuracy, promises an effective strategy for the metasurface design; thus, the\ndesigner is required only to focus on the design goal."
  },
  {
    "id": "arxiv-457",
    "title": "Revisiting Communication-Efficient Federated Learning with Balanced\n  Global and Local Updates",
    "abstract": "In federated learning (FL), a number of devices train their local models and\nupload the corresponding parameters or gradients to the base station (BS) to\nupdate the global model while protecting their data privacy. However, due to\nthe limited computation and communication resources, the number of local\ntrainings (a.k.a. local update) and that of aggregations (a.k.a. global update)\nneed to be carefully chosen. In this paper, we investigate and analyze the\noptimal trade-off between the number of local trainings and that of global\naggregations to speed up the convergence and enhance the prediction accuracy\nover the existing works. Our goal is to minimize the global loss function under\nboth the delay and the energy consumption constraints. In order to make the\noptimization problem tractable, we derive a new and tight upper bound on the\nloss function, which allows us to obtain closed-form expressions for the number\nof local trainings and that of global aggregations. Simulation results show\nthat our proposed scheme can achieve a better performance in terms of the\nprediction accuracy, and converge much faster than the baseline schemes.",
    "text": "Revisiting Communication-Efficient Federated Learning with Balanced\n  Global and Local Updates\n\nIn federated learning (FL), a number of devices train their local models and\nupload the corresponding parameters or gradients to the base station (BS) to\nupdate the global model while protecting their data privacy. However, due to\nthe limited computation and communication resources, the number of local\ntrainings (a.k.a. local update) and that of aggregations (a.k.a. global update)\nneed to be carefully chosen. In this paper, we investigate and analyze the\noptimal trade-off between the number of local trainings and that of global\naggregations to speed up the convergence and enhance the prediction accuracy\nover the existing works. Our goal is to minimize the global loss function under\nboth the delay and the energy consumption constraints. In order to make the\noptimization problem tractable, we derive a new and tight upper bound on the\nloss function, which allows us to obtain closed-form expressions for the number\nof local trainings and that of global aggregations. Simulation results show\nthat our proposed scheme can achieve a better performance in terms of the\nprediction accuracy, and converge much faster than the baseline schemes."
  },
  {
    "id": "arxiv-458",
    "title": "Provably Fair Federated Learning via Bounded Group Loss",
    "abstract": "In federated learning, fair prediction across various protected groups (e.g.,\ngender, race) is an important constraint for many applications. Unfortunately,\nprior work studying group fair federated learning lacks formal convergence or\nfairness guarantees. Our work provides a new definition for group fairness in\nfederated learning based on the notion of Bounded Group Loss (BGL), which can\nbe easily applied to common federated learning objectives. Based on our\ndefinition, we propose a scalable algorithm that optimizes the empirical risk\nand global fairness constraints, which we evaluate across common fairness and\nfederated learning benchmarks. Our resulting method and analysis are the first\nwe are aware of to provide formal theoretical guarantees for training a fair\nfederated learning model.",
    "text": "Provably Fair Federated Learning via Bounded Group Loss\n\nIn federated learning, fair prediction across various protected groups (e.g.,\ngender, race) is an important constraint for many applications. Unfortunately,\nprior work studying group fair federated learning lacks formal convergence or\nfairness guarantees. Our work provides a new definition for group fairness in\nfederated learning based on the notion of Bounded Group Loss (BGL), which can\nbe easily applied to common federated learning objectives. Based on our\ndefinition, we propose a scalable algorithm that optimizes the empirical risk\nand global fairness constraints, which we evaluate across common fairness and\nfederated learning benchmarks. Our resulting method and analysis are the first\nwe are aware of to provide formal theoretical guarantees for training a fair\nfederated learning model."
  },
  {
    "id": "arxiv-459",
    "title": "RDSGAN: Rank-based Distant Supervision Relation Extraction with\n  Generative Adversarial Framework",
    "abstract": "Distant supervision has been widely used for relation extraction but suffers\nfrom noise labeling problem. Neural network models are proposed to denoise with\nattention mechanism but cannot eliminate noisy data due to its non-zero\nweights. Hard decision is proposed to remove wrongly-labeled instances from the\npositive set though causes loss of useful information contained in removed\ninstances. In this paper, we propose a novel generative neural framework named\nRDSGAN (Rank-based Distant Supervision GAN) which automatically generates valid\ninstances for distant supervision relation extraction. Our framework combines\nsoft attention and hard decision to learn the distribution of true positive\ninstances via adversarial training and selects valid instances conforming to\nthe distribution via rank-based distant supervision, which addresses the false\npositive problem. Experimental results show the superiority of our framework\nover strong baselines.",
    "text": "RDSGAN: Rank-based Distant Supervision Relation Extraction with\n  Generative Adversarial Framework\n\nDistant supervision has been widely used for relation extraction but suffers\nfrom noise labeling problem. Neural network models are proposed to denoise with\nattention mechanism but cannot eliminate noisy data due to its non-zero\nweights. Hard decision is proposed to remove wrongly-labeled instances from the\npositive set though causes loss of useful information contained in removed\ninstances. In this paper, we propose a novel generative neural framework named\nRDSGAN (Rank-based Distant Supervision GAN) which automatically generates valid\ninstances for distant supervision relation extraction. Our framework combines\nsoft attention and hard decision to learn the distribution of true positive\ninstances via adversarial training and selects valid instances conforming to\nthe distribution via rank-based distant supervision, which addresses the false\npositive problem. Experimental results show the superiority of our framework\nover strong baselines."
  },
  {
    "id": "arxiv-460",
    "title": "Classification of Brain Tumours in MR Images using Deep Spatiospatial\n  Models",
    "abstract": "A brain tumour is a mass or cluster of abnormal cells in the brain, which has\nthe possibility of becoming life-threatening because of its ability to invade\nneighbouring tissues and also form metastases. An accurate diagnosis is\nessential for successful treatment planning and magnetic resonance imaging is\nthe principal imaging modality for diagnostic of brain tumours and their\nextent. Deep Learning methods in computer vision applications have shown\nsignificant improvement in recent years, most of which can be credited to the\nfact that a sizeable amount of data is available to train models on, and the\nimprovements in the model architectures yielding better approximations in a\nsupervised setting. Classifying tumours using such deep learning methods has\nmade significant progress with the availability of open datasets with reliable\nannotations. Typically those methods are either 3D models, which use 3D\nvolumetric MRIs or even 2D models considering each slice separately. However,\nby treating the slice spatial dimension separately, spatiotemporal models can\nbe employed as spatiospatial models for this task. These models have the\ncapabilities of learning specific spatial and temporal relationship, while\nreducing computational costs. This paper uses two spatiotemporal models, ResNet\n(2+1)D and ResNet Mixed Convolution, to classify different types of brain\ntumours. It was observed that both these models performed superior to the pure\n3D convolutional model, ResNet18. Furthermore, it was also observed that\npre-training the models on a different, even unrelated dataset before training\nthem for the task of tumour classification improves the performance. Finally,\nPre-trained ResNet Mixed Convolution was observed to be the best model in these\nexperiments, achieving a macro F1-score of 0.93 and a test accuracy of 96.98\\%,\nwhile at the same time being the model with the least computational cost.",
    "text": "Classification of Brain Tumours in MR Images using Deep Spatiospatial\n  Models\n\nA brain tumour is a mass or cluster of abnormal cells in the brain, which has\nthe possibility of becoming life-threatening because of its ability to invade\nneighbouring tissues and also form metastases. An accurate diagnosis is\nessential for successful treatment planning and magnetic resonance imaging is\nthe principal imaging modality for diagnostic of brain tumours and their\nextent. Deep Learning methods in computer vision applications have shown\nsignificant improvement in recent years, most of which can be credited to the\nfact that a sizeable amount of data is available to train models on, and the\nimprovements in the model architectures yielding better approximations in a\nsupervised setting. Classifying tumours using such deep learning methods has\nmade significant progress with the availability of open datasets with reliable\nannotations. Typically those methods are either 3D models, which use 3D\nvolumetric MRIs or even 2D models considering each slice separately. However,\nby treating the slice spatial dimension separately, spatiotemporal models can\nbe employed as spatiospatial models for this task. These models have the\ncapabilities of learning specific spatial and temporal relationship, while\nreducing computational costs. This paper uses two spatiotemporal models, ResNet\n(2+1)D and ResNet Mixed Convolution, to classify different types of brain\ntumours. It was observed that both these models performed superior to the pure\n3D convolutional model, ResNet18. Furthermore, it was also observed that\npre-training the models on a different, even unrelated dataset before training\nthem for the task of tumour classification improves the performance. Finally,\nPre-trained ResNet Mixed Convolution was observed to be the best model in these\nexperiments, achieving a macro F1-score of 0.93 and a test accuracy of 96.98\\%,\nwhile at the same time being the model with the least computational cost."
  },
  {
    "id": "arxiv-461",
    "title": "Document Neural Autoregressive Distribution Estimation",
    "abstract": "We present an approach based on feed-forward neural networks for learning the\ndistribution of textual documents. This approach is inspired by the Neural\nAutoregressive Distribution Estimator(NADE) model, which has been shown to be a\ngood estimator of the distribution of discrete-valued igh-dimensional vectors.\nIn this paper, we present how NADE can successfully be adapted to the case of\ntextual data, retaining from NADE the property that sampling or computing the\nprobability of observations can be done exactly and efficiently. The approach\ncan also be used to learn deep representations of documents that are\ncompetitive to those learned by the alternative topic modeling approaches.\nFinally, we describe how the approach can be combined with a regular neural\nnetwork N-gram model and substantially improve its performance, by making its\nlearned representation sensitive to the larger, document-specific context.",
    "text": "Document Neural Autoregressive Distribution Estimation\n\nWe present an approach based on feed-forward neural networks for learning the\ndistribution of textual documents. This approach is inspired by the Neural\nAutoregressive Distribution Estimator(NADE) model, which has been shown to be a\ngood estimator of the distribution of discrete-valued igh-dimensional vectors.\nIn this paper, we present how NADE can successfully be adapted to the case of\ntextual data, retaining from NADE the property that sampling or computing the\nprobability of observations can be done exactly and efficiently. The approach\ncan also be used to learn deep representations of documents that are\ncompetitive to those learned by the alternative topic modeling approaches.\nFinally, we describe how the approach can be combined with a regular neural\nnetwork N-gram model and substantially improve its performance, by making its\nlearned representation sensitive to the larger, document-specific context."
  },
  {
    "id": "arxiv-462",
    "title": "Ranking metrics on non-shuffled traffic",
    "abstract": "Ranking metrics are a family of metrics largely used to evaluate recommender\nsystems. However they typically suffer from the fact the reward is affected by\nthe order in which recommended items are displayed to the user. A classical way\nto overcome this position bias is to uniformly shuffle a proportion of the\nrecommendations, but this method may result in a bad user experience. It is\nnevertheless common to use a stochastic policy to generate the recommendations,\nand we suggest a new method to overcome the position bias, by leveraging the\nstochasticity of the policy used to collect the dataset.",
    "text": "Ranking metrics on non-shuffled traffic\n\nRanking metrics are a family of metrics largely used to evaluate recommender\nsystems. However they typically suffer from the fact the reward is affected by\nthe order in which recommended items are displayed to the user. A classical way\nto overcome this position bias is to uniformly shuffle a proportion of the\nrecommendations, but this method may result in a bad user experience. It is\nnevertheless common to use a stochastic policy to generate the recommendations,\nand we suggest a new method to overcome the position bias, by leveraging the\nstochasticity of the policy used to collect the dataset."
  },
  {
    "id": "arxiv-463",
    "title": "Cascading Bandits: Learning to Rank in the Cascade Model",
    "abstract": "A search engine usually outputs a list of $K$ web pages. The user examines\nthis list, from the first web page to the last, and chooses the first\nattractive page. This model of user behavior is known as the cascade model. In\nthis paper, we propose cascading bandits, a learning variant of the cascade\nmodel where the objective is to identify $K$ most attractive items. We\nformulate our problem as a stochastic combinatorial partial monitoring problem.\nWe propose two algorithms for solving it, CascadeUCB1 and CascadeKL-UCB. We\nalso prove gap-dependent upper bounds on the regret of these algorithms and\nderive a lower bound on the regret in cascading bandits. The lower bound\nmatches the upper bound of CascadeKL-UCB up to a logarithmic factor. We\nexperiment with our algorithms on several problems. The algorithms perform\nsurprisingly well even when our modeling assumptions are violated.",
    "text": "Cascading Bandits: Learning to Rank in the Cascade Model\n\nA search engine usually outputs a list of $K$ web pages. The user examines\nthis list, from the first web page to the last, and chooses the first\nattractive page. This model of user behavior is known as the cascade model. In\nthis paper, we propose cascading bandits, a learning variant of the cascade\nmodel where the objective is to identify $K$ most attractive items. We\nformulate our problem as a stochastic combinatorial partial monitoring problem.\nWe propose two algorithms for solving it, CascadeUCB1 and CascadeKL-UCB. We\nalso prove gap-dependent upper bounds on the regret of these algorithms and\nderive a lower bound on the regret in cascading bandits. The lower bound\nmatches the upper bound of CascadeKL-UCB up to a logarithmic factor. We\nexperiment with our algorithms on several problems. The algorithms perform\nsurprisingly well even when our modeling assumptions are violated."
  },
  {
    "id": "arxiv-464",
    "title": "Towards Unifying the Label Space for Aspect- and Sentence-based\n  Sentiment Analysis",
    "abstract": "The aspect-based sentiment analysis (ABSA) is a fine-grained task that aims\nto determine the sentiment polarity towards targeted aspect terms occurring in\nthe sentence. The development of the ABSA task is very much hindered by the\nlack of annotated data. To tackle this, the prior works have studied the\npossibility of utilizing the sentiment analysis (SA) datasets to assist in\ntraining the ABSA model, primarily via pretraining or multi-task learning. In\nthis article, we follow this line, and for the first time, we manage to apply\nthe Pseudo-Label (PL) method to merge the two homogeneous tasks. While it seems\nstraightforward to use generated pseudo labels to handle this case of label\ngranularity unification for two highly related tasks, we identify its major\nchallenge in this paper and propose a novel framework, dubbed as\nDual-granularity Pseudo Labeling (DPL). Further, similar to PL, we regard the\nDPL as a general framework capable of combining other prior methods in the\nliterature. Through extensive experiments, DPL has achieved state-of-the-art\nperformance on standard benchmarks surpassing the prior work significantly.",
    "text": "Towards Unifying the Label Space for Aspect- and Sentence-based\n  Sentiment Analysis\n\nThe aspect-based sentiment analysis (ABSA) is a fine-grained task that aims\nto determine the sentiment polarity towards targeted aspect terms occurring in\nthe sentence. The development of the ABSA task is very much hindered by the\nlack of annotated data. To tackle this, the prior works have studied the\npossibility of utilizing the sentiment analysis (SA) datasets to assist in\ntraining the ABSA model, primarily via pretraining or multi-task learning. In\nthis article, we follow this line, and for the first time, we manage to apply\nthe Pseudo-Label (PL) method to merge the two homogeneous tasks. While it seems\nstraightforward to use generated pseudo labels to handle this case of label\ngranularity unification for two highly related tasks, we identify its major\nchallenge in this paper and propose a novel framework, dubbed as\nDual-granularity Pseudo Labeling (DPL). Further, similar to PL, we regard the\nDPL as a general framework capable of combining other prior methods in the\nliterature. Through extensive experiments, DPL has achieved state-of-the-art\nperformance on standard benchmarks surpassing the prior work significantly."
  },
  {
    "id": "arxiv-465",
    "title": "Scalable Vector Gaussian Information Bottleneck",
    "abstract": "In the context of statistical learning, the Information Bottleneck method\nseeks a right balance between accuracy and generalization capability through a\nsuitable tradeoff between compression complexity, measured by minimum\ndescription length, and distortion evaluated under logarithmic loss measure. In\nthis paper, we study a variation of the problem, called scalable information\nbottleneck, in which the encoder outputs multiple descriptions of the\nobservation with increasingly richer features. The model, which is of\nsuccessive-refinement type with degraded side information streams at the\ndecoders, is motivated by some application scenarios that require varying\nlevels of accuracy depending on the allowed (or targeted) level of complexity.\nWe establish an analytic characterization of the optimal relevance-complexity\nregion for vector Gaussian sources. Then, we derive a variational inference\ntype algorithm for general sources with unknown distribution; and show means of\nparametrizing it using neural networks. Finally, we provide experimental\nresults on the MNIST dataset which illustrate that the proposed method\ngeneralizes better to unseen data during the training phase.",
    "text": "Scalable Vector Gaussian Information Bottleneck\n\nIn the context of statistical learning, the Information Bottleneck method\nseeks a right balance between accuracy and generalization capability through a\nsuitable tradeoff between compression complexity, measured by minimum\ndescription length, and distortion evaluated under logarithmic loss measure. In\nthis paper, we study a variation of the problem, called scalable information\nbottleneck, in which the encoder outputs multiple descriptions of the\nobservation with increasingly richer features. The model, which is of\nsuccessive-refinement type with degraded side information streams at the\ndecoders, is motivated by some application scenarios that require varying\nlevels of accuracy depending on the allowed (or targeted) level of complexity.\nWe establish an analytic characterization of the optimal relevance-complexity\nregion for vector Gaussian sources. Then, we derive a variational inference\ntype algorithm for general sources with unknown distribution; and show means of\nparametrizing it using neural networks. Finally, we provide experimental\nresults on the MNIST dataset which illustrate that the proposed method\ngeneralizes better to unseen data during the training phase."
  },
  {
    "id": "arxiv-466",
    "title": "Machine Learning in Precision Medicine to Preserve Privacy via\n  Encryption",
    "abstract": "Precision medicine is an emerging approach for disease treatment and\nprevention that delivers personalized care to individual patients by\nconsidering their genetic makeups, medical histories, environments, and\nlifestyles. Despite the rapid advancement of precision medicine and its\nconsiderable promise, several underlying technological challenges remain\nunsolved. One such challenge of great importance is the security and privacy of\nprecision health-related data, such as genomic data and electronic health\nrecords, which stifle collaboration and hamper the full potential of\nmachine-learning (ML) algorithms. To preserve data privacy while providing ML\nsolutions, this article makes three contributions. First, we propose a generic\nmachine learning with encryption (MLE) framework, which we used to build an ML\nmodel that predicts cancer from one of the most recent comprehensive genomics\ndatasets in the field. Second, our framework's prediction accuracy is slightly\nhigher than that of the most recent studies conducted on the same dataset, yet\nit maintains the privacy of the patients' genomic data. Third, to facilitate\nthe validation, reproduction, and extension of this work, we provide an\nopen-source repository that contains the design and implementation of the\nframework, all the ML experiments and code, and the final predictive model\ndeployed to a free cloud service.",
    "text": "Machine Learning in Precision Medicine to Preserve Privacy via\n  Encryption\n\nPrecision medicine is an emerging approach for disease treatment and\nprevention that delivers personalized care to individual patients by\nconsidering their genetic makeups, medical histories, environments, and\nlifestyles. Despite the rapid advancement of precision medicine and its\nconsiderable promise, several underlying technological challenges remain\nunsolved. One such challenge of great importance is the security and privacy of\nprecision health-related data, such as genomic data and electronic health\nrecords, which stifle collaboration and hamper the full potential of\nmachine-learning (ML) algorithms. To preserve data privacy while providing ML\nsolutions, this article makes three contributions. First, we propose a generic\nmachine learning with encryption (MLE) framework, which we used to build an ML\nmodel that predicts cancer from one of the most recent comprehensive genomics\ndatasets in the field. Second, our framework's prediction accuracy is slightly\nhigher than that of the most recent studies conducted on the same dataset, yet\nit maintains the privacy of the patients' genomic data. Third, to facilitate\nthe validation, reproduction, and extension of this work, we provide an\nopen-source repository that contains the design and implementation of the\nframework, all the ML experiments and code, and the final predictive model\ndeployed to a free cloud service."
  },
  {
    "id": "arxiv-467",
    "title": "Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based\n  Search",
    "abstract": "Bayesian model-based reinforcement learning is a formally elegant approach to\nlearning optimal behaviour under model uncertainty, trading off exploration and\nexploitation in an ideal way. Unfortunately, finding the resulting\nBayes-optimal policies is notoriously taxing, since the search space becomes\nenormous. In this paper we introduce a tractable, sample-based method for\napproximate Bayes-optimal planning which exploits Monte-Carlo tree search. Our\napproach outperformed prior Bayesian model-based RL algorithms by a significant\nmargin on several well-known benchmark problems -- because it avoids expensive\napplications of Bayes rule within the search tree by lazily sampling models\nfrom the current beliefs. We illustrate the advantages of our approach by\nshowing it working in an infinite state space domain which is qualitatively out\nof reach of almost all previous work in Bayesian exploration.",
    "text": "Efficient Bayes-Adaptive Reinforcement Learning using Sample-Based\n  Search\n\nBayesian model-based reinforcement learning is a formally elegant approach to\nlearning optimal behaviour under model uncertainty, trading off exploration and\nexploitation in an ideal way. Unfortunately, finding the resulting\nBayes-optimal policies is notoriously taxing, since the search space becomes\nenormous. In this paper we introduce a tractable, sample-based method for\napproximate Bayes-optimal planning which exploits Monte-Carlo tree search. Our\napproach outperformed prior Bayesian model-based RL algorithms by a significant\nmargin on several well-known benchmark problems -- because it avoids expensive\napplications of Bayes rule within the search tree by lazily sampling models\nfrom the current beliefs. We illustrate the advantages of our approach by\nshowing it working in an infinite state space domain which is qualitatively out\nof reach of almost all previous work in Bayesian exploration."
  },
  {
    "id": "arxiv-468",
    "title": "Estimating Treatment Effects with Observed Confounders and Mediators",
    "abstract": "Given a causal graph, the do-calculus can express treatment effects as\nfunctionals of the observational joint distribution that can be estimated\nempirically. Sometimes the do-calculus identifies multiple valid formulae,\nprompting us to compare the statistical properties of the corresponding\nestimators. For example, the backdoor formula applies when all confounders are\nobserved and the frontdoor formula applies when an observed mediator transmits\nthe causal effect. In this paper, we investigate the over-identified scenario\nwhere both confounders and mediators are observed, rendering both estimators\nvalid. Addressing the linear Gaussian causal model, we demonstrate that either\nestimator can dominate the other by an unbounded constant factor. Next, we\nderive an optimal estimator, which leverages all observed variables, and bound\nits finite-sample variance. We show that it strictly outperforms the backdoor\nand frontdoor estimators and that this improvement can be unbounded. We also\npresent a procedure for combining two datasets, one with observed confounders\nand another with observed mediators. Finally, we evaluate our methods on both\nsimulated data and the IHDP and JTPA datasets.",
    "text": "Estimating Treatment Effects with Observed Confounders and Mediators\n\nGiven a causal graph, the do-calculus can express treatment effects as\nfunctionals of the observational joint distribution that can be estimated\nempirically. Sometimes the do-calculus identifies multiple valid formulae,\nprompting us to compare the statistical properties of the corresponding\nestimators. For example, the backdoor formula applies when all confounders are\nobserved and the frontdoor formula applies when an observed mediator transmits\nthe causal effect. In this paper, we investigate the over-identified scenario\nwhere both confounders and mediators are observed, rendering both estimators\nvalid. Addressing the linear Gaussian causal model, we demonstrate that either\nestimator can dominate the other by an unbounded constant factor. Next, we\nderive an optimal estimator, which leverages all observed variables, and bound\nits finite-sample variance. We show that it strictly outperforms the backdoor\nand frontdoor estimators and that this improvement can be unbounded. We also\npresent a procedure for combining two datasets, one with observed confounders\nand another with observed mediators. Finally, we evaluate our methods on both\nsimulated data and the IHDP and JTPA datasets."
  },
  {
    "id": "arxiv-469",
    "title": "A Slice Sampler for Restricted Hierarchical Beta Process with\n  Applications to Shared Subspace Learning",
    "abstract": "Hierarchical beta process has found interesting applications in recent years.\nIn this paper we present a modified hierarchical beta process prior with\napplications to hierarchical modeling of multiple data sources. The novel use\nof the prior over a hierarchical factor model allows factors to be shared\nacross different sources. We derive a slice sampler for this model, enabling\ntractable inference even when the likelihood and the prior over parameters are\nnon-conjugate. This allows the application of the model in much wider contexts\nwithout restrictions. We present two different data generative models a linear\nGaussianGaussian model for real valued data and a linear Poisson-gamma model\nfor count data. Encouraging transfer learning results are shown for two real\nworld applications text modeling and content based image retrieval.",
    "text": "A Slice Sampler for Restricted Hierarchical Beta Process with\n  Applications to Shared Subspace Learning\n\nHierarchical beta process has found interesting applications in recent years.\nIn this paper we present a modified hierarchical beta process prior with\napplications to hierarchical modeling of multiple data sources. The novel use\nof the prior over a hierarchical factor model allows factors to be shared\nacross different sources. We derive a slice sampler for this model, enabling\ntractable inference even when the likelihood and the prior over parameters are\nnon-conjugate. This allows the application of the model in much wider contexts\nwithout restrictions. We present two different data generative models a linear\nGaussianGaussian model for real valued data and a linear Poisson-gamma model\nfor count data. Encouraging transfer learning results are shown for two real\nworld applications text modeling and content based image retrieval."
  },
  {
    "id": "arxiv-470",
    "title": "Faster Maximum Feasible Subsystem Solutions for Dense Constraint\n  Matrices",
    "abstract": "Finding the largest cardinality feasible subset of an infeasible set of\nlinear constraints is the Maximum Feasible Subsystem problem (MAX FS). Solving\nthis problem is crucial in a wide range of applications such as machine\nlearning and compressive sensing. Although MAX FS is NP-hard, useful heuristic\nalgorithms exist, but these can be slow for large problems. We extend the\nexisting heuristics for the case of dense constraint matrices to greatly\nincrease their speed while preserving or improving solution quality. We test\nthe extended algorithms on two applications that have dense constraint\nmatrices: binary classification, and sparse recovery in compressive sensing. In\nboth cases, speed is greatly increased with no loss of accuracy.",
    "text": "Faster Maximum Feasible Subsystem Solutions for Dense Constraint\n  Matrices\n\nFinding the largest cardinality feasible subset of an infeasible set of\nlinear constraints is the Maximum Feasible Subsystem problem (MAX FS). Solving\nthis problem is crucial in a wide range of applications such as machine\nlearning and compressive sensing. Although MAX FS is NP-hard, useful heuristic\nalgorithms exist, but these can be slow for large problems. We extend the\nexisting heuristics for the case of dense constraint matrices to greatly\nincrease their speed while preserving or improving solution quality. We test\nthe extended algorithms on two applications that have dense constraint\nmatrices: binary classification, and sparse recovery in compressive sensing. In\nboth cases, speed is greatly increased with no loss of accuracy."
  },
  {
    "id": "arxiv-471",
    "title": "Localization of Critical Findings in Chest X-Ray without Local\n  Annotations Using Multi-Instance Learning",
    "abstract": "The automatic detection of critical findings in chest X-rays (CXR), such as\npneumothorax, is important for assisting radiologists in their clinical\nworkflow like triaging time-sensitive cases and screening for incidental\nfindings. While deep learning (DL) models has become a promising predictive\ntechnology with near-human accuracy, they commonly suffer from a lack of\nexplainability, which is an important aspect for clinical deployment of DL\nmodels in the highly regulated healthcare industry. For example, localizing\ncritical findings in an image is useful for explaining the predictions of DL\nclassification algorithms. While there have been a host of joint classification\nand localization methods for computer vision, the state-of-the-art DL models\nrequire locally annotated training data in the form of pixel level labels or\nbounding box coordinates. In the medical domain, this requires an expensive\namount of manual annotation by medical experts for each critical finding. This\nrequirement becomes a major barrier for training models that can rapidly scale\nto various findings. In this work, we address these shortcomings with an\ninterpretable DL algorithm based on multi-instance learning that jointly\nclassifies and localizes critical findings in CXR without the need for local\nannotations. We show competitive classification results on three different\ncritical findings (pneumothorax, pneumonia, and pulmonary edema) from three\ndifferent CXR datasets.",
    "text": "Localization of Critical Findings in Chest X-Ray without Local\n  Annotations Using Multi-Instance Learning\n\nThe automatic detection of critical findings in chest X-rays (CXR), such as\npneumothorax, is important for assisting radiologists in their clinical\nworkflow like triaging time-sensitive cases and screening for incidental\nfindings. While deep learning (DL) models has become a promising predictive\ntechnology with near-human accuracy, they commonly suffer from a lack of\nexplainability, which is an important aspect for clinical deployment of DL\nmodels in the highly regulated healthcare industry. For example, localizing\ncritical findings in an image is useful for explaining the predictions of DL\nclassification algorithms. While there have been a host of joint classification\nand localization methods for computer vision, the state-of-the-art DL models\nrequire locally annotated training data in the form of pixel level labels or\nbounding box coordinates. In the medical domain, this requires an expensive\namount of manual annotation by medical experts for each critical finding. This\nrequirement becomes a major barrier for training models that can rapidly scale\nto various findings. In this work, we address these shortcomings with an\ninterpretable DL algorithm based on multi-instance learning that jointly\nclassifies and localizes critical findings in CXR without the need for local\nannotations. We show competitive classification results on three different\ncritical findings (pneumothorax, pneumonia, and pulmonary edema) from three\ndifferent CXR datasets."
  },
  {
    "id": "arxiv-472",
    "title": "Out-of-Distribution Generalization Analysis via Influence Function",
    "abstract": "The mismatch between training and target data is one major challenge for\ncurrent machine learning systems. When training data is collected from multiple\ndomains and the target domains include all training domains and other new\ndomains, we are facing an Out-of-Distribution (OOD) generalization problem that\naims to find a model with the best OOD accuracy. One of the definitions of OOD\naccuracy is worst-domain accuracy. In general, the set of target domains is\nunknown, and the worst over target domains may be unseen when the number of\nobserved domains is limited. In this paper, we show that the worst accuracy\nover the observed domains may dramatically fail to identify the OOD accuracy.\nTo this end, we introduce Influence Function, a classical tool from robust\nstatistics, into the OOD generalization problem and suggest the variance of\ninfluence function to monitor the stability of a model on training domains. We\nshow that the accuracy on test domains and the proposed index together can help\nus discern whether OOD algorithms are needed and whether a model achieves good\nOOD generalization.",
    "text": "Out-of-Distribution Generalization Analysis via Influence Function\n\nThe mismatch between training and target data is one major challenge for\ncurrent machine learning systems. When training data is collected from multiple\ndomains and the target domains include all training domains and other new\ndomains, we are facing an Out-of-Distribution (OOD) generalization problem that\naims to find a model with the best OOD accuracy. One of the definitions of OOD\naccuracy is worst-domain accuracy. In general, the set of target domains is\nunknown, and the worst over target domains may be unseen when the number of\nobserved domains is limited. In this paper, we show that the worst accuracy\nover the observed domains may dramatically fail to identify the OOD accuracy.\nTo this end, we introduce Influence Function, a classical tool from robust\nstatistics, into the OOD generalization problem and suggest the variance of\ninfluence function to monitor the stability of a model on training domains. We\nshow that the accuracy on test domains and the proposed index together can help\nus discern whether OOD algorithms are needed and whether a model achieves good\nOOD generalization."
  },
  {
    "id": "arxiv-473",
    "title": "Prosody Transfer in Neural Text to Speech Using Global Pitch and\n  Loudness Features",
    "abstract": "This paper presents a simple yet effective method to achieve prosody transfer\nfrom a reference speech signal to synthesized speech. The main idea is to\nincorporate well-known acoustic correlates of prosody such as pitch and\nloudness contours of the reference speech into a modern neural text-to-speech\n(TTS) synthesizer such as Tacotron2 (TC2). More specifically, a small set of\nacoustic features are extracted from reference audio and then used to condition\na TC2 synthesizer. The trained model is evaluated using subjective listening\ntests and a novel objective evaluation of prosody transfer is proposed.\nListening tests show that the synthesized speech is rated as highly natural and\nthat prosody is successfully transferred from the reference speech signal to\nthe synthesized signal.",
    "text": "Prosody Transfer in Neural Text to Speech Using Global Pitch and\n  Loudness Features\n\nThis paper presents a simple yet effective method to achieve prosody transfer\nfrom a reference speech signal to synthesized speech. The main idea is to\nincorporate well-known acoustic correlates of prosody such as pitch and\nloudness contours of the reference speech into a modern neural text-to-speech\n(TTS) synthesizer such as Tacotron2 (TC2). More specifically, a small set of\nacoustic features are extracted from reference audio and then used to condition\na TC2 synthesizer. The trained model is evaluated using subjective listening\ntests and a novel objective evaluation of prosody transfer is proposed.\nListening tests show that the synthesized speech is rated as highly natural and\nthat prosody is successfully transferred from the reference speech signal to\nthe synthesized signal."
  },
  {
    "id": "arxiv-474",
    "title": "Dementia Severity Classification under Small Sample Size and Weak\n  Supervision in Thick Slice MRI",
    "abstract": "Early detection of dementia through specific biomarkers in MR images plays a\ncritical role in developing support strategies proactively. Fazekas scale\nfacilitates an accurate quantitative assessment of the severity of white matter\nlesions and hence the disease. Imaging Biomarkers of dementia are multiple and\ncomprehensive documentation of them is time-consuming. Therefore, any effort to\nautomatically extract these biomarkers will be of clinical value while reducing\ninter-rater discrepancies. To tackle this problem, we propose to classify the\ndisease severity based on the Fazekas scale through the visual biomarkers,\nnamely the Periventricular White Matter (PVWM) and the Deep White Matter (DWM)\nchanges, in the real-world setting of thick-slice MRI. Small training sample\nsize and weak supervision in form of assigning severity labels to the whole MRI\nstack are among the main challenges. To combat the mentioned issues, we have\ndeveloped a deep learning pipeline that employs self-supervised representation\nlearning, multiple instance learning, and appropriate pre-processing steps. We\nuse pretext tasks such as non-linear transformation, local shuffling, in- and\nout-painting for self-supervised learning of useful features in this domain.\nFurthermore, an attention model is used to determine the relevance of each MRI\nslice for predicting the Fazekas scale in an unsupervised manner. We show the\nsignificant superiority of our method in distinguishing different classes of\ndementia compared to state-of-the-art methods in our mentioned setting, which\nimproves the macro averaged F1-score of state-of-the-art from 61% to 76% in\nPVWM, and from 58% to 69.2% in DWM.",
    "text": "Dementia Severity Classification under Small Sample Size and Weak\n  Supervision in Thick Slice MRI\n\nEarly detection of dementia through specific biomarkers in MR images plays a\ncritical role in developing support strategies proactively. Fazekas scale\nfacilitates an accurate quantitative assessment of the severity of white matter\nlesions and hence the disease. Imaging Biomarkers of dementia are multiple and\ncomprehensive documentation of them is time-consuming. Therefore, any effort to\nautomatically extract these biomarkers will be of clinical value while reducing\ninter-rater discrepancies. To tackle this problem, we propose to classify the\ndisease severity based on the Fazekas scale through the visual biomarkers,\nnamely the Periventricular White Matter (PVWM) and the Deep White Matter (DWM)\nchanges, in the real-world setting of thick-slice MRI. Small training sample\nsize and weak supervision in form of assigning severity labels to the whole MRI\nstack are among the main challenges. To combat the mentioned issues, we have\ndeveloped a deep learning pipeline that employs self-supervised representation\nlearning, multiple instance learning, and appropriate pre-processing steps. We\nuse pretext tasks such as non-linear transformation, local shuffling, in- and\nout-painting for self-supervised learning of useful features in this domain.\nFurthermore, an attention model is used to determine the relevance of each MRI\nslice for predicting the Fazekas scale in an unsupervised manner. We show the\nsignificant superiority of our method in distinguishing different classes of\ndementia compared to state-of-the-art methods in our mentioned setting, which\nimproves the macro averaged F1-score of state-of-the-art from 61% to 76% in\nPVWM, and from 58% to 69.2% in DWM."
  },
  {
    "id": "arxiv-475",
    "title": "Token-Level Supervised Contrastive Learning for Punctuation Restoration",
    "abstract": "Punctuation is critical in understanding natural language text. Currently,\nmost automatic speech recognition (ASR) systems do not generate punctuation,\nwhich affects the performance of downstream tasks, such as intent detection and\nslot filling. This gives rise to the need for punctuation restoration. Recent\nwork in punctuation restoration heavily utilizes pre-trained language models\nwithout considering data imbalance when predicting punctuation classes. In this\nwork, we address this problem by proposing a token-level supervised contrastive\nlearning method that aims at maximizing the distance of representation of\ndifferent punctuation marks in the embedding space. The result shows that\ntraining with token-level supervised contrastive learning obtains up to 3.2%\nabsolute F1 improvement on the test set.",
    "text": "Token-Level Supervised Contrastive Learning for Punctuation Restoration\n\nPunctuation is critical in understanding natural language text. Currently,\nmost automatic speech recognition (ASR) systems do not generate punctuation,\nwhich affects the performance of downstream tasks, such as intent detection and\nslot filling. This gives rise to the need for punctuation restoration. Recent\nwork in punctuation restoration heavily utilizes pre-trained language models\nwithout considering data imbalance when predicting punctuation classes. In this\nwork, we address this problem by proposing a token-level supervised contrastive\nlearning method that aims at maximizing the distance of representation of\ndifferent punctuation marks in the embedding space. The result shows that\ntraining with token-level supervised contrastive learning obtains up to 3.2%\nabsolute F1 improvement on the test set."
  },
  {
    "id": "arxiv-476",
    "title": "Approximating Interactive Human Evaluation with Self-Play for\n  Open-Domain Dialog Systems",
    "abstract": "Building an open-domain conversational agent is a challenging problem.\nCurrent evaluation methods, mostly post-hoc judgments of static conversation,\ndo not capture conversation quality in a realistic interactive context. In this\npaper, we investigate interactive human evaluation and provide evidence for its\nnecessity; we then introduce a novel, model-agnostic, and dataset-agnostic\nmethod to approximate it. In particular, we propose a self-play scenario where\nthe dialog system talks to itself and we calculate a combination of proxies\nsuch as sentiment and semantic coherence on the conversation trajectory. We\nshow that this metric is capable of capturing the human-rated quality of a\ndialog model better than any automated metric known to-date, achieving a\nsignificant Pearson correlation (r>.7, p<.05). To investigate the strengths of\nthis novel metric and interactive evaluation in comparison to state-of-the-art\nmetrics and human evaluation of static conversations, we perform extended\nexperiments with a set of models, including several that make novel\nimprovements to recent hierarchical dialog generation architectures through\nsentiment and semantic knowledge distillation on the utterance level. Finally,\nwe open-source the interactive evaluation platform we built and the dataset we\ncollected to allow researchers to efficiently deploy and evaluate dialog\nmodels.",
    "text": "Approximating Interactive Human Evaluation with Self-Play for\n  Open-Domain Dialog Systems\n\nBuilding an open-domain conversational agent is a challenging problem.\nCurrent evaluation methods, mostly post-hoc judgments of static conversation,\ndo not capture conversation quality in a realistic interactive context. In this\npaper, we investigate interactive human evaluation and provide evidence for its\nnecessity; we then introduce a novel, model-agnostic, and dataset-agnostic\nmethod to approximate it. In particular, we propose a self-play scenario where\nthe dialog system talks to itself and we calculate a combination of proxies\nsuch as sentiment and semantic coherence on the conversation trajectory. We\nshow that this metric is capable of capturing the human-rated quality of a\ndialog model better than any automated metric known to-date, achieving a\nsignificant Pearson correlation (r>.7, p<.05). To investigate the strengths of\nthis novel metric and interactive evaluation in comparison to state-of-the-art\nmetrics and human evaluation of static conversations, we perform extended\nexperiments with a set of models, including several that make novel\nimprovements to recent hierarchical dialog generation architectures through\nsentiment and semantic knowledge distillation on the utterance level. Finally,\nwe open-source the interactive evaluation platform we built and the dataset we\ncollected to allow researchers to efficiently deploy and evaluate dialog\nmodels."
  },
  {
    "id": "arxiv-477",
    "title": "Bridging the Gap of AutoGraph between Academia and Industry: Analysing\n  AutoGraph Challenge at KDD Cup 2020",
    "abstract": "Graph structured data is ubiquitous in daily life and scientific areas and\nhas attracted increasing attention. Graph Neural Networks (GNNs) have been\nproved to be effective in modeling graph structured data and many variants of\nGNN architectures have been proposed. However, much human effort is often\nneeded to tune the architecture depending on different datasets. Researchers\nnaturally adopt Automated Machine Learning on Graph Learning, aiming to reduce\nthe human effort and achieve generally top-performing GNNs, but their methods\nfocus more on the architecture search. To understand GNN practitioners'\nautomated solutions, we organized AutoGraph Challenge at KDD Cup 2020,\nemphasizing on automated graph neural networks for node classification. We\nreceived top solutions especially from industrial tech companies like Meituan,\nAlibaba and Twitter, which are already open sourced on Github. After detailed\ncomparisons with solutions from academia, we quantify the gaps between academia\nand industry on modeling scope, effectiveness and efficiency, and show that (1)\nacademia AutoML for Graph solutions focus on GNN architecture search while\nindustrial solutions, especially the winning ones in the KDD Cup, tend to\nobtain an overall solution (2) by neural architecture search only, academia\nsolutions achieve on average 97.3% accuracy of industrial solutions (3)\nacademia solutions are cheap to obtain with several GPU hours while industrial\nsolutions take a few months' labors. Academic solutions also contain much fewer\nparameters.",
    "text": "Bridging the Gap of AutoGraph between Academia and Industry: Analysing\n  AutoGraph Challenge at KDD Cup 2020\n\nGraph structured data is ubiquitous in daily life and scientific areas and\nhas attracted increasing attention. Graph Neural Networks (GNNs) have been\nproved to be effective in modeling graph structured data and many variants of\nGNN architectures have been proposed. However, much human effort is often\nneeded to tune the architecture depending on different datasets. Researchers\nnaturally adopt Automated Machine Learning on Graph Learning, aiming to reduce\nthe human effort and achieve generally top-performing GNNs, but their methods\nfocus more on the architecture search. To understand GNN practitioners'\nautomated solutions, we organized AutoGraph Challenge at KDD Cup 2020,\nemphasizing on automated graph neural networks for node classification. We\nreceived top solutions especially from industrial tech companies like Meituan,\nAlibaba and Twitter, which are already open sourced on Github. After detailed\ncomparisons with solutions from academia, we quantify the gaps between academia\nand industry on modeling scope, effectiveness and efficiency, and show that (1)\nacademia AutoML for Graph solutions focus on GNN architecture search while\nindustrial solutions, especially the winning ones in the KDD Cup, tend to\nobtain an overall solution (2) by neural architecture search only, academia\nsolutions achieve on average 97.3% accuracy of industrial solutions (3)\nacademia solutions are cheap to obtain with several GPU hours while industrial\nsolutions take a few months' labors. Academic solutions also contain much fewer\nparameters."
  },
  {
    "id": "arxiv-478",
    "title": "Iterative Compression of End-to-End ASR Model using AutoML",
    "abstract": "Increasing demand for on-device Automatic Speech Recognition (ASR) systems\nhas resulted in renewed interests in developing automatic model compression\ntechniques. Past research have shown that AutoML-based Low Rank Factorization\n(LRF) technique, when applied to an end-to-end Encoder-Attention-Decoder style\nASR model, can achieve a speedup of up to 3.7x, outperforming laborious manual\nrank-selection approaches. However, we show that current AutoML-based search\ntechniques only work up to a certain compression level, beyond which they fail\nto produce compressed models with acceptable word error rates (WER). In this\nwork, we propose an iterative AutoML-based LRF approach that achieves over 5x\ncompression without degrading the WER, thereby advancing the state-of-the-art\nin ASR compression.",
    "text": "Iterative Compression of End-to-End ASR Model using AutoML\n\nIncreasing demand for on-device Automatic Speech Recognition (ASR) systems\nhas resulted in renewed interests in developing automatic model compression\ntechniques. Past research have shown that AutoML-based Low Rank Factorization\n(LRF) technique, when applied to an end-to-end Encoder-Attention-Decoder style\nASR model, can achieve a speedup of up to 3.7x, outperforming laborious manual\nrank-selection approaches. However, we show that current AutoML-based search\ntechniques only work up to a certain compression level, beyond which they fail\nto produce compressed models with acceptable word error rates (WER). In this\nwork, we propose an iterative AutoML-based LRF approach that achieves over 5x\ncompression without degrading the WER, thereby advancing the state-of-the-art\nin ASR compression."
  },
  {
    "id": "arxiv-479",
    "title": "Transfer Learning for Fault Diagnosis of Transmission Lines",
    "abstract": "Recent artificial intelligence-based methods have shown great promise in the\nuse of neural networks for real-time sensing and detection of transmission line\nfaults and estimation of their locations. The expansion of power systems\nincluding transmission lines with various lengths have made a fault detection,\nclassification, and location estimation process more challenging. Transmission\nline datasets are stream data which are continuously collected by various\nsensors and hence, require generalized and fast fault diagnosis approaches.\nNewly collected datasets including voltages and currents might not have enough\nand accurate labels (fault and no fault) that are useful to train neural\nnetworks. In this paper, a novel transfer learning framework based on a\npre-trained LeNet-5 convolutional neural network is proposed. This method is\nable to diagnose faults for different transmission line lengths and impedances\nby transferring the knowledge from a source convolutional neural network to\npredict a dissimilar target dataset. By transferring this knowledge, faults\nfrom various transmission lines, without having enough labels, can be diagnosed\nfaster and more efficiently compared to the existing methods. To prove the\nfeasibility and effectiveness of this methodology, seven different datasets\nthat include various lengths of transmission lines are used. The robustness of\nthe proposed methodology against generator voltage fluctuation, variation in\nfault distance, fault inception angle, fault resistance, and phase difference\nbetween the two generators are well shown, thus proving its practical values in\nthe fault diagnosis of transmission lines.",
    "text": "Transfer Learning for Fault Diagnosis of Transmission Lines\n\nRecent artificial intelligence-based methods have shown great promise in the\nuse of neural networks for real-time sensing and detection of transmission line\nfaults and estimation of their locations. The expansion of power systems\nincluding transmission lines with various lengths have made a fault detection,\nclassification, and location estimation process more challenging. Transmission\nline datasets are stream data which are continuously collected by various\nsensors and hence, require generalized and fast fault diagnosis approaches.\nNewly collected datasets including voltages and currents might not have enough\nand accurate labels (fault and no fault) that are useful to train neural\nnetworks. In this paper, a novel transfer learning framework based on a\npre-trained LeNet-5 convolutional neural network is proposed. This method is\nable to diagnose faults for different transmission line lengths and impedances\nby transferring the knowledge from a source convolutional neural network to\npredict a dissimilar target dataset. By transferring this knowledge, faults\nfrom various transmission lines, without having enough labels, can be diagnosed\nfaster and more efficiently compared to the existing methods. To prove the\nfeasibility and effectiveness of this methodology, seven different datasets\nthat include various lengths of transmission lines are used. The robustness of\nthe proposed methodology against generator voltage fluctuation, variation in\nfault distance, fault inception angle, fault resistance, and phase difference\nbetween the two generators are well shown, thus proving its practical values in\nthe fault diagnosis of transmission lines."
  },
  {
    "id": "arxiv-480",
    "title": "Near-Optimal Offline Reinforcement Learning via Double Variance\n  Reduction",
    "abstract": "We consider the problem of offline reinforcement learning (RL) -- a\nwell-motivated setting of RL that aims at policy optimization using only\nhistorical data. Despite its wide applicability, theoretical understandings of\noffline RL, such as its optimal sample complexity, remain largely open even in\nbasic settings such as \\emph{tabular} Markov Decision Processes (MDPs).\n  In this paper, we propose Off-Policy Double Variance Reduction (OPDVR), a new\nvariance reduction based algorithm for offline RL. Our main result shows that\nOPDVR provably identifies an $\\epsilon$-optimal policy with\n$\\widetilde{O}(H^2/d_m\\epsilon^2)$ episodes of offline data in the\nfinite-horizon stationary transition setting, where $H$ is the horizon length\nand $d_m$ is the minimal marginal state-action distribution induced by the\nbehavior policy. This improves over the best known upper bound by a factor of\n$H$. Moreover, we establish an information-theoretic lower bound of\n$\\Omega(H^2/d_m\\epsilon^2)$ which certifies that OPDVR is optimal up to\nlogarithmic factors. Lastly, we show that OPDVR also achieves rate-optimal\nsample complexity under alternative settings such as the finite-horizon MDPs\nwith non-stationary transitions and the infinite horizon MDPs with discounted\nrewards.",
    "text": "Near-Optimal Offline Reinforcement Learning via Double Variance\n  Reduction\n\nWe consider the problem of offline reinforcement learning (RL) -- a\nwell-motivated setting of RL that aims at policy optimization using only\nhistorical data. Despite its wide applicability, theoretical understandings of\noffline RL, such as its optimal sample complexity, remain largely open even in\nbasic settings such as \\emph{tabular} Markov Decision Processes (MDPs).\n  In this paper, we propose Off-Policy Double Variance Reduction (OPDVR), a new\nvariance reduction based algorithm for offline RL. Our main result shows that\nOPDVR provably identifies an $\\epsilon$-optimal policy with\n$\\widetilde{O}(H^2/d_m\\epsilon^2)$ episodes of offline data in the\nfinite-horizon stationary transition setting, where $H$ is the horizon length\nand $d_m$ is the minimal marginal state-action distribution induced by the\nbehavior policy. This improves over the best known upper bound by a factor of\n$H$. Moreover, we establish an information-theoretic lower bound of\n$\\Omega(H^2/d_m\\epsilon^2)$ which certifies that OPDVR is optimal up to\nlogarithmic factors. Lastly, we show that OPDVR also achieves rate-optimal\nsample complexity under alternative settings such as the finite-horizon MDPs\nwith non-stationary transitions and the infinite horizon MDPs with discounted\nrewards."
  },
  {
    "id": "arxiv-481",
    "title": "Confidence-Budget Matching for Sequential Budgeted Learning",
    "abstract": "A core element in decision-making under uncertainty is the feedback on the\nquality of the performed actions. However, in many applications, such feedback\nis restricted. For example, in recommendation systems, repeatedly asking the\nuser to provide feedback on the quality of recommendations will annoy them. In\nthis work, we formalize decision-making problems with querying budget, where\nthere is a (possibly time-dependent) hard limit on the number of reward queries\nallowed. Specifically, we consider multi-armed bandits, linear bandits, and\nreinforcement learning problems. We start by analyzing the performance of\n`greedy' algorithms that query a reward whenever they can. We show that in\nfully stochastic settings, doing so performs surprisingly well, but in the\npresence of any adversity, this might lead to linear regret. To overcome this\nissue, we propose the Confidence-Budget Matching (CBM) principle that queries\nrewards when the confidence intervals are wider than the inverse square root of\nthe available budget. We analyze the performance of CBM based algorithms in\ndifferent settings and show that they perform well in the presence of adversity\nin the contexts, initial states, and budgets.",
    "text": "Confidence-Budget Matching for Sequential Budgeted Learning\n\nA core element in decision-making under uncertainty is the feedback on the\nquality of the performed actions. However, in many applications, such feedback\nis restricted. For example, in recommendation systems, repeatedly asking the\nuser to provide feedback on the quality of recommendations will annoy them. In\nthis work, we formalize decision-making problems with querying budget, where\nthere is a (possibly time-dependent) hard limit on the number of reward queries\nallowed. Specifically, we consider multi-armed bandits, linear bandits, and\nreinforcement learning problems. We start by analyzing the performance of\n`greedy' algorithms that query a reward whenever they can. We show that in\nfully stochastic settings, doing so performs surprisingly well, but in the\npresence of any adversity, this might lead to linear regret. To overcome this\nissue, we propose the Confidence-Budget Matching (CBM) principle that queries\nrewards when the confidence intervals are wider than the inverse square root of\nthe available budget. We analyze the performance of CBM based algorithms in\ndifferent settings and show that they perform well in the presence of adversity\nin the contexts, initial states, and budgets."
  },
  {
    "id": "arxiv-482",
    "title": "Delayed rejection Hamiltonian Monte Carlo for sampling multiscale\n  distributions",
    "abstract": "The efficiency of Hamiltonian Monte Carlo (HMC) can suffer when sampling a\ndistribution with a wide range of length scales, because the small step sizes\nneeded for stability in high-curvature regions are inefficient elsewhere. To\naddress this we present a delayed rejection variant: if an initial HMC\ntrajectory is rejected, we make one or more subsequent proposals each using a\nstep size geometrically smaller than the last. We extend the standard delayed\nrejection framework by allowing the probability of a retry to depend on the\nprobability of accepting the previous proposal. We test the scheme in several\nsampling tasks, including multiscale model distributions such as Neal's funnel,\nand statistical applications. Delayed rejection enables up to five-fold\nperformance gains over optimally-tuned HMC, as measured by effective sample\nsize per gradient evaluation. Even for simpler distributions, delayed rejection\nprovides increased robustness to step size misspecification. Along the way, we\nprovide an accessible but rigorous review of detailed balance for HMC.",
    "text": "Delayed rejection Hamiltonian Monte Carlo for sampling multiscale\n  distributions\n\nThe efficiency of Hamiltonian Monte Carlo (HMC) can suffer when sampling a\ndistribution with a wide range of length scales, because the small step sizes\nneeded for stability in high-curvature regions are inefficient elsewhere. To\naddress this we present a delayed rejection variant: if an initial HMC\ntrajectory is rejected, we make one or more subsequent proposals each using a\nstep size geometrically smaller than the last. We extend the standard delayed\nrejection framework by allowing the probability of a retry to depend on the\nprobability of accepting the previous proposal. We test the scheme in several\nsampling tasks, including multiscale model distributions such as Neal's funnel,\nand statistical applications. Delayed rejection enables up to five-fold\nperformance gains over optimally-tuned HMC, as measured by effective sample\nsize per gradient evaluation. Even for simpler distributions, delayed rejection\nprovides increased robustness to step size misspecification. Along the way, we\nprovide an accessible but rigorous review of detailed balance for HMC."
  },
  {
    "id": "arxiv-483",
    "title": "Robustifying Reinforcement Learning Policies with $\\mathcal{L}_1$\n  Adaptive Control",
    "abstract": "A reinforcement learning (RL) policy trained in a nominal environment could\nfail in a new/perturbed environment due to the existence of dynamic variations.\nExisting robust methods try to obtain a fixed policy for all envisioned dynamic\nvariation scenarios through robust or adversarial training. These methods could\nlead to conservative performance due to emphasis on the worst case, and often\ninvolve tedious modifications to the training environment. We propose an\napproach to robustifying a pre-trained non-robust RL policy with\n$\\mathcal{L}_1$ adaptive control. Leveraging the capability of an\n$\\mathcal{L}_1$ control law in the fast estimation of and active compensation\nfor dynamic variations, our approach can significantly improve the robustness\nof an RL policy trained in a standard (i.e., non-robust) way, either in a\nsimulator or in the real world. Numerical experiments are provided to validate\nthe efficacy of the proposed approach.",
    "text": "Robustifying Reinforcement Learning Policies with $\\mathcal{L}_1$\n  Adaptive Control\n\nA reinforcement learning (RL) policy trained in a nominal environment could\nfail in a new/perturbed environment due to the existence of dynamic variations.\nExisting robust methods try to obtain a fixed policy for all envisioned dynamic\nvariation scenarios through robust or adversarial training. These methods could\nlead to conservative performance due to emphasis on the worst case, and often\ninvolve tedious modifications to the training environment. We propose an\napproach to robustifying a pre-trained non-robust RL policy with\n$\\mathcal{L}_1$ adaptive control. Leveraging the capability of an\n$\\mathcal{L}_1$ control law in the fast estimation of and active compensation\nfor dynamic variations, our approach can significantly improve the robustness\nof an RL policy trained in a standard (i.e., non-robust) way, either in a\nsimulator or in the real world. Numerical experiments are provided to validate\nthe efficacy of the proposed approach."
  },
  {
    "id": "arxiv-484",
    "title": "FLIX: A Simple and Communication-Efficient Alternative to Local Methods\n  in Federated Learning",
    "abstract": "Federated Learning (FL) is an increasingly popular machine learning paradigm\nin which multiple nodes try to collaboratively learn under privacy,\ncommunication and multiple heterogeneity constraints. A persistent problem in\nfederated learning is that it is not clear what the optimization objective\nshould be: the standard average risk minimization of supervised learning is\ninadequate in handling several major constraints specific to federated\nlearning, such as communication adaptivity and personalization control. We\nidentify several key desiderata in frameworks for federated learning and\nintroduce a new framework, FLIX, that takes into account the unique challenges\nbrought by federated learning. FLIX has a standard finite-sum form, which\nenables practitioners to tap into the immense wealth of existing (potentially\nnon-local) methods for distributed optimization. Through a smart initialization\nthat does not require any communication, FLIX does not require the use of local\nsteps but is still provably capable of performing dissimilarity regularization\non par with local methods. We give several algorithms for solving the FLIX\nformulation efficiently under communication constraints. Finally, we\ncorroborate our theoretical results with extensive experimentation.",
    "text": "FLIX: A Simple and Communication-Efficient Alternative to Local Methods\n  in Federated Learning\n\nFederated Learning (FL) is an increasingly popular machine learning paradigm\nin which multiple nodes try to collaboratively learn under privacy,\ncommunication and multiple heterogeneity constraints. A persistent problem in\nfederated learning is that it is not clear what the optimization objective\nshould be: the standard average risk minimization of supervised learning is\ninadequate in handling several major constraints specific to federated\nlearning, such as communication adaptivity and personalization control. We\nidentify several key desiderata in frameworks for federated learning and\nintroduce a new framework, FLIX, that takes into account the unique challenges\nbrought by federated learning. FLIX has a standard finite-sum form, which\nenables practitioners to tap into the immense wealth of existing (potentially\nnon-local) methods for distributed optimization. Through a smart initialization\nthat does not require any communication, FLIX does not require the use of local\nsteps but is still provably capable of performing dissimilarity regularization\non par with local methods. We give several algorithms for solving the FLIX\nformulation efficiently under communication constraints. Finally, we\ncorroborate our theoretical results with extensive experimentation."
  },
  {
    "id": "arxiv-485",
    "title": "PMD: An Optimal Transportation-based User Distance for Recommender\n  Systems",
    "abstract": "Collaborative filtering, a widely-used recommendation technique, predicts a\nuser's preference by aggregating the ratings from similar users. As a result,\nthese measures cannot fully utilize the rating information and are not suitable\nfor real world sparse data. To solve these issues, we propose a novel user\ndistance measure named Preference Mover's Distance (PMD) which makes full use\nof all ratings made by each user. Our proposed PMD can properly measure the\ndistance between a pair of users even if they have no co-rated items. We show\nthat this measure can be cast as an instance of the Earth Mover's Distance, a\nwell-studied transportation problem for which several highly efficient solvers\nhave been developed. Experimental results show that PMD can help achieve\nsuperior recommendation accuracy than state-of-the-art methods, especially when\ntraining data is very sparse.",
    "text": "PMD: An Optimal Transportation-based User Distance for Recommender\n  Systems\n\nCollaborative filtering, a widely-used recommendation technique, predicts a\nuser's preference by aggregating the ratings from similar users. As a result,\nthese measures cannot fully utilize the rating information and are not suitable\nfor real world sparse data. To solve these issues, we propose a novel user\ndistance measure named Preference Mover's Distance (PMD) which makes full use\nof all ratings made by each user. Our proposed PMD can properly measure the\ndistance between a pair of users even if they have no co-rated items. We show\nthat this measure can be cast as an instance of the Earth Mover's Distance, a\nwell-studied transportation problem for which several highly efficient solvers\nhave been developed. Experimental results show that PMD can help achieve\nsuperior recommendation accuracy than state-of-the-art methods, especially when\ntraining data is very sparse."
  },
  {
    "id": "arxiv-486",
    "title": "A Sea of Words: An In-Depth Analysis of Anchors for Text Data",
    "abstract": "Anchors [Ribeiro et al. (2018)] is a post-hoc, rule-based interpretability\nmethod. For text data, it proposes to explain a decision by highlighting a\nsmall set of words (an anchor) such that the model to explain has similar\noutputs when they are present in a document. In this paper, we present the\nfirst theoretical analysis of Anchors, considering that the search for the best\nanchor is exhaustive. We leverage this analysis to gain insights on the\nbehavior of Anchors on simple models, including elementary if-then rules and\nlinear classifiers.",
    "text": "A Sea of Words: An In-Depth Analysis of Anchors for Text Data\n\nAnchors [Ribeiro et al. (2018)] is a post-hoc, rule-based interpretability\nmethod. For text data, it proposes to explain a decision by highlighting a\nsmall set of words (an anchor) such that the model to explain has similar\noutputs when they are present in a document. In this paper, we present the\nfirst theoretical analysis of Anchors, considering that the search for the best\nanchor is exhaustive. We leverage this analysis to gain insights on the\nbehavior of Anchors on simple models, including elementary if-then rules and\nlinear classifiers."
  },
  {
    "id": "arxiv-487",
    "title": "Deep neural network approximation for high-dimensional parabolic\n  Hamilton-Jacobi-Bellman equations",
    "abstract": "The approximation of solutions to second order Hamilton--Jacobi--Bellman\n(HJB) equations by deep neural networks is investigated. It is shown that for\nHJB equations that arise in the context of the optimal control of certain\nMarkov processes the solution can be approximated by deep neural networks\nwithout incurring the curse of dimension. The dynamics is assumed to depend\naffinely on the controls and the cost depends quadratically on the controls.\nThe admissible controls take values in a bounded set.",
    "text": "Deep neural network approximation for high-dimensional parabolic\n  Hamilton-Jacobi-Bellman equations\n\nThe approximation of solutions to second order Hamilton--Jacobi--Bellman\n(HJB) equations by deep neural networks is investigated. It is shown that for\nHJB equations that arise in the context of the optimal control of certain\nMarkov processes the solution can be approximated by deep neural networks\nwithout incurring the curse of dimension. The dynamics is assumed to depend\naffinely on the controls and the cost depends quadratically on the controls.\nThe admissible controls take values in a bounded set."
  },
  {
    "id": "arxiv-488",
    "title": "CAE-ADMM: Implicit Bitrate Optimization via ADMM-based Pruning in\n  Compressive Autoencoders",
    "abstract": "We introduce ADMM-pruned Compressive AutoEncoder (CAE-ADMM) that uses\nAlternative Direction Method of Multipliers (ADMM) to optimize the trade-off\nbetween distortion and efficiency of lossy image compression. Specifically,\nADMM in our method is to promote sparsity to implicitly optimize the bitrate,\ndifferent from entropy estimators used in the previous research. The\nexperiments on public datasets show that our method outperforms the original\nCAE and some traditional codecs in terms of SSIM/MS-SSIM metrics, at reasonable\ninference speed.",
    "text": "CAE-ADMM: Implicit Bitrate Optimization via ADMM-based Pruning in\n  Compressive Autoencoders\n\nWe introduce ADMM-pruned Compressive AutoEncoder (CAE-ADMM) that uses\nAlternative Direction Method of Multipliers (ADMM) to optimize the trade-off\nbetween distortion and efficiency of lossy image compression. Specifically,\nADMM in our method is to promote sparsity to implicitly optimize the bitrate,\ndifferent from entropy estimators used in the previous research. The\nexperiments on public datasets show that our method outperforms the original\nCAE and some traditional codecs in terms of SSIM/MS-SSIM metrics, at reasonable\ninference speed."
  },
  {
    "id": "arxiv-489",
    "title": "Proximal Langevin Algorithm: Rapid Convergence Under Isoperimetry",
    "abstract": "We study the Proximal Langevin Algorithm (PLA) for sampling from a\nprobability distribution $\\nu = e^{-f}$ on $\\mathbb{R}^n$ under isoperimetry.\nWe prove a convergence guarantee for PLA in Kullback-Leibler (KL) divergence\nwhen $\\nu$ satisfies log-Sobolev inequality (LSI) and $f$ has bounded second\nand third derivatives. This improves on the result for the Unadjusted Langevin\nAlgorithm (ULA), and matches the fastest known rate for sampling under LSI\n(without Metropolis filter) with a better dependence on the LSI constant. We\nalso prove convergence guarantees for PLA in R\\'enyi divergence of order $q >\n1$ when the biased limit satisfies either LSI or Poincar\\'e inequality.",
    "text": "Proximal Langevin Algorithm: Rapid Convergence Under Isoperimetry\n\nWe study the Proximal Langevin Algorithm (PLA) for sampling from a\nprobability distribution $\\nu = e^{-f}$ on $\\mathbb{R}^n$ under isoperimetry.\nWe prove a convergence guarantee for PLA in Kullback-Leibler (KL) divergence\nwhen $\\nu$ satisfies log-Sobolev inequality (LSI) and $f$ has bounded second\nand third derivatives. This improves on the result for the Unadjusted Langevin\nAlgorithm (ULA), and matches the fastest known rate for sampling under LSI\n(without Metropolis filter) with a better dependence on the LSI constant. We\nalso prove convergence guarantees for PLA in R\\'enyi divergence of order $q >\n1$ when the biased limit satisfies either LSI or Poincar\\'e inequality."
  },
  {
    "id": "arxiv-490",
    "title": "An Investigation of Transfer Learning-Based Sentiment Analysis in\n  Japanese",
    "abstract": "Text classification approaches have usually required task-specific model\narchitectures and huge labeled datasets. Recently, thanks to the rise of\ntext-based transfer learning techniques, it is possible to pre-train a language\nmodel in an unsupervised manner and leverage them to perform effective on\ndownstream tasks. In this work we focus on Japanese and show the potential use\nof transfer learning techniques in text classification. Specifically, we\nperform binary and multi-class sentiment classification on the Rakuten product\nreview and Yahoo movie review datasets. We show that transfer learning-based\napproaches perform better than task-specific models trained on 3 times as much\ndata. Furthermore, these approaches perform just as well for language modeling\npre-trained on only 1/30 of the data. We release our pre-trained models and\ncode as open source.",
    "text": "An Investigation of Transfer Learning-Based Sentiment Analysis in\n  Japanese\n\nText classification approaches have usually required task-specific model\narchitectures and huge labeled datasets. Recently, thanks to the rise of\ntext-based transfer learning techniques, it is possible to pre-train a language\nmodel in an unsupervised manner and leverage them to perform effective on\ndownstream tasks. In this work we focus on Japanese and show the potential use\nof transfer learning techniques in text classification. Specifically, we\nperform binary and multi-class sentiment classification on the Rakuten product\nreview and Yahoo movie review datasets. We show that transfer learning-based\napproaches perform better than task-specific models trained on 3 times as much\ndata. Furthermore, these approaches perform just as well for language modeling\npre-trained on only 1/30 of the data. We release our pre-trained models and\ncode as open source."
  },
  {
    "id": "arxiv-491",
    "title": "Tradeoffs Between Contrastive and Supervised Learning: An Empirical\n  Study",
    "abstract": "Contrastive learning has made considerable progress in computer vision,\noutperforming supervised pretraining on a range of downstream datasets.\nHowever, is contrastive learning the better choice in all situations? We\ndemonstrate two cases where it is not. First, under sufficiently small\npretraining budgets, supervised pretraining on ImageNet consistently\noutperforms a comparable contrastive model on eight diverse image\nclassification datasets. This suggests that the common practice of comparing\npretraining approaches at hundreds or thousands of epochs may not produce\nactionable insights for those with more limited compute budgets. Second, even\nwith larger pretraining budgets we identify tasks where supervised learning\nprevails, perhaps because the object-centric bias of supervised pretraining\nmakes the model more resilient to common corruptions and spurious\nforeground-background correlations. These results underscore the need to\ncharacterize tradeoffs of different pretraining objectives across a wider range\nof contexts and training regimes.",
    "text": "Tradeoffs Between Contrastive and Supervised Learning: An Empirical\n  Study\n\nContrastive learning has made considerable progress in computer vision,\noutperforming supervised pretraining on a range of downstream datasets.\nHowever, is contrastive learning the better choice in all situations? We\ndemonstrate two cases where it is not. First, under sufficiently small\npretraining budgets, supervised pretraining on ImageNet consistently\noutperforms a comparable contrastive model on eight diverse image\nclassification datasets. This suggests that the common practice of comparing\npretraining approaches at hundreds or thousands of epochs may not produce\nactionable insights for those with more limited compute budgets. Second, even\nwith larger pretraining budgets we identify tasks where supervised learning\nprevails, perhaps because the object-centric bias of supervised pretraining\nmakes the model more resilient to common corruptions and spurious\nforeground-background correlations. These results underscore the need to\ncharacterize tradeoffs of different pretraining objectives across a wider range\nof contexts and training regimes."
  },
  {
    "id": "arxiv-492",
    "title": "Task Adaptive Parameter Sharing for Multi-Task Learning",
    "abstract": "Adapting pre-trained models with broad capabilities has become standard\npractice for learning a wide range of downstream tasks. The typical approach of\nfine-tuning different models for each task is performant, but incurs a\nsubstantial memory cost. To efficiently learn multiple downstream tasks we\nintroduce Task Adaptive Parameter Sharing (TAPS), a general method for tuning a\nbase model to a new task by adaptively modifying a small, task-specific subset\nof layers. This enables multi-task learning while minimizing resources used and\ncompetition between tasks. TAPS solves a joint optimization problem which\ndetermines which layers to share with the base model and the value of the\ntask-specific weights. Further, a sparsity penalty on the number of active\nlayers encourages weight sharing with the base model. Compared to other\nmethods, TAPS retains high accuracy on downstream tasks while introducing few\ntask-specific parameters. Moreover, TAPS is agnostic to the model architecture\nand requires only minor changes to the training scheme. We evaluate our method\non a suite of fine-tuning tasks and architectures (ResNet, DenseNet, ViT) and\nshow that it achieves state-of-the-art performance while being simple to\nimplement.",
    "text": "Task Adaptive Parameter Sharing for Multi-Task Learning\n\nAdapting pre-trained models with broad capabilities has become standard\npractice for learning a wide range of downstream tasks. The typical approach of\nfine-tuning different models for each task is performant, but incurs a\nsubstantial memory cost. To efficiently learn multiple downstream tasks we\nintroduce Task Adaptive Parameter Sharing (TAPS), a general method for tuning a\nbase model to a new task by adaptively modifying a small, task-specific subset\nof layers. This enables multi-task learning while minimizing resources used and\ncompetition between tasks. TAPS solves a joint optimization problem which\ndetermines which layers to share with the base model and the value of the\ntask-specific weights. Further, a sparsity penalty on the number of active\nlayers encourages weight sharing with the base model. Compared to other\nmethods, TAPS retains high accuracy on downstream tasks while introducing few\ntask-specific parameters. Moreover, TAPS is agnostic to the model architecture\nand requires only minor changes to the training scheme. We evaluate our method\non a suite of fine-tuning tasks and architectures (ResNet, DenseNet, ViT) and\nshow that it achieves state-of-the-art performance while being simple to\nimplement."
  },
  {
    "id": "arxiv-493",
    "title": "Disease Trajectory Maps",
    "abstract": "Medical researchers are coming to appreciate that many diseases are in fact\ncomplex, heterogeneous syndromes composed of subpopulations that express\ndifferent variants of a related complication. Time series data extracted from\nindividual electronic health records (EHR) offer an exciting new way to study\nsubtle differences in the way these diseases progress over time. In this paper,\nwe focus on answering two questions that can be asked using these databases of\ntime series. First, we want to understand whether there are individuals with\nsimilar disease trajectories and whether there are a small number of degrees of\nfreedom that account for differences in trajectories across the population.\nSecond, we want to understand how important clinical outcomes are associated\nwith disease trajectories. To answer these questions, we propose the Disease\nTrajectory Map (DTM), a novel probabilistic model that learns low-dimensional\nrepresentations of sparse and irregularly sampled time series. We propose a\nstochastic variational inference algorithm for learning the DTM that allows the\nmodel to scale to large modern medical datasets. To demonstrate the DTM, we\nanalyze data collected on patients with the complex autoimmune disease,\nscleroderma. We find that DTM learns meaningful representations of disease\ntrajectories and that the representations are significantly associated with\nimportant clinical outcomes.",
    "text": "Disease Trajectory Maps\n\nMedical researchers are coming to appreciate that many diseases are in fact\ncomplex, heterogeneous syndromes composed of subpopulations that express\ndifferent variants of a related complication. Time series data extracted from\nindividual electronic health records (EHR) offer an exciting new way to study\nsubtle differences in the way these diseases progress over time. In this paper,\nwe focus on answering two questions that can be asked using these databases of\ntime series. First, we want to understand whether there are individuals with\nsimilar disease trajectories and whether there are a small number of degrees of\nfreedom that account for differences in trajectories across the population.\nSecond, we want to understand how important clinical outcomes are associated\nwith disease trajectories. To answer these questions, we propose the Disease\nTrajectory Map (DTM), a novel probabilistic model that learns low-dimensional\nrepresentations of sparse and irregularly sampled time series. We propose a\nstochastic variational inference algorithm for learning the DTM that allows the\nmodel to scale to large modern medical datasets. To demonstrate the DTM, we\nanalyze data collected on patients with the complex autoimmune disease,\nscleroderma. We find that DTM learns meaningful representations of disease\ntrajectories and that the representations are significantly associated with\nimportant clinical outcomes."
  },
  {
    "id": "arxiv-494",
    "title": "Neural Temporal Point Processes: A Review",
    "abstract": "Temporal point processes (TPP) are probabilistic generative models for\ncontinuous-time event sequences. Neural TPPs combine the fundamental ideas from\npoint process literature with deep learning approaches, thus enabling\nconstruction of flexible and efficient models. The topic of neural TPPs has\nattracted significant attention in the recent years, leading to the development\nof numerous new architectures and applications for this class of models. In\nthis review paper we aim to consolidate the existing body of knowledge on\nneural TPPs. Specifically, we focus on important design choices and general\nprinciples for defining neural TPP models. Next, we provide an overview of\napplication areas commonly considered in the literature. We conclude this\nsurvey with the list of open challenges and important directions for future\nwork in the field of neural TPPs.",
    "text": "Neural Temporal Point Processes: A Review\n\nTemporal point processes (TPP) are probabilistic generative models for\ncontinuous-time event sequences. Neural TPPs combine the fundamental ideas from\npoint process literature with deep learning approaches, thus enabling\nconstruction of flexible and efficient models. The topic of neural TPPs has\nattracted significant attention in the recent years, leading to the development\nof numerous new architectures and applications for this class of models. In\nthis review paper we aim to consolidate the existing body of knowledge on\nneural TPPs. Specifically, we focus on important design choices and general\nprinciples for defining neural TPP models. Next, we provide an overview of\napplication areas commonly considered in the literature. We conclude this\nsurvey with the list of open challenges and important directions for future\nwork in the field of neural TPPs."
  },
  {
    "id": "arxiv-495",
    "title": "Deepfake pornography as a male gaze on fan culture",
    "abstract": "This essay shows the impact of deepfake technology on fan culture. The\ninnovative technology provided the male audience with an instrument to express\nits ideas and plots. Which subsequently led to the rise of deepfake\npornography. It is often seen as a part of celebrity studies; however, the\nessay shows that it could also be considered a type of fanfic and a product of\nparticipatory culture, sharing community origin, exploitation by commercial\ncompanies and deep sexualisation. These two branches of fanfic evolution can be\nconnected via the genre of machinima pornography. Textual fanfics are mainly\ncreated by females for females, depicting males; otherwise, deepfake\npornography and machinima are made by males and for males targeting females.",
    "text": "Deepfake pornography as a male gaze on fan culture\n\nThis essay shows the impact of deepfake technology on fan culture. The\ninnovative technology provided the male audience with an instrument to express\nits ideas and plots. Which subsequently led to the rise of deepfake\npornography. It is often seen as a part of celebrity studies; however, the\nessay shows that it could also be considered a type of fanfic and a product of\nparticipatory culture, sharing community origin, exploitation by commercial\ncompanies and deep sexualisation. These two branches of fanfic evolution can be\nconnected via the genre of machinima pornography. Textual fanfics are mainly\ncreated by females for females, depicting males; otherwise, deepfake\npornography and machinima are made by males and for males targeting females."
  },
  {
    "id": "arxiv-496",
    "title": "Design and implementation of an environment for Learning to Run a Power\n  Network (L2RPN)",
    "abstract": "This report summarizes work performed as part of an internship at INRIA, in\npartial requirement for the completion of a master degree in math and\ninformatics. The goal of the internship was to develop a software environment\nto simulate electricity transmission in a power grid and actions performed by\noperators to maintain this grid in security. Our environment lends itself to\nautomate the control of the power grid with reinforcement learning agents,\nassisting human operators. It is amenable to organizing benchmarks, including a\nchallenge in machine learning planned by INRIA and RTE for 2019. Our framework,\nbuilt on top of open-source libraries, is available at\nhttps://github.com/MarvinLer/pypownet. In this report we present intermediary\nresults and its usage in the context of a reinforcement learning game.",
    "text": "Design and implementation of an environment for Learning to Run a Power\n  Network (L2RPN)\n\nThis report summarizes work performed as part of an internship at INRIA, in\npartial requirement for the completion of a master degree in math and\ninformatics. The goal of the internship was to develop a software environment\nto simulate electricity transmission in a power grid and actions performed by\noperators to maintain this grid in security. Our environment lends itself to\nautomate the control of the power grid with reinforcement learning agents,\nassisting human operators. It is amenable to organizing benchmarks, including a\nchallenge in machine learning planned by INRIA and RTE for 2019. Our framework,\nbuilt on top of open-source libraries, is available at\nhttps://github.com/MarvinLer/pypownet. In this report we present intermediary\nresults and its usage in the context of a reinforcement learning game."
  },
  {
    "id": "arxiv-497",
    "title": "Local Explanation of Dimensionality Reduction",
    "abstract": "Dimensionality reduction (DR) is a popular method for preparing and analyzing\nhigh-dimensional data. Reduced data representations are less computationally\nintensive and easier to manage and visualize, while retaining a significant\npercentage of their original information. Aside from these advantages, these\nreduced representations can be difficult or impossible to interpret in most\ncircumstances, especially when the DR approach does not provide further\ninformation about which features of the original space led to their\nconstruction. This problem is addressed by Interpretable Machine Learning, a\nsubfield of Explainable Artificial Intelligence that addresses the opacity of\nmachine learning models. However, current research on Interpretable Machine\nLearning has been focused on supervised tasks, leaving unsupervised tasks like\nDimensionality Reduction unexplored. In this paper, we introduce LXDR, a\ntechnique capable of providing local interpretations of the output of DR\ntechniques. Experiment results and two LXDR use case examples are presented to\nevaluate its usefulness.",
    "text": "Local Explanation of Dimensionality Reduction\n\nDimensionality reduction (DR) is a popular method for preparing and analyzing\nhigh-dimensional data. Reduced data representations are less computationally\nintensive and easier to manage and visualize, while retaining a significant\npercentage of their original information. Aside from these advantages, these\nreduced representations can be difficult or impossible to interpret in most\ncircumstances, especially when the DR approach does not provide further\ninformation about which features of the original space led to their\nconstruction. This problem is addressed by Interpretable Machine Learning, a\nsubfield of Explainable Artificial Intelligence that addresses the opacity of\nmachine learning models. However, current research on Interpretable Machine\nLearning has been focused on supervised tasks, leaving unsupervised tasks like\nDimensionality Reduction unexplored. In this paper, we introduce LXDR, a\ntechnique capable of providing local interpretations of the output of DR\ntechniques. Experiment results and two LXDR use case examples are presented to\nevaluate its usefulness."
  },
  {
    "id": "arxiv-498",
    "title": "Traffic-Twitter Transformer: A Nature Language Processing-joined Framework For Network-wide Traffic Forecasting",
    "abstract": "With accurate and timely traffic forecasting, the impacted traffic conditions\ncan be predicted in advance to guide agencies and residents to respond to\nchanges in traffic patterns appropriately. However, existing works on traffic\nforecasting mainly relied on historical traffic patterns confining to\nshort-term prediction, under 1 hour, for instance. To better manage future\nroadway capacity and accommodate social and human impacts, it is crucial to\npropose a flexible and comprehensive framework to predict physical-aware\nlong-term traffic conditions for public users and transportation agencies. In\nthis paper, the gap of robust long-term traffic forecasting was bridged by\ntaking social media features into consideration. A correlation study and a\nlinear regression model were first implemented to evaluate the significance of\nthe correlation between two time-series data, traffic intensity and Twitter\ndata intensity. Two time-series data were then fed into our proposed\nsocial-aware framework, Traffic-Twitter Transformer, which integrated Nature\nLanguage representations into time-series records for long-term traffic\nprediction. Experimental results in the Great Seattle Area showed that our\nproposed model outperformed baseline models in all evaluation matrices. This\nNLP-joined social-aware framework can become a valuable implement of\nnetwork-wide traffic prediction and management for traffic agencies.",
    "text": "Traffic-Twitter Transformer: A Nature Language Processing-joined Framework For Network-wide Traffic Forecasting\n\nWith accurate and timely traffic forecasting, the impacted traffic conditions\ncan be predicted in advance to guide agencies and residents to respond to\nchanges in traffic patterns appropriately. However, existing works on traffic\nforecasting mainly relied on historical traffic patterns confining to\nshort-term prediction, under 1 hour, for instance. To better manage future\nroadway capacity and accommodate social and human impacts, it is crucial to\npropose a flexible and comprehensive framework to predict physical-aware\nlong-term traffic conditions for public users and transportation agencies. In\nthis paper, the gap of robust long-term traffic forecasting was bridged by\ntaking social media features into consideration. A correlation study and a\nlinear regression model were first implemented to evaluate the significance of\nthe correlation between two time-series data, traffic intensity and Twitter\ndata intensity. Two time-series data were then fed into our proposed\nsocial-aware framework, Traffic-Twitter Transformer, which integrated Nature\nLanguage representations into time-series records for long-term traffic\nprediction. Experimental results in the Great Seattle Area showed that our\nproposed model outperformed baseline models in all evaluation matrices. This\nNLP-joined social-aware framework can become a valuable implement of\nnetwork-wide traffic prediction and management for traffic agencies."
  },
  {
    "id": "arxiv-499",
    "title": "Clustering For Point Pattern Data",
    "abstract": "Clustering is one of the most common unsupervised learning tasks in machine\nlearning and data mining. Clustering algorithms have been used in a plethora of\napplications across several scientific fields. However, there has been limited\nresearch in the clustering of point patterns - sets or multi-sets of unordered\nelements - that are found in numerous applications and data sources. In this\npaper, we propose two approaches for clustering point patterns. The first is a\nnon-parametric method based on novel distances for sets. The second is a\nmodel-based approach, formulated via random finite set theory, and solved by\nthe Expectation-Maximization algorithm. Numerical experiments show that the\nproposed methods perform well on both simulated and real data.",
    "text": "Clustering For Point Pattern Data\n\nClustering is one of the most common unsupervised learning tasks in machine\nlearning and data mining. Clustering algorithms have been used in a plethora of\napplications across several scientific fields. However, there has been limited\nresearch in the clustering of point patterns - sets or multi-sets of unordered\nelements - that are found in numerous applications and data sources. In this\npaper, we propose two approaches for clustering point patterns. The first is a\nnon-parametric method based on novel distances for sets. The second is a\nmodel-based approach, formulated via random finite set theory, and solved by\nthe Expectation-Maximization algorithm. Numerical experiments show that the\nproposed methods perform well on both simulated and real data."
  }
]