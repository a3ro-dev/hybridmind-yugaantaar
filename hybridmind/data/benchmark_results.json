{
  "vector": {
    "hybridmind": [
      {
        "query": "transformer attention mechanism neural network",
        "time_ms": 3671.64,
        "count": 10
      },
      {
        "query": "deep learning optimization gradient descent",
        "time_ms": 10.09,
        "count": 10
      },
      {
        "query": "convolutional neural network image classification",
        "time_ms": 12.28,
        "count": 10
      },
      {
        "query": "reinforcement learning policy gradient",
        "time_ms": 10.51,
        "count": 10
      },
      {
        "query": "generative adversarial network image synthesis",
        "time_ms": 11.51,
        "count": 10
      },
      {
        "query": "natural language processing BERT model",
        "time_ms": 14.14,
        "count": 10
      },
      {
        "query": "graph neural network node embedding",
        "time_ms": 10.6,
        "count": 10
      },
      {
        "query": "recurrent neural network sequence modeling",
        "time_ms": 10.92,
        "count": 10
      },
      {
        "query": "autoencoder latent representation learning",
        "time_ms": 9.69,
        "count": 10
      },
      {
        "query": "machine learning model training efficiency",
        "time_ms": 9.67,
        "count": 10
      }
    ],
    "chromadb": [
      {
        "query": "transformer attention mechanism neural network",
        "time_ms": 3668.386459350586,
        "count": 10
      },
      {
        "query": "deep learning optimization gradient descent",
        "time_ms": 10.002613067626953,
        "count": 10
      },
      {
        "query": "convolutional neural network image classification",
        "time_ms": 10.002851486206055,
        "count": 10
      },
      {
        "query": "reinforcement learning policy gradient",
        "time_ms": 10.502815246582031,
        "count": 10
      },
      {
        "query": "generative adversarial network image synthesis",
        "time_ms": 11.009454727172852,
        "count": 10
      },
      {
        "query": "natural language processing BERT model",
        "time_ms": 11.518001556396484,
        "count": 10
      },
      {
        "query": "graph neural network node embedding",
        "time_ms": 10.000944137573242,
        "count": 10
      },
      {
        "query": "recurrent neural network sequence modeling",
        "time_ms": 10.373830795288086,
        "count": 10
      },
      {
        "query": "autoencoder latent representation learning",
        "time_ms": 11.997699737548828,
        "count": 10
      },
      {
        "query": "machine learning model training efficiency",
        "time_ms": 10.012149810791016,
        "count": 10
      }
    ]
  },
  "graph": {
    "hybridmind": [
      {
        "start_id": "arxiv-493",
        "time_ms": 1.18,
        "count": 22
      },
      {
        "start_id": "arxiv-494",
        "time_ms": 2.47,
        "count": 47
      },
      {
        "start_id": "arxiv-495",
        "time_ms": 0.85,
        "count": 19
      },
      {
        "start_id": "arxiv-496",
        "time_ms": 0.34,
        "count": 7
      },
      {
        "start_id": "arxiv-497",
        "time_ms": 0.6,
        "count": 15
      },
      {
        "start_id": "arxiv-498",
        "time_ms": 1.15,
        "count": 28
      },
      {
        "start_id": "arxiv-499",
        "time_ms": 0.15,
        "count": 2
      },
      {
        "start_id": "arxiv-486",
        "time_ms": 0.4,
        "count": 9
      },
      {
        "start_id": "arxiv-487",
        "time_ms": 0.68,
        "count": 16
      },
      {
        "start_id": "arxiv-488",
        "time_ms": 1.03,
        "count": 27
      }
    ],
    "neo4j": []
  },
  "hybrid": {
    "hybridmind": [
      {
        "query": "transformer attention mechanism neural network",
        "time_ms": 17.19,
        "count": 10,
        "results": [
          {
            "node_id": "arxiv-209",
            "text": "Universal Graph Transformer Self-Attention Networks\n\nWe introduce a transformer-based GNN model, named UGformer, to learn graph\nrepresentations. In particular, we present two UGformer variants, wherein the\nfirst variant (publicized in September 2019) is to leverage the transformer on\na set of sampled neighbors for each input node, while the second (publicized in\nMay 2021) is to leverage the transformer on all input nodes. Experimental\nresults demonstrate that the first UGformer variant achieves state-of-the-art\naccuracies on benchmark datasets for graph classification in both inductive\nsetting and unsupervised transductive setting; and the second UGformer variant\nobtains state-of-the-art accuracies for inductive text classification. The code\nis available at: \\url{https://github.com/daiquocnguyen/Graph-Transformer}.",
            "metadata": {
              "title": "Universal Graph Transformer Self-Attention Networks",
              "source": "arxiv"
            },
            "vector_score": 0.5745,
            "graph_score": 1.0,
            "combined_score": 0.7447,
            "reasoning": "Moderate semantic similarity (57%), strongly connected in graph (100%)"
          },
          {
            "node_id": "arxiv-455",
            "text": "An Artificial Neural Network Architecture Based on Context\n  Transformations in Cortical Minicolumns\n\nCortical minicolumns are considered a model of cortical organization. Their\nfunction is still a source of research and not reflected properly in modern\narchitecture of nets in algorithms of Artificial Intelligence. We assume its\nfunction and describe it in this article. Furthermore, we show how this\nproposal allows to construct a new architecture, that is not based on\nconvolutional neural networks, test it on MNIST data and receive close to\nConvolutional Neural Network accuracy. We also show that the proposed\narchitecture possesses an ability to train on a small quantity of samples. To\nachieve these results, we enable the minicolumns to remember context\ntransformations.",
            "metadata": {
              "title": "An Artificial Neural Network Architecture Based on Context\n  Transformations in Cortical Minicolumns",
              "source": "arxiv"
            },
            "vector_score": 0.486,
            "graph_score": 1.0,
            "combined_score": 0.6916,
            "reasoning": "Low semantic similarity (49%), strongly connected in graph (100%)"
          },
          {
            "node_id": "arxiv-202",
            "text": "PoNet: Pooling Network for Efficient Token Mixing in Long Sequences\n\nTransformer-based models have achieved great success in various NLP, vision,\nand speech tasks. However, the core of Transformer, the self-attention\nmechanism, has a quadratic time and memory complexity with respect to the\nsequence length, which hinders applications of Transformer-based models to long\nsequences. Many approaches have been proposed to mitigate this problem, such as\nsparse attention mechanisms, low-rank matrix approximations and scalable\nkernels, and token mixing alternatives to self-attention. We propose a novel\nPooling Network (PoNet) for token mixing in long sequences with linear\ncomplexity. We design multi-granularity pooling and pooling fusion to capture\ndifferent levels of contextual information and combine their interactions with\ntokens. On the Long Range Arena benchmark, PoNet significantly outperforms\nTransformer and achieves competitive accuracy, while being only slightly slower\nthan the fastest model, FNet, across all sequence lengths measured on GPUs. We\nalso conduct systematic studies on the transfer learning capability of PoNet\nand observe that PoNet achieves 95.7% of the accuracy of BERT on the GLUE\nbenchmark, outperforming FNet by 4.5% relative. Comprehensive ablation analysis\ndemonstrates effectiveness of the designed multi-granularity pooling and\npooling fusion for token mixing in long sequences and efficacy of the designed\npre-training tasks for PoNet to learn transferable contextualized language\nrepresentations.",
            "metadata": {
              "title": "PoNet: Pooling Network for Efficient Token Mixing in Long Sequences",
              "source": "arxiv"
            },
            "vector_score": 0.4629,
            "graph_score": 1.0,
            "combined_score": 0.6777,
            "reasoning": "Low semantic similarity (46%), strongly connected in graph (100%)"
          }
        ]
      },
      {
        "query": "deep learning optimization gradient descent",
        "time_ms": 14.5,
        "count": 10,
        "results": [
          {
            "node_id": "arxiv-138",
            "text": "Characterization of Gradient Dominance and Regularity Conditions for\n  Neural Networks\n\nThe past decade has witnessed a successful application of deep learning to\nsolving many challenging problems in machine learning and artificial\nintelligence. However, the loss functions of deep neural networks (especially\nnonlinear networks) are still far from being well understood from a theoretical\naspect. In this paper, we enrich the current understanding of the landscape of\nthe square loss functions for three types of neural networks. Specifically,\nwhen the parameter matrices are square, we provide an explicit characterization\nof the global minimizers for linear networks, linear residual networks, and\nnonlinear networks with one hidden layer. Then, we establish two quadratic\ntypes of landscape properties for the square loss of these neural networks,\ni.e., the gradient dominance condition within the neighborhood of their full\nrank global minimizers, and the regularity condition along certain directions\nand within the neighborhood of their global minimizers. These two landscape\nproperties are desirable for the optimization around the global minimizers of\nthe loss function for these neural networks.",
            "metadata": {
              "title": "Characterization of Gradient Dominance and Regularity Conditions for\n  Neural Networks",
              "source": "arxiv"
            },
            "vector_score": 0.5889,
            "graph_score": 1.0,
            "combined_score": 0.7533,
            "reasoning": "Moderate semantic similarity (59%), strongly connected in graph (100%)"
          },
          {
            "node_id": "arxiv-179",
            "text": "Constrained Monotonic Neural Networks\n\nDeep neural networks are becoming increasingly popular in approximating\narbitrary functions from noisy data. But wider adoption is being hindered by\nthe need to explain such models and to impose additional constraints on them.\nMonotonicity constraint is one of the most requested properties in real-world\nscenarios and is the focus of this paper. One of the oldest ways to construct a\nmonotonic fully connected neural network is to constrain its weights to be\nnon-negative while employing a monotonic activation function. Unfortunately,\nthis construction does not work with popular non-saturated activation functions\nsuch as ReLU, ELU, SELU etc, as it can only approximate convex functions. We\nshow this shortcoming can be fixed by employing the original activation\nfunction for a part of the neurons in the layer, and employing its point\nreflection for the other part. Our experiments show this approach of building\nmonotonic deep neural networks have matching or better accuracy when compared\nto other state-of-the-art methods such as deep lattice networks or monotonic\nnetworks obtained by heuristic regularization. This method is the simplest one\nin the sense of having the least number of parameters, not requiring any\nmodifications to the learning procedure or steps post-learning steps.",
            "metadata": {
              "title": "Constrained Monotonic Neural Networks",
              "source": "arxiv"
            },
            "vector_score": 0.5546,
            "graph_score": 1.0,
            "combined_score": 0.7328,
            "reasoning": "Moderate semantic similarity (55%), strongly connected in graph (100%)"
          },
          {
            "node_id": "arxiv-129",
            "text": "A Frobenius norm regularization method for convolutional kernels to\n  avoid unstable gradient problem\n\nConvolutional neural network is a very important model of deep learning. It\ncan help avoid the exploding/vanishing gradient problem and improve the\ngeneralizability of a neural network if the singular values of the Jacobian of\na layer are bounded around $1$ in the training process. We propose a new\npenalty function for a convolutional kernel to let the singular values of the\ncorresponding transformation matrix are bounded around $1$. We show how to\ncarry out the gradient type methods. The penalty is about the structured\ntransformation matrix corresponding to a convolutional kernel. This provides a\nnew regularization method about the weights of convolutional layers.",
            "metadata": {
              "title": "A Frobenius norm regularization method for convolutional kernels to\n  avoid unstable gradient problem",
              "source": "arxiv"
            },
            "vector_score": 0.5173,
            "graph_score": 1.0,
            "combined_score": 0.7104,
            "reasoning": "Moderate semantic similarity (52%), strongly connected in graph (100%)"
          }
        ]
      },
      {
        "query": "convolutional neural network image classification",
        "time_ms": 11.79,
        "count": 10,
        "results": [
          {
            "node_id": "arxiv-341",
            "text": "Agricultural Plantation Classification using Transfer Learning Approach based on CNN\n\nHyper-spectral images are images captured from a satellite that gives spatial\nand spectral information of specific region.A Hyper-spectral image contains\nmuch more number of channels as compared to a RGB image, hence containing more\ninformation about entities within the image. It makes them well suited for the\nclassification of objects in a snap. In the past years, the efficiency of\nhyper-spectral image recognition has increased significantly with deep\nlearning. The Convolution Neural Network(CNN) and Multi-Layer Perceptron(MLP)\nhas demonstrated to be an excellent process of classifying images. However,\nthey suffer from the issues of long training time and requirement of large\namounts of the labeled data, to achieve the expected outcome. These issues\nbecome more complex while dealing with hyper-spectral images. To decrease the\ntraining time and reduce the dependence on large labeled data-set, we propose\nusing the method of transfer learning.The features learned by CNN and MLP\nmodels are then used by the transfer learning model to solve a new\nclassification problem on an unseen dataset. A detailed comparison of CNN and\nmultiple MLP architectural models is performed, to determine an optimum\narchitecture that suits best the objective. The results show that the scaling\nof layers not always leads to increase in accuracy but often leads to\nover-fitting, and also an increase in the training time.The training time is\nreduced to greater extent by applying the transfer learning approach rather\nthan just approaching the problem by directly training a new model on large\ndata-sets, without much affecting the accuracy.",
            "metadata": {
              "title": "Agricultural Plantation Classification using Transfer Learning Approach based on CNN",
              "source": "arxiv"
            },
            "vector_score": 0.5645,
            "graph_score": 1.0,
            "combined_score": 0.7387,
            "reasoning": "Moderate semantic similarity (56%), strongly connected in graph (100%)"
          },
          {
            "node_id": "arxiv-321",
            "text": "Scattering Networks for Hybrid Representation Learning\n\nScattering networks are a class of designed Convolutional Neural Networks\n(CNNs) with fixed weights. We argue they can serve as generic representations\nfor modelling images. In particular, by working in scattering space, we achieve\ncompetitive results both for supervised and unsupervised learning tasks, while\nmaking progress towards constructing more interpretable CNNs. For supervised\nlearning, we demonstrate that the early layers of CNNs do not necessarily need\nto be learned, and can be replaced with a scattering network instead. Indeed,\nusing hybrid architectures, we achieve the best results with predefined\nrepresentations to-date, while being competitive with end-to-end learned CNNs.\nSpecifically, even applying a shallow cascade of small-windowed scattering\ncoefficients followed by 1$\\times$1-convolutions results in AlexNet accuracy on\nthe ILSVRC2012 classification task. Moreover, by combining scattering networks\nwith deep residual networks, we achieve a single-crop top-5 error of 11.4% on\nILSVRC2012. Also, we show they can yield excellent performance in the small\nsample regime on CIFAR-10 and STL-10 datasets, exceeding their end-to-end\ncounterparts, through their ability to incorporate geometrical priors. For\nunsupervised learning, scattering coefficients can be a competitive\nrepresentation that permits image recovery. We use this fact to train hybrid\nGANs to generate images. Finally, we empirically analyze several properties\nrelated to stability and reconstruction of images from scattering coefficients.",
            "metadata": {
              "title": "Scattering Networks for Hybrid Representation Learning",
              "source": "arxiv"
            },
            "vector_score": 0.5264,
            "graph_score": 1.0,
            "combined_score": 0.7158,
            "reasoning": "Moderate semantic similarity (53%), strongly connected in graph (100%)"
          },
          {
            "node_id": "arxiv-203",
            "text": "Imaging Time-Series to Improve Classification and Imputation\n\nInspired by recent successes of deep learning in computer vision, we propose\na novel framework for encoding time series as different types of images,\nnamely, Gramian Angular Summation/Difference Fields (GASF/GADF) and Markov\nTransition Fields (MTF). This enables the use of techniques from computer\nvision for time series classification and imputation. We used Tiled\nConvolutional Neural Networks (tiled CNNs) on 20 standard datasets to learn\nhigh-level features from the individual and compound GASF-GADF-MTF images. Our\napproaches achieve highly competitive results when compared to nine of the\ncurrent best time series classification approaches. Inspired by the bijection\nproperty of GASF on 0/1 rescaled data, we train Denoised Auto-encoders (DA) on\nthe GASF images of four standard and one synthesized compound dataset. The\nimputation MSE on test data is reduced by 12.18%-48.02% when compared to using\nthe raw data. An analysis of the features and weights learned via tiled CNNs\nand DAs explains why the approaches work.",
            "metadata": {
              "title": "Imaging Time-Series to Improve Classification and Imputation",
              "source": "arxiv"
            },
            "vector_score": 0.5167,
            "graph_score": 1.0,
            "combined_score": 0.71,
            "reasoning": "Moderate semantic similarity (52%), strongly connected in graph (100%)"
          }
        ]
      },
      {
        "query": "reinforcement learning policy gradient",
        "time_ms": 10.38,
        "count": 10,
        "results": [
          {
            "node_id": "arxiv-200",
            "text": "Off-Policy Actor-Critic with Emphatic Weightings\n\nA variety of theoretically-sound policy gradient algorithms exist for the\non-policy setting due to the policy gradient theorem, which provides a\nsimplified form for the gradient. The off-policy setting, however, has been\nless clear due to the existence of multiple objectives and the lack of an\nexplicit off-policy policy gradient theorem. In this work, we unify these\nobjectives into one off-policy objective, and provide a policy gradient theorem\nfor this unified objective. The derivation involves emphatic weightings and\ninterest functions. We show multiple strategies to approximate the gradients,\nin an algorithm called Actor Critic with Emphatic weightings (ACE). We prove in\na counterexample that previous (semi-gradient) off-policy actor-critic\nmethods--particularly OffPAC and DPG--converge to the wrong solution whereas\nACE finds the optimal solution. We also highlight why these semi-gradient\napproaches can still perform well in practice, suggesting strategies for\nvariance reduction in ACE. We empirically study several variants of ACE on two\nclassic control environments and an image-based environment designed to\nillustrate the tradeoffs made by each gradient approximation. We find that by\napproximating the emphatic weightings directly, ACE performs as well as or\nbetter than OffPAC in all settings tested.",
            "metadata": {
              "title": "Off-Policy Actor-Critic with Emphatic Weightings",
              "source": "arxiv"
            },
            "vector_score": 0.6342,
            "graph_score": 1.0,
            "combined_score": 0.7805,
            "reasoning": "Moderate semantic similarity (63%), strongly connected in graph (100%)"
          },
          {
            "node_id": "arxiv-316",
            "text": "Risk-Constrained Reinforcement Learning with Percentile Risk Criteria\n\nIn many sequential decision-making problems one is interested in minimizing\nan expected cumulative cost while taking into account \\emph{risk}, i.e.,\nincreased awareness of events of small probability and high consequences.\nAccordingly, the objective of this paper is to present efficient reinforcement\nlearning algorithms for risk-constrained Markov decision processes (MDPs),\nwhere risk is represented via a chance constraint or a constraint on the\nconditional value-at-risk (CVaR) of the cumulative cost. We collectively refer\nto such problems as percentile risk-constrained MDPs.\n  Specifically, we first derive a formula for computing the gradient of the\nLagrangian function for percentile risk-constrained MDPs. Then, we devise\npolicy gradient and actor-critic algorithms that (1) estimate such gradient,\n(2) update the policy in the descent direction, and (3) update the Lagrange\nmultiplier in the ascent direction. For these algorithms we prove convergence\nto locally optimal policies. Finally, we demonstrate the effectiveness of our\nalgorithms in an optimal stopping problem and an online marketing application.",
            "metadata": {
              "title": "Risk-Constrained Reinforcement Learning with Percentile Risk Criteria",
              "source": "arxiv"
            },
            "vector_score": 0.6247,
            "graph_score": 1.0,
            "combined_score": 0.7748,
            "reasoning": "Moderate semantic similarity (62%), strongly connected in graph (100%)"
          },
          {
            "node_id": "arxiv-288",
            "text": "Improved Soft Actor-Critic: Mixing Prioritized Off-Policy Samples with\n  On-Policy Experience\n\nSoft Actor-Critic (SAC) is an off-policy actor-critic reinforcement learning\nalgorithm, essentially based on entropy regularization. SAC trains a policy by\nmaximizing the trade-off between expected return and entropy (randomness in the\npolicy). It has achieved state-of-the-art performance on a range of\ncontinuous-control benchmark tasks, outperforming prior on-policy and\noff-policy methods. SAC works in an off-policy fashion where data are sampled\nuniformly from past experiences (stored in a buffer) using which parameters of\nthe policy and value function networks are updated. We propose certain crucial\nmodifications for boosting the performance of SAC and make it more sample\nefficient. In our proposed improved SAC, we firstly introduce a new\nprioritization scheme for selecting better samples from the experience replay\nbuffer. Secondly we use a mixture of the prioritized off-policy data with the\nlatest on-policy data for training the policy and the value function networks.\nWe compare our approach with the vanilla SAC and some recent variants of SAC\nand show that our approach outperforms the said algorithmic benchmarks. It is\ncomparatively more stable and sample efficient when tested on a number of\ncontinuous control tasks in MuJoCo environments.",
            "metadata": {
              "title": "Improved Soft Actor-Critic: Mixing Prioritized Off-Policy Samples with\n  On-Policy Experience",
              "source": "arxiv"
            },
            "vector_score": 0.5974,
            "graph_score": 1.0,
            "combined_score": 0.7584,
            "reasoning": "Moderate semantic similarity (60%), strongly connected in graph (100%)"
          }
        ]
      },
      {
        "query": "generative adversarial network image synthesis",
        "time_ms": 13.63,
        "count": 10,
        "results": [
          {
            "node_id": "arxiv-342",
            "text": "Unsupervised learning for cross-domain medical image synthesis using\n  deformation invariant cycle consistency networks\n\nRecently, the cycle-consistent generative adversarial networks (CycleGAN) has\nbeen widely used for synthesis of multi-domain medical images. The\ndomain-specific nonlinear deformations captured by CycleGAN make the\nsynthesized images difficult to be used for some applications, for example,\ngenerating pseudo-CT for PET-MR attenuation correction. This paper presents a\ndeformation-invariant CycleGAN (DicycleGAN) method using deformable\nconvolutional layers and new cycle-consistency losses. Its robustness dealing\nwith data that suffer from domain-specific nonlinear deformations has been\nevaluated through comparison experiments performed on a multi-sequence brain MR\ndataset and a multi-modality abdominal dataset. Our method has displayed its\nability to generate synthesized data that is aligned with the source while\nmaintaining a proper quality of signal compared to CycleGAN-generated data. The\nproposed model also obtained comparable performance with CycleGAN when data\nfrom the source and target domains are alignable through simple affine\ntransformations.",
            "metadata": {
              "title": "Unsupervised learning for cross-domain medical image synthesis using\n  deformation invariant cycle consistency networks",
              "source": "arxiv"
            },
            "vector_score": 0.5962,
            "graph_score": 1.0,
            "combined_score": 0.7577,
            "reasoning": "Moderate semantic similarity (60%), strongly connected in graph (100%)"
          },
          {
            "node_id": "arxiv-16",
            "text": "Generative Adversarial Networks (GANs): What it can generate and What it\n  cannot?\n\nIn recent years, Generative Adversarial Networks (GANs) have received\nsignificant attention from the research community. With a straightforward\nimplementation and outstanding results, GANs have been used for numerous\napplications. Despite the success, GANs lack a proper theoretical explanation.\nThese models suffer from issues like mode collapse, non-convergence, and\ninstability during training. To address these issues, researchers have proposed\ntheoretically rigorous frameworks inspired by varied fields of Game theory,\nStatistical theory, Dynamical systems, etc.\n  In this paper, we propose to give an appropriate structure to study these\ncontributions systematically. We essentially categorize the papers based on the\nissues they raise and the kind of novelty they introduce to address them.\nBesides, we provide insight into how each of the discussed articles solves the\nconcerned problems. We compare and contrast different results and put forth a\nsummary of theoretical contributions about GANs with focus on image/visual\napplications. We expect this summary paper to give a bird's eye view to a\nperson wishing to understand the theoretical progress in GANs so far.",
            "metadata": {
              "title": "Generative Adversarial Networks (GANs): What it can generate and What it\n  cannot?",
              "source": "arxiv"
            },
            "vector_score": 0.5837,
            "graph_score": 1.0,
            "combined_score": 0.7502,
            "reasoning": "Moderate semantic similarity (58%), strongly connected in graph (100%)"
          },
          {
            "node_id": "arxiv-424",
            "text": "DwNet: Dense warp-based network for pose-guided human video generation\n\nGeneration of realistic high-resolution videos of human subjects is a\nchallenging and important task in computer vision. In this paper, we focus on\nhuman motion transfer - generation of a video depicting a particular subject,\nobserved in a single image, performing a series of motions exemplified by an\nauxiliary (driving) video. Our GAN-based architecture, DwNet, leverages dense\nintermediate pose-guided representation and refinement process to warp the\nrequired subject appearance, in the form of the texture, from a source image\ninto a desired pose. Temporal consistency is maintained by further conditioning\nthe decoding process within a GAN on the previously generated frame. In this\nway a video is generated in an iterative and recurrent fashion. We illustrate\nthe efficacy of our approach by showing state-of-the-art quantitative and\nqualitative performance on two benchmark datasets: TaiChi and Fashion Modeling.\nThe latter is collected by us and will be made publicly available to the\ncommunity.",
            "metadata": {
              "title": "DwNet: Dense warp-based network for pose-guided human video generation",
              "source": "arxiv"
            },
            "vector_score": 0.5488,
            "graph_score": 1.0,
            "combined_score": 0.7293,
            "reasoning": "Moderate semantic similarity (55%), strongly connected in graph (100%)"
          }
        ]
      },
      {
        "query": "natural language processing BERT model",
        "time_ms": 12.13,
        "count": 10,
        "results": [
          {
            "node_id": "arxiv-317",
            "text": "Attention Word Embedding\n\nWord embedding models learn semantically rich vector representations of words\nand are widely used to initialize natural processing language (NLP) models. The\npopular continuous bag-of-words (CBOW) model of word2vec learns a vector\nembedding by masking a given word in a sentence and then using the other words\nas a context to predict it. A limitation of CBOW is that it equally weights the\ncontext words when making a prediction, which is inefficient, since some words\nhave higher predictive value than others. We tackle this inefficiency by\nintroducing the Attention Word Embedding (AWE) model, which integrates the\nattention mechanism into the CBOW model. We also propose AWE-S, which\nincorporates subword information. We demonstrate that AWE and AWE-S outperform\nthe state-of-the-art word embedding models both on a variety of word similarity\ndatasets and when used for initialization of NLP models.",
            "metadata": {
              "title": "Attention Word Embedding",
              "source": "arxiv"
            },
            "vector_score": 0.4964,
            "graph_score": 1.0,
            "combined_score": 0.6978,
            "reasoning": "Low semantic similarity (50%), strongly connected in graph (100%)"
          },
          {
            "node_id": "arxiv-242",
            "text": "A New Framework for Machine Intelligence: Concepts and Prototype\n\nMachine learning (ML) and artificial intelligence (AI) have become hot topics\nin many information processing areas, from chatbots to scientific data\nanalysis. At the same time, there is uncertainty about the possibility of\nextending predominant ML technologies to become general solutions with\ncontinuous learning capabilities. Here, a simple, yet comprehensive,\ntheoretical framework for intelligent systems is presented. A combination of\nMirror Compositional Representations (MCR) and a Solution-Critic Loop (SCL) is\nproposed as a generic approach for different types of problems. A prototype\nimplementation is presented for document comparison using English Wikipedia\ncorpus.",
            "metadata": {
              "title": "A New Framework for Machine Intelligence: Concepts and Prototype",
              "source": "arxiv"
            },
            "vector_score": 0.4371,
            "graph_score": 1.0,
            "combined_score": 0.6623,
            "reasoning": "Low semantic similarity (44%), strongly connected in graph (100%)"
          },
          {
            "node_id": "arxiv-236",
            "text": "An Interpretable Deep Learning System for Automatically Scoring Request\n  for Proposals\n\nThe Managed Care system within Medicaid (US Healthcare) uses Request For\nProposals (RFP) to award contracts for various healthcare and related services.\nRFP responses are very detailed documents (hundreds of pages) submitted by\ncompeting organisations to win contracts. Subject matter expertise and domain\nknowledge play an important role in preparing RFP responses along with analysis\nof historical submissions. Automated analysis of these responses through\nNatural Language Processing (NLP) systems can reduce time and effort needed to\nexplore historical responses, and assisting in writing better responses. Our\nwork draws parallels between scoring RFPs and essay scoring models, while\nhighlighting new challenges and the need for interpretability. Typical scoring\nmodels focus on word level impacts to grade essays and other short write-ups.\nWe propose a novel Bi-LSTM based regression model, and provide deeper insight\ninto phrases which latently impact scoring of responses. We contend the merits\nof our proposed methodology using extensive quantitative experiments. We also\nqualitatively asses the impact of important phrases using human evaluators.\nFinally, we introduce a novel problem statement that can be used to further\nimprove the state of the art in NLP based automatic scoring systems.",
            "metadata": {
              "title": "An Interpretable Deep Learning System for Automatically Scoring Request\n  for Proposals",
              "source": "arxiv"
            },
            "vector_score": 0.4359,
            "graph_score": 1.0,
            "combined_score": 0.6615,
            "reasoning": "Low semantic similarity (44%), strongly connected in graph (100%)"
          }
        ]
      },
      {
        "query": "graph neural network node embedding",
        "time_ms": 12.41,
        "count": 10,
        "results": [
          {
            "node_id": "arxiv-438",
            "text": "Towards Expressive Graph Representation\n\nGraph Neural Network (GNN) aggregates the neighborhood of each node into the\nnode embedding and shows its powerful capability for graph representation\nlearning. However, most existing GNN variants aggregate the neighborhood\ninformation in a fixed non-injective fashion, which may map different graphs or\nnodes to the same embedding, reducing the model expressiveness. We present a\ntheoretical framework to design a continuous injective set function for\nneighborhood aggregation in GNN. Using the framework, we propose expressive GNN\nthat aggregates the neighborhood of each node with a continuous injective set\nfunction, so that a GNN layer maps similar nodes with similar neighborhoods to\nsimilar embeddings, different nodes to different embeddings and the equivalent\nnodes or isomorphic graphs to the same embeddings. Moreover, the proposed\nexpressive GNN can naturally learn expressive representations for graphs with\ncontinuous node attributes. We validate the proposed expressive GNN (ExpGNN)\nfor graph classification on multiple benchmark datasets including simple graphs\nand attributed graphs. The experimental results demonstrate that our model\nachieves state-of-the-art performances on most of the benchmarks.",
            "metadata": {
              "title": "Towards Expressive Graph Representation",
              "source": "arxiv"
            },
            "vector_score": 0.6404,
            "graph_score": 1.0,
            "combined_score": 0.7842,
            "reasoning": "Moderate semantic similarity (64%), strongly connected in graph (100%)"
          },
          {
            "node_id": "arxiv-40",
            "text": "dynnode2vec: Scalable Dynamic Network Embedding\n\nNetwork representation learning in low dimensional vector space has attracted\nconsiderable attention in both academic and industrial domains. Most real-world\nnetworks are dynamic with addition/deletion of nodes and edges. The existing\ngraph embedding methods are designed for static networks and they cannot\ncapture evolving patterns in a large dynamic network. In this paper, we propose\na dynamic embedding method, dynnode2vec, based on the well-known graph\nembedding method node2vec. Node2vec is a random walk based embedding method for\nstatic networks. Applying static network embedding in dynamic settings has two\ncrucial problems: 1) Generating random walks for every time step is time\nconsuming 2) Embedding vector spaces in each timestamp are different. In order\nto tackle these challenges, dynnode2vec uses evolving random walks and\ninitializes the current graph embedding with previous embedding vectors. We\ndemonstrate the advantages of the proposed dynamic network embedding by\nconducting empirical evaluations on several large dynamic network datasets.",
            "metadata": {
              "title": "dynnode2vec: Scalable Dynamic Network Embedding",
              "source": "arxiv"
            },
            "vector_score": 0.6355,
            "graph_score": 1.0,
            "combined_score": 0.7813,
            "reasoning": "Moderate semantic similarity (64%), strongly connected in graph (100%)"
          },
          {
            "node_id": "arxiv-235",
            "text": "A Dual-Perception Graph Neural Network with Multi-hop Graph Generator\n\nGraph neural networks (GNNs) have drawn increasing attention in recent years\nand achieved remarkable performance in many graph-based tasks, especially in\nsemi-supervised learning on graphs. However, most existing GNNs excessively\nrely on topological structures and aggregate multi-hop neighborhood information\nby simply stacking network layers, which may introduce superfluous noise\ninformation, limit the expressive power of GNNs and lead to the over-smoothing\nproblem ultimately. In light of this, we propose a novel Dual-Perception Graph\nNeural Network (DPGNN) to address these issues. In DPGNN, we utilize node\nfeatures to construct a feature graph, and perform node representations\nlearning based on the original topology graph and the constructed feature graph\nsimultaneously, which conduce to capture the structural neighborhood\ninformation and the feature-related information. Furthermore, we design a\nMulti-Hop Graph Generator (MHGG), which applies a node-to-hop attention\nmechanism to aggregate node-specific multi-hop neighborhood information\nadaptively. Finally, we apply self-ensembling to form a consistent prediction\nfor unlabeled node representations. Experimental results on five datasets with\ndifferent topological structures demonstrate that our proposed DPGNN\noutperforms all the latest state-of-the-art models on all datasets, which\nproves the superiority and versatility of our model. The source code of our\nmodel is available at https://github.com.",
            "metadata": {
              "title": "A Dual-Perception Graph Neural Network with Multi-hop Graph Generator",
              "source": "arxiv"
            },
            "vector_score": 0.5874,
            "graph_score": 1.0,
            "combined_score": 0.7524,
            "reasoning": "Moderate semantic similarity (59%), strongly connected in graph (100%)"
          }
        ]
      },
      {
        "query": "recurrent neural network sequence modeling",
        "time_ms": 14.73,
        "count": 10,
        "results": [
          {
            "node_id": "arxiv-237",
            "text": "Variational Recurrent Auto-Encoders\n\nIn this paper we propose a model that combines the strengths of RNNs and\nSGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used\nfor efficient, large scale unsupervised learning on time series data, mapping\nthe time series data to a latent vector representation. The model is\ngenerative, such that data can be generated from samples of the latent space.\nAn important contribution of this work is that the model can make use of\nunlabeled data in order to facilitate supervised training of RNNs by\ninitialising the weights and network state.",
            "metadata": {
              "title": "Variational Recurrent Auto-Encoders",
              "source": "arxiv"
            },
            "vector_score": 0.5643,
            "graph_score": 1.0,
            "combined_score": 0.7386,
            "reasoning": "Moderate semantic similarity (56%), strongly connected in graph (100%)"
          },
          {
            "node_id": "arxiv-216",
            "text": "Sequential Short-Text Classification with Recurrent and Convolutional\n  Neural Networks\n\nRecent approaches based on artificial neural networks (ANNs) have shown\npromising results for short-text classification. However, many short texts\noccur in sequences (e.g., sentences in a document or utterances in a dialog),\nand most existing ANN-based systems do not leverage the preceding short texts\nwhen classifying a subsequent one. In this work, we present a model based on\nrecurrent neural networks and convolutional neural networks that incorporates\nthe preceding short texts. Our model achieves state-of-the-art results on three\ndifferent datasets for dialog act prediction.",
            "metadata": {
              "title": "Sequential Short-Text Classification with Recurrent and Convolutional\n  Neural Networks",
              "source": "arxiv"
            },
            "vector_score": 0.5195,
            "graph_score": 1.0,
            "combined_score": 0.7117,
            "reasoning": "Moderate semantic similarity (52%), strongly connected in graph (100%)"
          },
          {
            "node_id": "arxiv-39",
            "text": "Recurrent Neural Networks for Time Series Forecasting\n\nTime series forecasting is difficult. It is difficult even for recurrent\nneural networks with their inherent ability to learn sequentiality. This\narticle presents a recurrent neural network based time series forecasting\nframework covering feature engineering, feature importances, point and interval\npredictions, and forecast evaluation. The description of the method is followed\nby an empirical study using both LSTM and GRU networks.",
            "metadata": {
              "title": "Recurrent Neural Networks for Time Series Forecasting",
              "source": "arxiv"
            },
            "vector_score": 0.5106,
            "graph_score": 1.0,
            "combined_score": 0.7064,
            "reasoning": "Moderate semantic similarity (51%), strongly connected in graph (100%)"
          }
        ]
      },
      {
        "query": "autoencoder latent representation learning",
        "time_ms": 15.87,
        "count": 10,
        "results": [
          {
            "node_id": "arxiv-237",
            "text": "Variational Recurrent Auto-Encoders\n\nIn this paper we propose a model that combines the strengths of RNNs and\nSGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used\nfor efficient, large scale unsupervised learning on time series data, mapping\nthe time series data to a latent vector representation. The model is\ngenerative, such that data can be generated from samples of the latent space.\nAn important contribution of this work is that the model can make use of\nunlabeled data in order to facilitate supervised training of RNNs by\ninitialising the weights and network state.",
            "metadata": {
              "title": "Variational Recurrent Auto-Encoders",
              "source": "arxiv"
            },
            "vector_score": 0.6093,
            "graph_score": 1.0,
            "combined_score": 0.7656,
            "reasoning": "Moderate semantic similarity (61%), strongly connected in graph (100%)"
          },
          {
            "node_id": "arxiv-376",
            "text": "Latent Code-Based Fusion: A Volterra Neural Network Approach\n\nWe propose a deep structure encoder using the recently introduced Volterra\nNeural Networks (VNNs) to seek a latent representation of multi-modal data\nwhose features are jointly captured by a union of subspaces. The so-called\nself-representation embedding of the latent codes leads to a simplified fusion\nwhich is driven by a similarly constructed decoding. The Volterra Filter\narchitecture achieved reduction in parameter complexity is primarily due to\ncontrolled non-linearities being introduced by the higher-order convolutions in\ncontrast to generalized activation functions. Experimental results on two\ndifferent datasets have shown a significant improvement in the clustering\nperformance for VNNs auto-encoder over conventional Convolutional Neural\nNetworks (CNNs) auto-encoder. In addition, we also show that the proposed\napproach demonstrates a much-improved sample complexity over CNN-based\nauto-encoder with a superb robust classification performance.",
            "metadata": {
              "title": "Latent Code-Based Fusion: A Volterra Neural Network Approach",
              "source": "arxiv"
            },
            "vector_score": 0.5797,
            "graph_score": 1.0,
            "combined_score": 0.7478,
            "reasoning": "Moderate semantic similarity (58%), strongly connected in graph (100%)"
          },
          {
            "node_id": "arxiv-430",
            "text": "AAG: Self-Supervised Representation Learning by Auxiliary Augmentation\n  with GNT-Xent Loss\n\nSelf-supervised representation learning is an emerging research topic for its\npowerful capacity in learning with unlabeled data. As a mainstream\nself-supervised learning method, augmentation-based contrastive learning has\nachieved great success in various computer vision tasks that lack manual\nannotations. Despite current progress, the existing methods are often limited\nby extra cost on memory or storage, and their performance still has large room\nfor improvement. Here we present a self-supervised representation learning\nmethod, namely AAG, which is featured by an auxiliary augmentation strategy and\nGNT-Xent loss. The auxiliary augmentation is able to promote the performance of\ncontrastive learning by increasing the diversity of images. The proposed\nGNT-Xent loss enables a steady and fast training process and yields competitive\naccuracy. Experiment results demonstrate the superiority of AAG to previous\nstate-of-the-art methods on CIFAR10, CIFAR100, and SVHN. Especially, AAG\nachieves 94.5% top-1 accuracy on CIFAR10 with batch size 64, which is 0.5%\nhigher than the best result of SimCLR with batch size 1024.",
            "metadata": {
              "title": "AAG: Self-Supervised Representation Learning by Auxiliary Augmentation\n  with GNT-Xent Loss",
              "source": "arxiv"
            },
            "vector_score": 0.5211,
            "graph_score": 1.0,
            "combined_score": 0.7127,
            "reasoning": "Moderate semantic similarity (52%), strongly connected in graph (100%)"
          }
        ]
      },
      {
        "query": "machine learning model training efficiency",
        "time_ms": 13.82,
        "count": 10,
        "results": [
          {
            "node_id": "arxiv-335",
            "text": "Using Known Information to Accelerate HyperParameters Optimization Based\n  on SMBO\n\nAutoml is the key technology for machine learning problem. Current state of\nart hyperparameter optimization methods are based on traditional black-box\noptimization methods like SMBO (SMAC, TPE). The objective function of black-box\noptimization is non-smooth, or time-consuming to evaluate, or in some way\nnoisy. Recent years, many researchers offered the work about the properties of\nhyperparameters. However, traditional hyperparameter optimization methods do\nnot take those information into consideration. In this paper, we use gradient\ninformation and machine learning model analysis information to accelerate\ntraditional hyperparameter optimization methods SMBO. In our L2 norm\nexperiments, our method yielded state-of-the-art performance, and in many cases\noutperformed the previous best configuration approach.",
            "metadata": {
              "title": "Using Known Information to Accelerate HyperParameters Optimization Based\n  on SMBO",
              "source": "arxiv"
            },
            "vector_score": 0.5024,
            "graph_score": 1.0,
            "combined_score": 0.7014,
            "reasoning": "Moderate semantic similarity (50%), strongly connected in graph (100%)"
          },
          {
            "node_id": "arxiv-226",
            "text": "Tree-based local explanations of machine learning model predictions,\n  AraucanaXAI\n\nIncreasingly complex learning methods such as boosting, bagging and deep\nlearning have made ML models more accurate, but harder to understand and\ninterpret. A tradeoff between performance and intelligibility is often to be\nfaced, especially in high-stakes applications like medicine. In the present\narticle we propose a novel methodological approach for generating explanations\nof the predictions of a generic ML model, given a specific instance for which\nthe prediction has been made, that can tackle both classification and\nregression tasks. Advantages of the proposed XAI approach include improved\nfidelity to the original model, the ability to deal with non-linear decision\nboundaries, and native support to both classification and regression problems",
            "metadata": {
              "title": "Tree-based local explanations of machine learning model predictions,\n  AraucanaXAI",
              "source": "arxiv"
            },
            "vector_score": 0.5017,
            "graph_score": 1.0,
            "combined_score": 0.701,
            "reasoning": "Moderate semantic similarity (50%), strongly connected in graph (100%)"
          },
          {
            "node_id": "arxiv-354",
            "text": "Are We Ready For Learned Cardinality Estimation?\n\nCardinality estimation is a fundamental but long unresolved problem in query\noptimization. Recently, multiple papers from different research groups\nconsistently report that learned models have the potential to replace existing\ncardinality estimators. In this paper, we ask a forward-thinking question: Are\nwe ready to deploy these learned cardinality models in production? Our study\nconsists of three main parts. Firstly, we focus on the static environment\n(i.e., no data updates) and compare five new learned methods with eight\ntraditional methods on four real-world datasets under a unified workload\nsetting. The results show that learned models are indeed more accurate than\ntraditional methods, but they often suffer from high training and inference\ncosts. Secondly, we explore whether these learned models are ready for dynamic\nenvironments (i.e., frequent data updates). We find that they cannot catch up\nwith fast data up-dates and return large errors for different reasons. For less\nfrequent updates, they can perform better but there is no clear winner among\nthemselves. Thirdly, we take a deeper look into learned models and explore when\nthey may go wrong. Our results show that the performance of learned methods can\nbe greatly affected by the changes in correlation, skewness, or domain size.\nMore importantly, their behaviors are much harder to interpret and often\nunpredictable. Based on these findings, we identify two promising research\ndirections (control the cost of learned models and make learned models\ntrustworthy) and suggest a number of research opportunities. We hope that our\nstudy can guide researchers and practitioners to work together to eventually\npush learned cardinality estimators into real database systems.",
            "metadata": {
              "title": "Are We Ready For Learned Cardinality Estimation?",
              "source": "arxiv"
            },
            "vector_score": 0.4996,
            "graph_score": 1.0,
            "combined_score": 0.6998,
            "reasoning": "Low semantic similarity (50%), strongly connected in graph (100%)"
          }
        ]
      }
    ]
  }
}